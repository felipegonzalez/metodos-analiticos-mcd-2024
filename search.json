[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Métodos analíticos",
    "section": "",
    "text": "Temario y referencias\nEste es un curso de estadística bayesiana con énfasis en inferencia causal y flujos de trabajo robustos para análisis de datos. Está basado en el material de McElreath (2020).\nTodas las notas y material del curso estarán en este repositorio.\n\nModelos estadísticos e inferencia causal\nBásicos del flujo de trabajo para inferencia bayesiana\nBásicos de modelación\nModelos gráficos (DAGS) y efectos causales\nExperimentos. Buenos y malos controles\nMCMC, Monte Carlo Hamiltoniano y Stan\nFlujo de trabajo bayesiano avanzado\nModelos jerárquicos\nError de medición y clasificación incorrecta\nDatos faltantes\nOtros métodos de inferencia causal\n\n\nEvaluación\n\nTareas semanales (20%)\nExamen parcial (40% teórico)\nUn examen final (40% práctico)\n\n\n\nMaterial\nCada semestre las notas cambian, en algunas partes considerablemente. Las de este semestre están en este repositorio, incluyendo ejemplos, ejercicios y tareas.\n\n\nReferencias principales\nEste curso sigue aproximadamente la primera referencia (Statistical Rethinking).\n\nStatistical Rethinking\nCausal Inference in Statistics: a primer\nCounterfactuals and Causal Inference: Methods and Principles for Social Research\nBayesian workflow]\nTowards a principled Bayesian workflow\n\n\n\nOtras referencias\n\nThe Book of Why\nCausal Inference: The Mixtape\nData Analysis Using Regression and Multilevel/Hierarchical Models\nPattern Recognition and Machine Learning\n\n\n\nSoftware: R y Rstudio\nPara hacer las tareas y exámenes pueden usar cualquier lenguaje de programación que les convenga (R o Python, por ejemplo) - el único requisito esté basado en código y no point-and-click. Adicionalmente usaremos Stan:\n\nStan: a state-of-the-art platform for statistical modeling and high-performance statistical computation, que tiene interfaces en R, Python, Julia, etc.\n\n\n\n\n\nMcElreath, R. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. A Chapman & Hall libro. CRC Press. https://books.google.com.mx/books?id=Ie2vxQEACAAJ.",
    "crumbs": [
      "Temario y referencias"
    ]
  },
  {
    "objectID": "01-introduccion.html",
    "href": "01-introduccion.html",
    "title": "1  Introducción",
    "section": "",
    "text": "1.1 Diagramas causales\nEn primer lugar, observamos (McElreath (2020)):\nLas causas de los datos no pueden extrarse de los datos solamente. Muchas veces nos referimos a las causas de los datos como el proceso generador de los datos: esto incluye aspectos del fenómeno que nos interesa (ciencia o proceso de negocios, etc.), así como el proceso de observación (muestras, valores no observados, etc.).\nConsideremos un ejemplo simple para ilustrar este primer principio:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#diagramas-causales",
    "href": "01-introduccion.html#diagramas-causales",
    "title": "1  Introducción",
    "section": "",
    "text": "Causas y mecanismos\n\n\n\nLas razones de cómo hacemos análisis estadístico (que procedimiento o algoritmo seleccionamos, por ejemplo) en un problema dado no están en los datos observados, las causas de los datos.\n\n\n\n\n\nEjemplo (cálculos renales)\nEste es un estudio real acerca de tratamientos para cálculos renales (Julious y Mullee (1994)). Pacientes se asignaron de una forma no controlada a dos tipos de tratamientos para reducir cálculos renales. Para cada paciente, conocemos el tipo de ćalculos que tenía (grandes o chicos) y si el tratamiento tuvo éxito o no.\nLa tabla original tiene 700 renglones (cada renglón es un paciente)\n\ncalculos &lt;- read_csv(\"../datos/kidney_stone_data.csv\")\nnames(calculos) &lt;- c(\"tratamiento\", \"tamaño\", \"éxito\")\ncalculos &lt;- calculos |&gt; \n   mutate(tamaño = ifelse(tamaño == \"large\", \"grandes\", \"chicos\")) |&gt; \n   mutate(resultado = ifelse(éxito == 1, \"mejora\", \"sin_mejora\")) |&gt; \n   select(tratamiento, tamaño, resultado)\nnrow(calculos)\n\n[1] 700\n\n\ny se ve como sigue (muestreamos algunos renglones):\n\ncalculos |&gt; \n   sample_n(10) |&gt; kable() |&gt; \n   kable_paper(full_width = FALSE)\n\n\n\n\ntratamiento\ntamaño\nresultado\n\n\n\n\nA\ngrandes\nmejora\n\n\nA\ngrandes\nmejora\n\n\nB\ngrandes\nmejora\n\n\nA\ngrandes\nmejora\n\n\nB\nchicos\nmejora\n\n\nB\nchicos\nmejora\n\n\nA\ngrandes\nmejora\n\n\nA\ngrandes\nmejora\n\n\nB\ngrandes\nmejora\n\n\nA\ngrandes\nsin_mejora\n\n\n\n\n\n\n\nAunque estos datos contienen información de 700 pacientes, los datos pueden resumirse sin pérdida de información contando como sigue:\n\ncalculos_agregada &lt;- calculos |&gt; \n   group_by(tratamiento, tamaño, resultado) |&gt; \n   count()\ncalculos_agregada |&gt; kable() |&gt; \n   kable_paper(full_width = FALSE)\n\n\n\n\ntratamiento\ntamaño\nresultado\nn\n\n\n\n\nA\nchicos\nmejora\n81\n\n\nA\nchicos\nsin_mejora\n6\n\n\nA\ngrandes\nmejora\n192\n\n\nA\ngrandes\nsin_mejora\n71\n\n\nB\nchicos\nmejora\n234\n\n\nB\nchicos\nsin_mejora\n36\n\n\nB\ngrandes\nmejora\n55\n\n\nB\ngrandes\nsin_mejora\n25\n\n\n\n\n\n\n\nComo en este caso nos interesa principalmente la tasa de éxito de cada tratamiento, podemos mejorar mostrando como sigue:\n\ncalculos_agregada |&gt; pivot_wider(names_from = resultado, values_from = n) |&gt; \n   mutate(total = mejora + sin_mejora) |&gt; \n   mutate(prop_mejora = round(mejora / total, 2)) |&gt; \n   select(tratamiento, tamaño, total, prop_mejora) |&gt; \n   arrange(tamaño) |&gt; \n   kable() |&gt; \n   kable_paper(full_width = FALSE)\n\n\n\n\ntratamiento\ntamaño\ntotal\nprop_mejora\n\n\n\n\nA\nchicos\n87\n0.93\n\n\nB\nchicos\n270\n0.87\n\n\nA\ngrandes\n263\n0.73\n\n\nB\ngrandes\n80\n0.69\n\n\n\n\n\n\n\nEsta tabla descriptiva es una reescritura de los datos, y no hemos resumido nada todavía. Pero es apropiada para empezar a contestar la pregunta:\n\n¿Qué indican estos datos acerca de qué tratamiento es mejor? ¿Acerca del tamaño de cálculos grandes o chicos?\n\nSupongamos que otro analista decide comparar los pacientes que recibieron cada tratamiento, ignorando la variable de tamaño:\n\ncalculos |&gt; group_by(tratamiento) |&gt; \n   summarise(prop_mejora = mean(resultado == \"mejora\") |&gt; round(2)) |&gt; \n   kable() |&gt; \n   kable_paper(full_width = FALSE)\n\n\n\n\ntratamiento\nprop_mejora\n\n\n\n\nA\n0.78\n\n\nB\n0.83\n\n\n\n\n\n\n\ny parece ser que el tratamiento \\(B\\) es mejor que el \\(A\\). Esta es una paradoja (un ejemplo de la paradoja de Simpson) . Si un médico no sabe que tipo de cálculos tiene el paciente, ¿entonces debería recetar \\(B\\)? ¿Si sabe debería recetar \\(A\\)? Esta discusión parece no tener mucho sentido.\nPodemos investigar por qué está pasando esto considerando la siguiente tabla, que solo examina cómo se asignó el tratamiento dependiendo del tipo de cálculos de cada paciente:\n\ncalculos |&gt; group_by(tratamiento, tamaño) |&gt; count() |&gt; \n   kable() |&gt; \n   kable_paper(full_width = FALSE)\n\n\n\n\ntratamiento\ntamaño\nn\n\n\n\n\nA\nchicos\n87\n\n\nA\ngrandes\n263\n\n\nB\nchicos\n270\n\n\nB\ngrandes\n80\n\n\n\n\n\n\n\nNuestra hipótesis aquí es que la decisión de qué tratamiento usar depende del tamaño de los cálculos. En este caso, hay una decisión pues A es una cirugía y B es un procedimiento menos invasivo, y se prefiere utilizar el tratamiento \\(A\\) para cálculos grandes, y \\(B\\) para cálculos chicos. Esto quiere decir que en la tabla total el tratamiento \\(A\\) está en desventaja porque se usa en casos más difíciles, pero el tratamiento \\(A\\) parece ser en general mejor. La razón es probablemente un proceso de optimización de recursos y riesgo que hacen los doctores.\n\nEn este caso, una mejor respuesta a la pregunta de qué tratamiento es mejor es la que presenta los datos desagregados.\nLa tabla desagregada de asignación del tratamiento nos informa acerca de cómo se está distribuyendo el tratamiento en los pacientes.\n\n\n\n\n\n\n\nNota\n\n\n\nLos resúmenes descriptivos acompañados de hipótesis causales acerca del proceso generador de datos, nos guía hacia descripciones interpretables de los datos.\n\n\nLas explicaciones no son tan simples y, otra vez, interviene el comportamiento de doctores, tratamientos, y distintos tipos de padecimientos.\nPodemos codificar la información causal con un diagrama:\n\n\nCódigo\ngrViz(\"\ndigraph {\n  graph [ranksep = 0.2]\n  node [shape=plaintext]\n    T \n    M \n    C\n  edge [minlen = 3]\n    T -&gt; M\n    C -&gt; T\n    C -&gt; M\n{ rank = same; M; T }\n}\n\", width = 200, height = 50)\n\n\n\n\n\n\nEs decir, el tamaño de los cálculos es una causa común de tratamiento (T) y resultado (M). Veremos más adelante que la decisión de condicionar a el tipo de cálculos proviene de un análisis relativamente simple de este diagrama causal, independientemente de los métodos que usemos para estimar las proporciones de interés (en este ejemplo, examinar las tablas cruzadas es equivalente a hacer estimaciones de máxima verosimlitud).\n\n\nEjemplo (cálculos renales 2)\nContrastemos el ejemplo anterior usando exactamente la misma tabla de datos, pero con el supuesto de un proceso generador diferente. En este caso, los tratamientos son para mejorar alguna enfermedad del corazón. Sabemos que parte del efecto de este tratamiento ocurre gracias a una baja en presión arterial de los pacientes, así que después de administrar el tratamiento, se toma la presión arterial de los pacientes. Ahora tenemos la tabla agregada y desagregada como sigue:\n\ncorazon &lt;- calculos |&gt; \n  select(tratamiento, presión = tamaño, resultado) |&gt; \n  mutate(presión = ifelse(presión == \"grandes\", \"alta\", \"baja\"))\ncorazon_agregada &lt;- corazon |&gt; \n   group_by(tratamiento, presión, resultado) |&gt; \n   count()\ncorazon_agregada |&gt; pivot_wider(names_from = resultado, values_from = n) |&gt; \n   mutate(total = mejora + sin_mejora) |&gt; \n   mutate(prop_mejora = round(mejora / total, 2)) |&gt; \n   select(tratamiento, presión, total, prop_mejora) |&gt; \n   arrange(presión) |&gt; \n   kable() |&gt; \n   kable_paper(full_width = FALSE)\n\n\n\n\ntratamiento\npresión\ntotal\nprop_mejora\n\n\n\n\nA\nalta\n263\n0.73\n\n\nB\nalta\n80\n0.69\n\n\nA\nbaja\n87\n0.93\n\n\nB\nbaja\n270\n0.87\n\n\n\n\n\n\n\n\ncorazon |&gt; group_by(tratamiento) |&gt; \n   summarise(prop_mejora = mean(resultado == \"mejora\") |&gt; round(2)) |&gt; \n   kable() |&gt; \n   kable_paper(full_width = FALSE)\n\n\n\n\ntratamiento\nprop_mejora\n\n\n\n\nA\n0.78\n\n\nB\n0.83\n\n\n\n\n\n\n\n¿Cuál creemos que es el mejor tratamiento en este caso? ¿Deberíamos usar la tabla agregada o la desagregada por presión?\n\nEn este caso, la tabla agregada es más apropiada (B es mejor tratamiento).\nLa razón es que presión en este caso es una consecuencia de tomar el tratamiento, y como las tablas muestran, B es más exitoso en bajar la presión de los pacientes.\nSi sólo comparamos dentro de los grupos de presión baja o de presión alta, ignoramos lo más importante del tratamiento en la probabilidad de mejorar.\n\nNuestros supuestos causales podemos mostrarlos con el siguiente diagrama:\n\n\nCódigo\ngrViz(\"\ndigraph {\n  graph [ranksep = 0.2]\n  node [shape=plaintext]\n    P\n    T \n    M \n  edge [minlen = 3]\n    T -&gt; P\n    P -&gt; M\n    T -&gt; M\n{ rank = same; M; T}\n}\n\", width = 200, height = 50)\n\n\n\n\n\n\nNótese que el análisis más apropiado no está en los datos: en ambos casos la tabla de datos es exactamente la misma. Los supuestos acerca del proceso que genera los datos sin embargo nos lleva a respuestas opuestas.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#diagramas-causales-1",
    "href": "01-introduccion.html#diagramas-causales-1",
    "title": "1  Introducción",
    "section": "Diagramas causales",
    "text": "Diagramas causales\nLos diagramas de arriba se llaman DAGs (Gráficas dirigidas acíclicas), y no son generadas por datos observados, sino que codifican conocimiento acerca del fenómenos y los datos observados. Nos ayudan a (McElreath (2020)):\n\nPensar claramente en términos científicos/de negocio acerca de nuestro problema\nExpresar los supuestos que hacemos que soportan nuestro análisis\nEntender qué podemos entender o explicar, sin hacer supuestos adicionales acerca de las relaciones particulares entre las variables.\nGuiar el análisis para decidir que modelos o procedimientos usar para contestar preguntas de interés.\n\nLos DAGs se construyen con causas, e implican asociaciones observables, pero no se construyen con asociaciones simplemente. El pensamiento causal es útil siempre que queremos responder preguntas acerca de un fenómeno de interés. En particular nos asisten en :\n\nAnálisis descriptivo\n\nComo vimos en el ejemplo anterior, incluso el análisis descriptivo (qué tabla usar, qué gráfica usar) de datos requiere de un análisis causal.\nMuchas veces los datos que tenemos, por distintas razones, tienen características que requieren procesarlos (por ejemplo ponderarlos) para que nos den respuestas entendibles.\n\n\n\nInferencia causal\n\nEfectos de intervenciones: En algunos casos, queremos saber consecuencias de una intervención sobre un sistema o proceso dados (por ejemplo, ¿cuántos accidentes graves habría si pusiéramos una multa por no usar cinturón de seguridad?). Esto requiere utilizar pensamiento causal.\nContrafactuales: También es usual necesitar pensar cómo serían las cosas si el pasado se hubiera desarrollado de manera distinta (por ejemplo, ¿cómo serían las ventas si no se hubiera gastado en publicidad?) en publicidad ?).\n\n\n\nDiseño de estudios o experimentos\n\nSi queremos recolectar datos acerca de un fenómeno particular (por ejemplo, ¿cómo debo seleccionar una muestra para medir orientación política de una población?), diseños eficientes requieren tener conocimiento de dominio acerca de las causas de las variables que nos interesa medir. Por ejemplo, si queremos tomar una muestra de casillas para estimar el resultado de una votación, deberíamos considerar variables geográficas como distrito electoral, grado de urbanización, etc.\n\n\n\nPredicción\n\nIncluso en problemas de predicción, modelos útiles resultan de pensar en la estructura causal del problema. Ignorar estos aspectos puede llevar fácilmente a evaluación incorrecta del desempeño, filtración de datos, o modelos que no pueden implementarse en la práctica.\n\n\n\nOtro ejemplo (admisiones de Berkeley)\nUna ejemplo al que regresaremos más adelante es el siguiente: en 1973 se recolectaron datos agregados de solicitantes para estudiar en Berkeley para los 6 departamentos más grandes, clasificados por sexo del solicitante y si fue admitido o no. Los resultados se muestran a continuación:\n\ndata(\"UCBAdmissions\")\nadm_original &lt;- UCBAdmissions |&gt; as_tibble() |&gt; \n   pivot_wider(names_from = Admit, values_from = n) \nadm_original |&gt; knitr::kable() |&gt; \n   kable_paper(full_width = FALSE)\n\n\n\n\nGender\nDept\nAdmitted\nRejected\n\n\n\n\nMale\nA\n512\n313\n\n\nFemale\nA\n89\n19\n\n\nMale\nB\n353\n207\n\n\nFemale\nB\n17\n8\n\n\nMale\nC\n120\n205\n\n\nFemale\nC\n202\n391\n\n\nMale\nD\n138\n279\n\n\nFemale\nD\n131\n244\n\n\nMale\nE\n53\n138\n\n\nFemale\nE\n94\n299\n\n\nMale\nF\n22\n351\n\n\nFemale\nF\n24\n317\n\n\n\n\n\n\n\ny las proporciones de admisión por sexo y departamente son las siguientes:\n\nadm_tbl &lt;- adm_original |&gt; \n   mutate(prop_adm = round(Admitted / (Admitted + Rejected), 2), total = Admitted + Rejected) |&gt; \n   select(Gender, Dept, prop_adm, total) |&gt; \n   pivot_wider(names_from = Gender, values_from = prop_adm:total)\nadm_tbl |&gt; knitr::kable() |&gt; \n   kable_paper(full_width = FALSE)\n\n\n\n\nDept\nprop_adm_Male\nprop_adm_Female\ntotal_Male\ntotal_Female\n\n\n\n\nA\n0.62\n0.82\n825\n108\n\n\nB\n0.63\n0.68\n560\n25\n\n\nC\n0.37\n0.34\n325\n593\n\n\nD\n0.33\n0.35\n417\n375\n\n\nE\n0.28\n0.24\n191\n393\n\n\nF\n0.06\n0.07\n373\n341\n\n\n\n\n\n\n\nComplementamos con las tasas de aceptación a total por género, y tasas de aceptación por departamento:\n\nadm_original |&gt; group_by(Gender) |&gt; \n   summarise(Admitted = sum(Admitted), Rejected = sum(Rejected)) |&gt; \n   mutate(prop_adm = round(Admitted / (Admitted + Rejected),2)) |&gt; \n   kable() |&gt; \n   kable_paper(full_width = FALSE)\n\n\n\n\nGender\nAdmitted\nRejected\nprop_adm\n\n\n\n\nFemale\n557\n1278\n0.30\n\n\nMale\n1198\n1493\n0.45\n\n\n\n\n\n\n\nLa pregunta que queremos hacer es: ¿existe discriminación por sexo en la selección de candidatos? Examinando las tablas no está clara cuál es la respuesta.\n\nadm_original |&gt; group_by(Dept) |&gt; \n   summarise(Admitted = sum(Admitted), Rejected = sum(Rejected)) |&gt; \n   mutate(prop_adm = round(Admitted / (Admitted + Rejected),2)) |&gt; \n   kable() |&gt; \n   kable_paper(full_width = FALSE)\n\n\n\n\nDept\nAdmitted\nRejected\nprop_adm\n\n\n\n\nA\n601\n332\n0.64\n\n\nB\n370\n215\n0.63\n\n\nC\n322\n596\n0.35\n\n\nD\n269\n523\n0.34\n\n\nE\n147\n437\n0.25\n\n\nF\n46\n668\n0.06\n\n\n\n\n\n\n\nDiscutiremos este ejemplo con más detalle más adelante. La interpretación debe ser hecha con cuidado, y debemos establecer claramente los supuestos que fundamentan nuestra decisión de mostrar cada tabla y de qué forma mostrarlas.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#modelos-y-algoritmos",
    "href": "01-introduccion.html#modelos-y-algoritmos",
    "title": "1  Introducción",
    "section": "1.2 Modelos y algoritmos",
    "text": "1.2 Modelos y algoritmos\nEn muchos cursos introductorios de estadística se muestran distintos tipos de procedimientos, que aplican según el tipo de datos (por ejemplo, categóricos o numéricos, pareados, no pareados, etc), generalmente con el propósito de evaluar evidencia en contra de una hipótesis nula. Por ejemplo, de McElreath (2020):\n\n\n\nEjemplo de proceso de decisión para procedimientos estadísticos\n\n\nEste enfoque puede ser confuso en un principio (¿cómo se relacionan todos estos procedimientos?), y también restringir nuestra capacidad para analizar datos: ¿qué hacemos cuando no se cumplen los supuestos de un procedimiento? Adicionalmente si no tenemos mucha experiencia, la manera en que fallan estas herramientas puede ser poco intuitiva y difícil de descubrir.\nY aunque son herramientas poderosas, no sustituyen el pensamiento científico o de proceso de negocios. Estas herramientas no generan hallazgos si no están acompañados de pensamiento causal.\nBuscamos entonces:\n\nDar herramientas (bayesianas) para analizar datos que son más flexibles, y se puedan adaptar a distintas situaciones.\nProponer un proceso para analizar datos, que sea más sistemático, robusto, y maneras de checar que el proceso es correcto o hace lo que pensamos que tiene qué hacer.\nLigar 1 y 2 con supuestos causales claros para proponer una interpretación sólida de nuestros resultados.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#análisis-como-proceso",
    "href": "01-introduccion.html#análisis-como-proceso",
    "title": "1  Introducción",
    "section": "1.3 Análisis como proceso",
    "text": "1.3 Análisis como proceso\nIremos refinando nuestro poco a poco, conforme veamos distintas herramientas y problemas. El más básico es el siguiente (McElreath (2020)):\n\nDefinir un modelo generativo para la muestra de datos.\nDefinir la cantidad que queremos estimar en relación al fenómeno de interés.\nDefinir un proceso estadístico para hacer una estimación.\nProbar el proceso 3 usando 1 y 2.\n(Usar datos) Analizar los datos, resumir resultados.\nChecar cómputos y desempeño del modelo.\n\nEste proceso no es exclusivo de los modelos bayesianos, pero quizá es más natural, como veremos, cuando adoptamos el punto de vista bayesiano. Su propósito es múltiple: verificar que nuestros modelos están estimando las cantidades que realmente nos interesan, según nuestros supuestos, verificar los programas y cómputos con los que se obtienen resultados, y checar la adecuación del modelo a datos reales, cuestionando supuestos teóricos y supuestos de modelación.\nFinalmente, quisiéramos llegar a un proceso como el que se describe en Towards a Principled Bayesian Workflow, e incorporar el que se detalla en Gelman et al. (2020):\n\n\n\nGelman et al, Bayesian Workflow",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#modelación-y-análisis-ingeniería",
    "href": "01-introduccion.html#modelación-y-análisis-ingeniería",
    "title": "1  Introducción",
    "section": "1.4 Modelación y análisis: ingeniería",
    "text": "1.4 Modelación y análisis: ingeniería\nCualquier proceso de análisis de datos se beneficia de muchos aspectos de ingenería de software. Parte de la profesionalización del análisis de datos que observamos en ciencia de datos es utilizar las herramientas reconocidas para resolver problemas de desarrollo y calidad de código, así como su documentación.\n\nAnálisis como software: Una parte de este proceso está relacionado con la reproducibilidad y documentación del trabajo, y su objetivo es evitar errores de programación y de organización (esta parte hablaremos menos: es necesario seguir los estándares de la industria para obtener resultados más confiables).\nOtra parte es el proceso con el cual construimos y contrastamos modelos para contestar preguntas, verificamos los modelos y sus respuestas y checamos resultados de cómputos.\n\n\n\n\n\nGelman, Andrew, Aki Vehtari, Daniel Simpson, Charles C. Margossian, Bob Carpenter, Yuling Yao, Lauren Kennedy, Jonah Gabry, Paul-Christian Bürkner, y Martin Modrák. 2020. «Bayesian Workflow». https://arxiv.org/abs/2011.01808.\n\n\nJulious, Steven A, y Mark A Mullee. 1994. «Confounding and Simpson’s paradox». BMJ 309 (6967): 1480-81. https://doi.org/10.1136/bmj.309.6967.1480.\n\n\nMcElreath, R. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. A Chapman & Hall libro. CRC Press. https://books.google.com.mx/books?id=Ie2vxQEACAAJ.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "02-flujo-basico.html",
    "href": "02-flujo-basico.html",
    "title": "2  Flujo de trabajo básico: motivación",
    "section": "",
    "text": "2.1 Paso 1: Modelo generativo\nConsideremos primero qué variables de interés tenemos: \\(p\\), la proporción de seropositivos en la población, \\(N\\) que es el número de personas a las que les hicimos la prueba, y \\(N_{+}\\) y \\(N_{-}\\) que cuentan el número de positivos y seronegativos en la muestra. Supondremos que la prueba da resultados exactos. Denotaremos por \\(\\theta\\) a la proporción de seropositivos en la muestra.\nComenzamos construyendo el diagrama que indica cómo influye cada variable en otra (nota: no son asociaciones, sino que indican qué variables “escuchan” a otras para determinar su valor). En este caso, \\(N\\) y \\(\\theta\\) son variable que no depende de ninguna otra, mientras que \\(N_{+}\\) y \\(N_{-}\\) dependen de \\(N\\) y \\(\\theta\\). Como \\(\\theta\\) es una cantidad que no observamos directamente, mostramos su nodo como un círculo.\nCódigo\ngrViz(\"\ndigraph {\n  graph [ranksep = 0.3, rankdir = LR]\n  node [shape=circle]\n    theta [label = &lt;&theta;&gt;]\n  node [shape=plaintext]\n    N\n    Npos [label = &lt;N&lt;SUB&gt;+&lt;/SUB&gt;&gt;]\n    Nneg [label = &lt;N&lt;SUB&gt;-&lt;/SUB&gt;&gt;]\n    #sens\n    #esp\n  edge [minlen = 3]\n    theta -&gt; Npos\n    theta -&gt; Nneg\n    N -&gt; Npos\n    N -&gt; Nneg\n    #esp -&gt; Pos\n    #sens -&gt; Pos\n    #esp -&gt; Neg\n    #sens -&gt; Neg\n{ rank = same; theta; N }\n{ rank = same; Npos; Nneg}\n#{ rank = max; sens; esp}\n\n  \n}\n\", width = 300, height = 100)\nQue también podríamos simplificar (suponiendo la \\(N\\) fija y conocida, pues \\(N_+\\) y \\(M\\) dan \\(N_{-}\\)) como:\nCódigo\ngrViz(\"\ndigraph {\n  graph [ranksep = 0.3, rankdir = LR]\n  node [shape=circle]\n    theta [label = &lt;&theta;&gt;]\n  node [shape=plaintext]\n    N\n    Npos [label = &lt;N&lt;SUB&gt;+&lt;/SUB&gt;&gt;]\n    #sens\n    #esp\n  edge [minlen = 3]\n    theta -&gt; Npos\n    N -&gt; Npos\n    #esp -&gt; Pos\n    #sens -&gt; Pos\n    #esp -&gt; Neg\n    #sens -&gt; Neg\n{ rank = same; theta; N }\n{ rank = same; Npos}\n#{ rank = max; sens; esp}\n\n  \n}\n\", width = 300, height = 100)\nY ahora construimos el modelo generativo. Supondremos que la muestra de \\(N\\) personas se toma de manera aleatoria de la población (una población grande, así que podemos ignorar el efecto de muestreo). Supondremos provisionalmente, además, que la prueba es perfecta, es decir, no hay falsos positivos o negativos.\nLa siguiente función simula una muestra de \\(N\\) personas, y regresa el número de Positivos y Negativos en la muestra.\nsim_pos_neg &lt;- function(theta = 0.01, N = 20, sens = 1, esp = 1) {\n  # verdaderos positivos que capturamos en la muestra\n  Pos_verdadero &lt;- rbinom(N, 1, theta)\n  Neg_verdadero &lt;- 1 - Pos_verdadero\n  # positivos observados en la muestra\n  Pos &lt;- Pos_verdadero\n  Neg &lt;- 1 - Pos\n  # Observaciones\n  tibble(Pos = Pos, Neg = Neg)\n}\nPodemos hacer algunas pruebas del modelo generativo en casos extremos:\nset.seed(8212)\nsim_pos_neg(theta = 1.0, N = 10)\n\n# A tibble: 10 × 2\n     Pos   Neg\n   &lt;int&gt; &lt;dbl&gt;\n 1     1     0\n 2     1     0\n 3     1     0\n 4     1     0\n 5     1     0\n 6     1     0\n 7     1     0\n 8     1     0\n 9     1     0\n10     1     0\n\nsim_pos_neg(theta = 0.0, N = 10)\n\n# A tibble: 10 × 2\n     Pos   Neg\n   &lt;int&gt; &lt;dbl&gt;\n 1     0     1\n 2     0     1\n 3     0     1\n 4     0     1\n 5     0     1\n 6     0     1\n 7     0     1\n 8     0     1\n 9     0     1\n10     0     1\n\nsim_pos_neg(theta = 0.1, N = 1e7) |&gt; pull(Pos) |&gt; mean() |&gt; \n  round(4)\n\n[1] 0.1001\nEn la práctica podemos definir pruebas más exhaustivas si es necesario. En este caso, se trata principalmente de pruebas unitarias que se utilizan comunmente en desarrollo de software.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Flujo de trabajo básico: motivación</span>"
    ]
  },
  {
    "objectID": "02-flujo-basico.html#paso-1-modelo-generativo",
    "href": "02-flujo-basico.html#paso-1-modelo-generativo",
    "title": "2  Flujo de trabajo básico: motivación",
    "section": "",
    "text": "Pruebas unitarias\n\n\n\nLa práctica estándar de pruebas unitarias consiste en probar unidades relativamente pequeñas de código (por ejemplo funciones) para verificar que funcionan correctamente.\nEsta estrategia debe utilizarse también, en la medida de los posible, en estadística.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Flujo de trabajo básico: motivación</span>"
    ]
  },
  {
    "objectID": "02-flujo-basico.html#paso-2-definir-estimando",
    "href": "02-flujo-basico.html#paso-2-definir-estimando",
    "title": "2  Flujo de trabajo básico: motivación",
    "section": "2.2 Paso 2: Definir estimando",
    "text": "2.2 Paso 2: Definir estimando\nAhora podemos definir en términos de nuestro modelo el valor que queremos estimar. En este caso, coincide con un párametro del modelo \\(\\theta\\), pero no necesariamente es así siempre: como veremos más adelante, puede ser una cantidad que se deriva de otras variables y parámetros del modelo.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Flujo de trabajo básico: motivación</span>"
    ]
  },
  {
    "objectID": "02-flujo-basico.html#paso-3-definir-un-proceso-estadístico",
    "href": "02-flujo-basico.html#paso-3-definir-un-proceso-estadístico",
    "title": "2  Flujo de trabajo básico: motivación",
    "section": "2.3 Paso 3: definir un proceso estadístico",
    "text": "2.3 Paso 3: definir un proceso estadístico\nDada la información limitada que tenemos acerca de la población, esperamos tener cierta incertidumbre en nuestra estimación del valor de \\(\\theta\\). En estadística bayesiana esta incertidumbre la expresamos mediante una distribución de probabilidades sobre posibles valores del \\(\\theta\\). Si denotamos por \\(D\\) a los datos observados, nuestro objetivo es calcular o aproximar\n\\[p(\\theta|D)\\] que es una distribución sobre los posibles valores de \\(\\theta\\), una vez que tenemos información de la muestra, y que pone más masa de probabilidad sobre las conjeturas de \\(\\theta\\) que son más probables o creíbles. A esta distribución le llamamos la distribución posterior de \\(\\theta\\).\nCon esta posterior podemos hacer afirmaciones probabilísticas de la forma:\n\n¿Cuál es la probabilidad de que \\(\\theta\\) sea menor a 1%? (Muy pocos seropositivos)\n¿Cuál es la probabildad de que \\(\\theta\\) sea mayor a 80%? (Población cerca de saturación)\n\nEstas cantidades se calculan, al menos teóricamente, integrando \\(p(\\theta|D)\\) sobre los valores de \\(\\theta\\) que nos interesan, por ejemplo,\n\\[P(\\theta &lt;= 0.01) = \\int_0^{0.01} p(\\theta|D) d\\theta\\] Nota: la integral la interpretamos como suma en el caso discreto.\nSupongamos entonces una \\(\\theta\\) dada, y que observamos la muestra \\(1,0,0,1,0\\). La probabilidad de observar esta muestra es (suponiendo observaciones independientes):\n\\[\\theta(1-\\theta)(1-\\theta)\\theta(1-\\theta) = \\theta^2(1-\\theta)^3\\] Para algunos valores de \\(\\theta\\) (posibles conjeturas acerca del valor de \\(\\theta\\)) podemos escribir una tabla como sigue (Nota: discretizamos por el momento a un número finito de valores de \\(\\theta\\) para hacer el argumento más simple):\n\ntheta &lt;- seq(0, 1, length.out = 11)\ntibble(conjetura_theta = theta, verosimiltud = theta^2 * (1 - theta)^3) |&gt; \n  kbl(col.names = c(\"Conjetura θ\", \"p(D|θ)\"),\n      escape = FALSE) \n\n\n\n\nConjetura θ\np(D|θ)\n\n\n\n\n0.0\n0.00000\n\n\n0.1\n0.00729\n\n\n0.2\n0.02048\n\n\n0.3\n0.03087\n\n\n0.4\n0.03456\n\n\n0.5\n0.03125\n\n\n0.6\n0.02304\n\n\n0.7\n0.01323\n\n\n0.8\n0.00512\n\n\n0.9\n0.00081\n\n\n1.0\n0.00000\n\n\n\n\n\n\n\nEn la tabla vemos que hay algunas conjeturas, o posibles valores de \\(\\theta\\), que tienen probabilidad considerablemente más alta que otra. La notación\n\\[p(D|\\theta)\\] significa: la probabilidad de los datos \\(D\\) dado el valor de \\(\\theta\\). Nótese que esta distribución no es la posterior que describimos arriba, y no es una distribución de probabilidad sobre \\(\\theta\\) (las probabilidades no suman uno). Esta función se llama usualmente verosimilitud de los datos, e incorpora supuestos concretos del proceso generador de los datos.\nUsando reglas de probabilidad (en particular la regla de Bayes), observamos que\n\\[p(\\theta | D) = \\frac{p(D|\\theta)p(\\theta)} { p(D)}.\\] Como \\(p(\\theta|D)\\) debe dar una distribución de probabilidad (suma o integra a 1), entonces \\(p(D)\\) debe ser una constante de normalización para el numerador de la derecha, es decir, basta escribir\n\\[p(\\theta | D) \\propto p(D|\\theta)p(\\theta) \\] Ahora es donde encontramos que tenemos que tener \\(p(\\theta)\\) para poder calcular la cantidad que nos interesa, que es la distribución posterior \\(p(\\theta|D)\\). \\(p(\\theta)\\), la distribución a priori o distribución inicial es simplemente una afirmación de dónde puede estar \\(\\theta\\), antes de observar ningún dato.\nPor el momento, podríamos poner \\(p(\\theta)\\) constante, de manera que es parte de la constante de normalización, y sólo tendríamos que normalizar como sigue:\n\ntheta &lt;- seq(0, 1, length.out = 11)\nprob_post &lt;- tibble(conjetura = theta, probablidad = theta^2 * (1 - theta)^3) |&gt; \n  mutate(prob_posterior = probablidad / sum(probablidad)) \nprob_post |&gt; \n  kable(col.names = c(\"Conjetura θ\", \"p(D|θ)\",\"p(θ|D)\")) |&gt;\n  kable_paper()\n\n\n\n\nConjetura θ\np(D|θ)\np(θ|D)\n\n\n\n\n0.0\n0.00000\n0.0000000\n\n\n0.1\n0.00729\n0.0437444\n\n\n0.2\n0.02048\n0.1228923\n\n\n0.3\n0.03087\n0.1852385\n\n\n0.4\n0.03456\n0.2073807\n\n\n0.5\n0.03125\n0.1875188\n\n\n0.6\n0.02304\n0.1382538\n\n\n0.7\n0.01323\n0.0793879\n\n\n0.8\n0.00512\n0.0307231\n\n\n0.9\n0.00081\n0.0048605\n\n\n1.0\n0.00000\n0.0000000\n\n\n\n\n\n\n\nCon esto, expresamos nuestro conocimiento acerca de \\(\\theta\\), después de observar los datos, con una distribución posterior de probabilidad sobre las posibles conjecturas. Este es el resultado principal de inferencia bayesiana, y es la base para tomar decisiones relativas a \\(\\theta\\).\n\nUsando información adicional\nSupongamos que tenemos información adicional acerca de \\(\\theta\\), por ejemplo, que en un experimento similar anterior alguien tomó una muestra de dos personas, y encontraron dos negativos. Tenemos entonces como creencias inciales:\n\ntheta &lt;- seq(0, 1, length.out = 11)\nprob_priori &lt;- tibble(conjetura = theta) |&gt; \n  mutate(prob_priori = (1 - theta) * (1 - theta)) |&gt; \n  mutate(prob_priori = prob_priori / sum(prob_priori)) \nprob_priori |&gt;\n  kable(col.names = c(\"Conjetura θ\", \"p(θ)\")) |&gt; kable_paper()\n\n\n\n\nConjetura θ\np(θ)\n\n\n\n\n0.0\n0.2597403\n\n\n0.1\n0.2103896\n\n\n0.2\n0.1662338\n\n\n0.3\n0.1272727\n\n\n0.4\n0.0935065\n\n\n0.5\n0.0649351\n\n\n0.6\n0.0415584\n\n\n0.7\n0.0233766\n\n\n0.8\n0.0103896\n\n\n0.9\n0.0025974\n\n\n1.0\n0.0000000\n\n\n\n\n\n\n\nPor ejemplo, al probabilidad inicial de que \\(\\theta\\) sea muy grande es cercana a cero, pues observamos dos negativos y ningún positivo. Ahora regresamos a considerar nuestra fórmula\n\\[p(\\theta | D) \\propto p(D|\\theta)p(\\theta), \\]\nEn este caso, la apriori o inicial tiene un efecto sobre la posterior. Reconsideramos entonces la posterior de nuestra muestra de 5 personas, y calculamos el producto de \\(P(D|\\theta)\\) por \\(p(\\theta)\\):\n\nprob_post &lt;- prob_priori |&gt; \n  mutate(verosimilitud = theta^2 * (1 - theta)^3) |&gt; \n  mutate(prod = verosimilitud * prob_priori)\n\nprob_post|&gt; \n  kable(col.names = c(\"Conjetura θ\", \"p(θ)\", \"p(D|θ)\",\n                      \"p(D|θ)p(θ)\")) |&gt; kable_paper()\n\n\n\n\nConjetura θ\np(θ)\np(D|θ)\np(D|θ)p(θ)\n\n\n\n\n0.0\n0.2597403\n0.00000\n0.0000000\n\n\n0.1\n0.2103896\n0.00729\n0.0015337\n\n\n0.2\n0.1662338\n0.02048\n0.0034045\n\n\n0.3\n0.1272727\n0.03087\n0.0039289\n\n\n0.4\n0.0935065\n0.03456\n0.0032316\n\n\n0.5\n0.0649351\n0.03125\n0.0020292\n\n\n0.6\n0.0415584\n0.02304\n0.0009575\n\n\n0.7\n0.0233766\n0.01323\n0.0003093\n\n\n0.8\n0.0103896\n0.00512\n0.0000532\n\n\n0.9\n0.0025974\n0.00081\n0.0000021\n\n\n1.0\n0.0000000\n0.00000\n0.0000000\n\n\n\n\n\n\n\nY finalmente, normalizamos para encontrar la probabilidad posterior:\n\nprob_post &lt;- prob_post |&gt; \n  mutate(prob_posterior = prod / sum(prod))\n\nprob_post|&gt; \n  kable(col.names = c(\"Conjetura θ\", \"p(θ)\", \"p(D|θ)\",\n    \"p(D|θ)p(θ)\", \"p(θ|D)\"), escape = FALSE) |&gt; kable_paper()\n\n\n\n\nConjetura θ\np(θ)\np(D|θ)\np(D|θ)p(θ)\np(θ|D)\n\n\n\n\n0.0\n0.2597403\n0.00000\n0.0000000\n0.0000000\n\n\n0.1\n0.2103896\n0.00729\n0.0015337\n0.0992712\n\n\n0.2\n0.1662338\n0.02048\n0.0034045\n0.2203539\n\n\n0.3\n0.1272727\n0.03087\n0.0039289\n0.2542983\n\n\n0.4\n0.0935065\n0.03456\n0.0032316\n0.2091640\n\n\n0.5\n0.0649351\n0.03125\n0.0020292\n0.1313412\n\n\n0.6\n0.0415584\n0.02304\n0.0009575\n0.0619745\n\n\n0.7\n0.0233766\n0.01323\n0.0003093\n0.0200177\n\n\n0.8\n0.0103896\n0.00512\n0.0000532\n0.0034430\n\n\n0.9\n0.0025974\n0.00081\n0.0000021\n0.0001362\n\n\n1.0\n0.0000000\n0.00000\n0.0000000\n0.0000000\n\n\n\n\n\n\n\nLa última columna nos da el resultado final de la inferencia bayesiana. Podemos resumir algunas de sus características, por ejemplo:\n\nEs muy poco probable que la seropositividad sea mayor o igual a 0.7\nUn intervalo de 90% de probabilidad para la seropositividad es \\([0.1, 0.5]\\)\n\nLa gráfica de la posterior es:\n\nprob_post |&gt;\n  ggplot(aes(x = conjetura, y = prob_posterior)) +\n  geom_col() +\n  labs(x = \"theta\", y = \"Prob posterior\") \n\n\n\n\n\n\n\n\nAhora podemos definir, para nuestro ejemplo discretizado, la función que calcula la posterior dados los pasos 1 y 2:\n\ncalcular_posterior &lt;- function(muestra, prob_priori){\n  # distribución inicial o a prior\n  theta &lt;- seq(0, 1, length.out = 11)\n  priori &lt;- tibble(theta = theta, prob_priori = (1 - theta) * (1 - theta)) |&gt; \n    mutate(prob_priori = prob_priori / sum(prob_priori))\n  # calcular la probabilidad posterior\n  N &lt;- length(muestra)\n  Npos &lt;- sum(muestra)\n  prob_post &lt;- tibble(theta = theta) |&gt; \n      left_join(priori, by = \"theta\") |&gt; \n      mutate(prob_posterior = theta ^ Npos * (1 - theta)^(N - Npos) * prob_priori) |&gt; \n    mutate(prob_posterior = prob_posterior / sum(prob_posterior)) \n  prob_post |&gt; select(theta, prob_posterior)\n}\n\n\nmuestra &lt;- c(1,0,0,1,0)\n\n\ncalcular_posterior(muestra, prob_priori) \n\n# A tibble: 11 × 2\n   theta prob_posterior\n   &lt;dbl&gt;          &lt;dbl&gt;\n 1   0         0       \n 2   0.1       0.0993  \n 3   0.2       0.220   \n 4   0.3       0.254   \n 5   0.4       0.209   \n 6   0.5       0.131   \n 7   0.6       0.0620  \n 8   0.7       0.0200  \n 9   0.8       0.00344 \n10   0.9       0.000136\n11   1         0       \n\n\nProcedemos ahora a hacer algunas pruebas simples de nuestra función:\n\ncalcular_posterior(rep(0, 50)) |&gt; round(3)\n\n# A tibble: 11 × 2\n   theta prob_posterior\n   &lt;dbl&gt;          &lt;dbl&gt;\n 1   0            0.996\n 2   0.1          0.004\n 3   0.2          0    \n 4   0.3          0    \n 5   0.4          0    \n 6   0.5          0    \n 7   0.6          0    \n 8   0.7          0    \n 9   0.8          0    \n10   0.9          0    \n11   1            0    \n\ncalcular_posterior(rep(1, 50)) |&gt; round(3)\n\n# A tibble: 11 × 2\n   theta prob_posterior\n   &lt;dbl&gt;          &lt;dbl&gt;\n 1   0            0    \n 2   0.1          0    \n 3   0.2          0    \n 4   0.3          0    \n 5   0.4          0    \n 6   0.5          0    \n 7   0.6          0    \n 8   0.7          0    \n 9   0.8          0.011\n10   0.9          0.989\n11   1            0    \n\ncalcular_posterior(c(rep(0, 100), rep(1, 100))) |&gt; round(3)\n\n# A tibble: 11 × 2\n   theta prob_posterior\n   &lt;dbl&gt;          &lt;dbl&gt;\n 1   0            0    \n 2   0.1          0    \n 3   0.2          0    \n 4   0.3          0    \n 5   0.4          0.023\n 6   0.5          0.966\n 7   0.6          0.01 \n 8   0.7          0    \n 9   0.8          0    \n10   0.9          0    \n11   1            0    \n\n\n\n\nMás verificaciones a priori\nOtra verificación útil que podemos hacer es, una vez que hemos definido nuestro modelo generativo y un modelos estadístico asociado, generar bajo simulación datos que podríamos observar. Esto tiene como fin verificar que nuestro modelo generativo y nuestro modelo estadístico producen datos que están de acuerdo con el conocimiento experto (teoría científica o conocimiento de negocio).\nAsí que simulamos datos del modelo:\n\nset.seed(231)\nsimulacion_datos_tbl &lt;- map_df(1:500, \n    function(rep){\n      # simular valor inicial\n      theta_sim &lt;- sample(seq(0, 1, length.out = 11), \n        prob = prob_priori$prob_priori, size = 1)\n      datos_sim &lt;- sim_pos_neg(theta = theta_sim, N = 30)\n      tibble(rep = rep, theta_sim = theta_sim, datos_sim)\n    })\n\nPodemos ver por ejemplo dónde esperamos ver el número de positivos a lo largo de distintas muestras, cuando \\(N=30\\):\n\nsimulacion_datos_tbl |&gt; \n  group_by(rep, theta_sim) |&gt; \n  summarise(Npos = sum(Pos), .groups = \"drop\") |&gt; \n  ggplot(aes(x = Npos)) +\n  geom_bar() +\n  labs(x = \"Número de positivos\", y = \"Frecuencia (muestras)\") \n\n\n\n\n\n\n\n\nObservamos que con nuestros supuestos, hay una probabilidad alta de observar 0 positivos (alrededor de 0.30). Esto se debe en parte a la discretización que hicimos, y que nuestra apriori pone peso considerable en prevalencia igual a cero, lo que quizá no es muy realista, y probablemente deberíamos escoger al menos una discretización más fina.\nTambién, si consideramos los supuestos como correctos, esto puede indicar el riesgo de usar una muestra chica para estimar prevalencia si esta es muy baja: es probable que obtengamos 0 observaciones positivas.\n\n\n\n\n\n\nVerificación predictiva a priori\n\n\n\nCon este tipo de verificaciones podemos detectar las consecuencias de nuestros supuestos (incluyendo la elección de distribuciones a priori), así como otras decisiones de modelado (como la discretización).\nConflictos con el conocimiento del área deben ser explorados para entenderlos y si es necesario corregir nuestros supuestos.\n\n\nEste tipo de verificaciones es muy flexible, y debe adaptarse a los aspectos del conocimiento del área que son importantes para los expertos. Podemos usar todos nuestros recursos analíticos (tablas, resúmenes, gráficas) para producir estos chequeos.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Flujo de trabajo básico: motivación</span>"
    ]
  },
  {
    "objectID": "02-flujo-basico.html#paso-4-probar-el-proceso-de-estimación",
    "href": "02-flujo-basico.html#paso-4-probar-el-proceso-de-estimación",
    "title": "2  Flujo de trabajo básico: motivación",
    "section": "2.4 Paso 4: Probar el proceso de estimación",
    "text": "2.4 Paso 4: Probar el proceso de estimación\nAntes de utilizar datos, verificamos cómo se comporta nuestro proceso de estimación de acuerdo a los supuestos de nuestro modelo generativo.\n\n\n\n\n\n\nVerificación a priori\n\n\n\nLo mínimo que esperamos de nuestro método es que, bajo nuestros propios supuestos acerca del proceso generador de datos y nuestro procedimiento de estimación definido, nuestra función de estimación no tenga problemas numéricos o de programación, y que las estimaciones que arroja son apropiadas para la cantidad que nos interesa estimar. El procedimiento a grandes rasgos es:\n\nEstablecer valores de los parámetros a estimar\nSimular datos observados (con una \\(N\\) apropiada, dependiendo del tamaño de muestra que esperamos, aunque se puede explorar hacer más grande o más chico este valor).\nCalcular posterior de las cantidades de interés\nCompara los valores de 1) con la posterior de 3)\n\n\n\nDefinir que las posteriores son apropiadas para la cantidad que nos interesa estimar es delicado, y más adelante veremos algunos criterios para evaluar este aspecto. Por lo pronto, haremos algunas pruebas simples que pueden diagnosticar errores graves:\n\ntheta &lt;- 0.2\nN &lt;- 30\n# simular\nset.seed(9914)\ndatos_sim &lt;- sim_pos_neg(theta = theta, N = N)\nposterior &lt;- calcular_posterior(datos_sim$Pos)\nggplot(posterior, aes(x = theta, y = prob_posterior)) +\n  geom_col() +\n  labs(x = \"theta\", y = \"Prob posterior\") +\n  geom_vline(xintercept = theta, color = \"red\", linetype = \"dashed\")\n\n\n\n\n\n\n\n\nEn este caso, la estimación parece correcta. Podemo repetir el proceso con distintos valores de \\(\\theta\\):\n\nset.seed(21)\nsimulacion_rep &lt;- map_df(1:20, \n    function(rep){\n      # simular valor inicial\n      theta_sim &lt;- sample(seq(0, 1, length.out = 11), \n        prob = prob_priori$prob_priori, size = 1)\n      datos_sim &lt;- sim_pos_neg(theta = theta_sim, N = 30)\n      posterior &lt;- calcular_posterior(datos_sim$Pos)\n      posterior |&gt; mutate(theta = theta) |&gt; \n        mutate(rep = rep) |&gt; \n        mutate(theta_sim = theta_sim)\n    })\nggplot(simulacion_rep, aes(x = theta, y = prob_posterior)) +\n  geom_col() +\n  labs(x = \"theta\", y = \"Prob posterior\") +\n  geom_vline(aes(xintercept = theta_sim), color = \"red\", linetype = \"dashed\") +\n  facet_wrap(~rep)\n\n\n\n\n\n\n\n\nY vemos que en general nuestro método parece funcionar correctamente.\n\n\n\n\n\n\nObservaciones\n\n\n\n\nMás adelante veremos cómo comparar valores a estimar con la posterior a través de varias simulaciones de manera más rigurosa. Por el momento, recuerda que incluso pruebas simples o limitadas son mejores que ninguna prueba.\nTípicamente los valores iniciales se toman de la distribución a priori, como hicimos arriba. Esta prueba es en general más apropiada, pues no nos interesan configuración de parámetros con probabilidad inicial extremadamente baja (imposibles según nuestros supuestos), pero también es posible tomar algunos valores fijos de interés.\nVeremos más de chequeos o pruebas predictivas a priori, que en general también sirven para entender la adecuación del modelo y supuestos en términos de como coinciden o no datos generados con la teoría.\n\n\n\nEste paso también es importante para entender si, bajo nuestros propios supuestos, es factible obtener información útil bajo el diseño que propongamos. Por ejemplo, alguien podría proponer un diseño de muestra que sólo tome 5 personas. Podemos probar cómo se comportan nuestras estimaciones:\n\nsimulacion_rep &lt;- map_df(1:20, \n    function(rep){\n      theta_sim &lt;- sample(seq(0, 1, length.out = 11), \n        prob = prob_priori$prob_priori, size = 1)\n      datos_sim &lt;- sim_pos_neg(theta = theta_sim, N = 3)\n      posterior &lt;- calcular_posterior(datos_sim$Pos)\n      posterior |&gt; mutate(theta = theta) |&gt; \n        mutate(rep = rep) |&gt; \n        mutate(theta_sim = theta_sim)\n    })\nggplot(simulacion_rep, aes(x = theta, y = prob_posterior)) +\n  geom_col() +\n  labs(x = \"theta\", y = \"Prob posterior\") +\n  geom_vline(aes(xintercept = theta_sim), color = \"red\", linetype = \"dashed\") +\n  facet_wrap(~rep)\n\n\n\n\n\n\n\n\nNuestra respuesta en este caso es que quizá con 3 personas la información obtenida no será suficiente para tomar decisiones útiles: nótese que la posterior está muy poco concentrada alrededor del verdadero valor de \\(\\theta\\).\n\n2.4.1 Introduciendo un bug\nSupongamos que tenemos un error en el cálculo de la posterior:\n\ncalcular_posterior_bug &lt;- function(muestra, prob_priori){\n  # distribución inicial o a prior\n  theta &lt;- seq(0, 1, length.out = 11)\n  priori &lt;- tibble(theta = theta, prob_priori = (1 - theta) * (1 - theta)) |&gt; \n    mutate(prob_priori = prob_priori / sum(prob_priori))\n  # calcular la probabilidad posterior\n  N &lt;- length(muestra)\n  Npos &lt;- sum(muestra)\n  prob_post &lt;- tibble(theta = theta) |&gt; \n      left_join(priori, by = \"theta\") |&gt; \n    # la siguiente línea tiene un error!\n      mutate(prob_posterior = theta ^ Npos * (1 - theta)^((N - Npos * prob_priori))) |&gt; \n    mutate(prob_posterior = prob_posterior / sum(prob_posterior)) \n  prob_post |&gt; select(theta, prob_posterior)\n}\n\nNuestro chequeo apriori se ve entonces:\n\nsimulacion_rep &lt;- map_df(1:20, \n    function(rep){\n      # simular valor inicial\n      theta_sim &lt;- sample(seq(0, 1, length.out = 11), \n        prob = prob_priori$prob_priori, size = 1)\n      datos_sim &lt;- sim_pos_neg(theta = theta_sim, N = 30)\n      posterior &lt;- calcular_posterior_bug(datos_sim$Pos)\n      posterior |&gt; mutate(theta = theta) |&gt; \n        mutate(rep = rep) |&gt; \n        mutate(theta_sim = theta_sim)\n    })\nggplot(simulacion_rep, aes(x = theta, y = prob_posterior)) +\n  geom_col() +\n  labs(x = \"theta\", y = \"Prob posterior\") +\n  geom_vline(aes(xintercept = theta_sim), color = \"red\", linetype = \"dashed\") +\n  facet_wrap(~rep)\n\n\n\n\n\n\n\n\nDonde vemos en varios casos que la “posterior” está lejos de ser consistente con los valores simulados de prueba para \\(\\theta\\).\n\n\n\n\n\n\nAspectos numéricos\n\n\n\nEs importante notar que los cálculos que hicimos arriba ingoran un principio importante al hacer cálculos de productos de probabilidades: generalmente es mejor utilizar la escala logarítmica para hacer los cálculos, y sólo al final convertir a probabilidades. Esto es porque es fácil tener subflujos numéricos al multiplicar muchas probabilidades pequeñas.\n\n\nAunque en este caso no es crítico, la siguiente función sigue esta práctica que en general es necesario seguir:\n\n# Evitar desbordes al sumar exponenciales\nlog_sum_exp &lt;- function(x){\n  max_x &lt;- max(x)\n  max_x + log(sum(exp(x - max_x)))\n}\ncalcular_posterior &lt;- function(muestra, prob_priori){\n  # evitar 0 o 1 exactos\n  theta &lt;- seq(1e-12, 1 - 1e-12, length.out = 11)\n  # no es necesario normalizar esta distribución apriori\n  log_priori &lt;- tibble(theta = theta, log_prob_priori = 2 * log(1 - theta)) \n  # calcular la probabilidad posterior\n  N &lt;- length(muestra)\n  Npos &lt;- sum(muestra)\n  prob_post_tbl &lt;- tibble(theta = theta) |&gt; \n    left_join(log_priori, by = \"theta\") |&gt; \n    # log verosimilitud\n    mutate(log_prob_posterior = \n        Npos * log(theta) + log(1 - theta) * (N - Npos)) |&gt; \n    # sumar log apriori\n    mutate(log_prob_posterior = log_prob_posterior + log_prob_priori) |&gt; \n    mutate(log_prob_posterior_norm = \n      log_prob_posterior - log_sum_exp(log_prob_posterior)) |&gt; \n    mutate(prob_posterior = exp(log_prob_posterior_norm))\n  prob_post_tbl |&gt; select(theta, prob_posterior)\n}\n\nEjercicio: corre las pruebas para esta versión de la función como hicimos arriba. Este es un cambio correcto, y desde el punto de vista de desarrollo, si nuestra batería de pruebas es apropiado podemos hacerlo con más confianza.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Flujo de trabajo básico: motivación</span>"
    ]
  },
  {
    "objectID": "02-flujo-basico.html#paso-5-analizar-los-datos-y-resumir-resultados.",
    "href": "02-flujo-basico.html#paso-5-analizar-los-datos-y-resumir-resultados.",
    "title": "2  Flujo de trabajo básico: motivación",
    "section": "2.5 Paso 5: Analizar los datos y resumir resultados.",
    "text": "2.5 Paso 5: Analizar los datos y resumir resultados.\nCon este trabajo hecho (ojo: para modelos grandes es un trabajo considerable, pero importante), podemos proceder a analizar los datos.\nSupongamos que se tomó una muestra de \\(N=20\\) personas, con 17 negativos y 3 positivos. Calculamos la posterior:\n\n# en nuestro modelo *no* importa el orden, verifica:\ndatos_tbl &lt;- tibble(Pos = c(rep(1, 3), rep(0, 17)))\nposterior &lt;- calcular_posterior(muestra = datos_tbl$Pos)\nggplot(posterior, aes(x = theta, y = prob_posterior)) +\n  geom_col() +\n  labs(x = \"theta\", y = \"Prob posterior\") \n\n\n\n\n\n\n\n\nY hay varias maneras de resumir esta posterior. Por ejemplo, podemos calcular (ojo: veremos más detalles de esto más adelante):\n\n# Media\nposterior |&gt; \n  mutate(theta = theta) |&gt; \n  summarise(media = sum(theta * prob_posterior))\n\n# A tibble: 1 × 1\n  media\n  &lt;dbl&gt;\n1 0.166\n\n# Intervalo de alta probabilidad 90%\nposterior |&gt; \n  mutate(theta = theta) |&gt; \n  arrange(desc(prob_posterior)) |&gt; \n  mutate(cumsum = cumsum(prob_posterior)) |&gt; \n  filter(cumsum &lt;= 0.9) |&gt; \n  pull(theta) |&gt; \n  range()\n\n[1] 0.1 0.2",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Flujo de trabajo básico: motivación</span>"
    ]
  },
  {
    "objectID": "02-flujo-basico.html#paso-6-evaluar-el-modelo-y-cómputos",
    "href": "02-flujo-basico.html#paso-6-evaluar-el-modelo-y-cómputos",
    "title": "2  Flujo de trabajo básico: motivación",
    "section": "2.6 Paso 6: Evaluar el modelo y cómputos",
    "text": "2.6 Paso 6: Evaluar el modelo y cómputos\nEn este ejemplo, el modelo es muy simple, y los cómputos son sencillos. Para modelos más complejos es necesario checar que los cómputos sean correctos, y que el modelo ajusta razonablemente bien a los datos en los aspectos que nos interesan, de modo que dejaremos esta discusión cuando veamos el flujo bayesiano más avanzado.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Flujo de trabajo básico: motivación</span>"
    ]
  },
  {
    "objectID": "02-flujo-basico.html#versión-continua",
    "href": "02-flujo-basico.html#versión-continua",
    "title": "2  Flujo de trabajo básico: motivación",
    "section": "2.7 Versión continua",
    "text": "2.7 Versión continua\nEn el ejemplo anterior utilizamos una variable aleatoria discreta para modelar la seroprevalencia, pero esto generalmente no es conveniente. Ahora repetimos el ejercicio considerando más naturalmente que \\(\\theta\\) puede tomar cualquier valor en \\([0,1]\\).\nPara el paso 1 y 2 (definir modelo generativo y cantidad a estimar), utilizamos el mismo diagrama de arriba y la misma función que simula datos. Igual que antes, para cualquier muestra \\(D\\) compuesta de 0 y 1’s (negativos y positivos), la probabilidad de observar la muestra \\(D\\) dada una conjetura \\(\\theta\\) es:\n\\[ p(D|\\theta) = \\theta^{N_+}(1-\\theta)^{N_-}\\] Y recordamos que desde el punto de vista bayesiano, queremos resumir nuestra información obtenida con la distribución posterior \\(p(\\theta|D)\\), e igual que antes tenemos que:\n\\[p(\\theta | D) \\propto p(D|\\theta)p(\\theta).\\] Por el momento pondremos la densidad continua uniforme \\(p(\\theta) = 1\\) para \\(\\theta\\in [0,1]\\) (densidad uniforme), entonces\n\\[p(\\theta|D) \\propto \\theta^{N_+}(1-\\theta)^{N_-}\\]\nEn este caso, para normalizar tenemos que hacer la integral de la expresión de la derecha, y dividir por el resultado. En general, escribiremos\n\\[B(a,b) = \\int_{0}^1 \\theta^{a-1}(1-\\theta)^{b-1} d\\theta\\] así que en nuestro caso, la posterior es:\n\\[p(\\theta|D) = \\frac{1}{B(N_{+} + 1,N_{-}+1)} \\theta^{N_+}(1-\\theta)^{N_-}\\] Es posible demostrar con cálculo que \\(B(a,b) = \\frac{(a-1)!(b-1)!}{(a+b-1)!}\\), pero eso no es importante ahora. Este tipo de densidades pertenecen a la familia beta con parámetros \\((a,b)\\), donde \\(a&gt;0, b&gt;0\\).\nPor ejemplo, si observamos 2 positivos y tres negativos, nuestra posterior es una beta con parámetros \\((3,4)\\), y se ve así:\n\nlibrary(tidyverse)\ntheta &lt;- seq(0,1, 0.01)\ntibble(theta = theta, densidad = dbeta(theta, 3, 4)) |&gt; \n  ggplot(aes(x = theta, y = densidad)) +\n  geom_line() +\n  labs(x = \"theta\", y = \"Densidad posterior\") \n\n\n\n\n\n\n\n\nNotamos adicionalmente que es posible seleccionar otra distribución inicial que no sea la uniforme. En este caso particular es conveniente (aunque no siempre tiene sentido) usar una distribución beta, de manera que es fácil ver que si ponemos por ejemplo\n\\[p(\\theta) \\propto \\theta^{a-1}(1-\\theta)^{b-1}\\]\nentonces la posterior, por la fórmula de Bayes, es:\n\\[p(\\theta|D) \\propto \\theta^{N_+ +a -1 }(1-\\theta)^{N_{-}+b-1}\\] que también es de la familia beta, pero con parámetros \\((N_{+} +a, N_{-} +b)\\).\n\n2.7.1 Ejercicio: actualizaciones de posterior\nPodemos examinar la posterior para dados distintos datos. Supondremos que la distribución a priori es uniforme.\n\nset.seed(92192)\ntheta_seq &lt;- seq(0,1, 0.001)\ndatos_sim &lt;- sim_pos_neg(theta = 0.25, N = 12) |&gt; \n  mutate(obs = ifelse(Pos==1, \"P\", \"N\")) |&gt; \n  mutate(n = 1:12)\n# graficar posteriores para cada n\ndatos_graf &lt;- datos_sim |&gt; \n  mutate(n_pos = cumsum(Pos), n_neg = cumsum(Neg)) |&gt; \n  mutate(muestra = accumulate(obs, ~ paste0(.x, .y))) |&gt; \n  group_by(n) |&gt;\n  mutate(dens_graf = \n    list(tibble(theta = theta_seq, \n      densidad = dbeta(theta_seq, n_pos + 1, n_neg + 1)))) |&gt; \n  unnest(dens_graf)\nggplot(datos_graf, aes(x=theta, y = densidad, group = n)) +\n  geom_line() + \n  facet_wrap(~ muestra) +\n  geom_abline(slope = 0, intercept = 1, color = \"gray\") \n\n\n\n\n\n\n\n\nAhora repetimos con una inicial beta \\((0,2)\\) (que equivale a observar dos negativos y ningún positivo en una muestra de 3 personas), de modo que \\(p(\\theta) = 2(1-\\theta)\\):\n\nset.seed(92192)\ntheta_seq &lt;- seq(0,1, 0.001)\ndatos_sim &lt;- sim_pos_neg(theta = 0.25, N = 12) |&gt; \n  mutate(obs = ifelse(Pos==1, \"P\", \"N\")) |&gt; \n  mutate(n = 1:12)\n# graficar posteriores para cada n\ndatos_graf &lt;- datos_sim |&gt; \n  mutate(n_pos = cumsum(Pos), n_neg = cumsum(Neg)) |&gt; \n  mutate(muestra = accumulate(obs, ~ paste0(.x, .y))) |&gt; \n  group_by(n) |&gt;\n  mutate(dens_graf = \n    list(tibble(theta = theta_seq, \n                densidad = dbeta(theta_seq, n_pos + 1, n_neg + 3)))) |&gt; \n  unnest(dens_graf)\nggplot(datos_graf, aes(x=theta, y = densidad, group = n)) +\n  geom_line() + \n  facet_wrap(~ muestra) +\n  geom_abline(slope = -2, intercept = 2, color = \"gray\") \n\n\n\n\n\n\n\n\n\nEn este punto, podríamos ir al siguiente paso, que es escribir una función para calcular la posterior. En realidad ya sabemos su función de densidad, pero cualquier resumen que hagamos de esta distribución requerirá de integrales (¿por qué? piensa en cómo calcular la probabilidad de ser menor que un valor, o cómo se calcula la media).\nAunque en este ejemplo simple la posterior tiene una forma conocida y hay manera de calcular (analíticamente o con rutinas numéricas ya implementadas) esos resúmenes de interés (media, cuantiles, etc.), en general calcular integrales no es una estrategia que podamos llevar muy lejos.\n\n\nMás de verificaciones apriori\nAntes de continuar, sin embargo, veremos cómo se veo el chequeo predictivo a priori que consideramos en la sección de arriba.\n\nset.seed(231)\nsimulacion_datos_tbl &lt;- map_df(1:500, \n    function(rep){\n      # apriori seleccionada\n      theta_sim &lt;- rbeta(1, 1, 3)\n      datos_sim &lt;- sim_pos_neg(theta = theta_sim, N = 30)\n      tibble(rep = rep, theta_sim = theta_sim, datos_sim)\n    })\n\nPodemos ver por ejemplo dónde esperamos ver el número de positivos a lo largo de distintas muestras, cuando \\(N=30\\):\n\nsimulacion_datos_tbl |&gt; \n  group_by(rep, theta_sim) |&gt; \n  summarise(Npos = sum(Pos)) |&gt; \n  ggplot(aes(x = Npos)) +\n  geom_bar() +\n  labs(x = \"Número de positivos\", y = \"Frecuencia (muestras)\") \n\n`summarise()` has grouped output by 'rep'. You can override using the `.groups`\nargument.\n\n\n\n\n\n\n\n\n\nEste resultado es consecuencia de nuestros supuestos, antes de ver los datos, y resume que esperamos con mayor probabilidad un número bajo de positivos (en una muestra de N=30), y que es muy poco probable que observemos prevalencias muy altas. Dependiendo de la situación, este puede ser un resultado aceptable.\nUn resultado no aceptable para una enfermedad que sabemos que es relativamente rara (aunque tenemos incertidumbre), por ejemplo, podría ser el siguiente:\n\nset.seed(231)\nsimulacion_datos_tbl &lt;- map_df(1:500, \n    function(rep){\n      # apriori seleccionada\n      theta_sim &lt;- rbeta(1, 30, 3)\n      datos_sim &lt;- sim_pos_neg(theta = theta_sim, N = 30)\n      tibble(rep = rep, theta_sim = theta_sim, datos_sim)\n    })\nsimulacion_datos_tbl |&gt; \n  group_by(rep, theta_sim) |&gt; \n  summarise(Npos = sum(Pos)) |&gt; \n  ggplot(aes(x = Npos)) +\n  geom_bar() +\n  labs(x = \"Número de positivos\", y = \"Frecuencia (muestras)\") \n\n`summarise()` has grouped output by 'rep'. You can override using the `.groups`\nargument.\n\n\n\n\n\n\n\n\n\nEste resultado no es aceptable cuando sabemos que es prácticamente imposible que la mayoría de la población está infectada. Debemos entonces regresar y ajustar nuestros supuestos: el problema en este caso es la elección de la distribución a priori para \\(\\theta\\).\nObservación: la crítica es sobre el conjunto completo de supuestos iniciales que hacemos acerca del problema. Cuando los diagnósticos no son aceptables desde el punto de vista teórico es necesario investigar dónde está el problema. Las distribuciones apriori que usamos, igual que cualquier supuesto, están sujetas a esta crítica. Nótese que esta crítica la estamos haciendo sin ver los datos que esperamos observar: es una crítica de supuestos.\n\n\n2.7.2 Métodos Monte Carlo\nUna vez que tenemos la densidad posterior podemos mostrarla o resumirla de varias maneras. Si tenemos una expresión analítica, esos resúmen típicamente consisten de integrales, por ejemplo:\n\nLa media o mediana posterior\nDeciles o u otro tipo de percentiles de la posterior\nIntervalos de probabilidad posterior\n\nEste proceso puede ser no trivial incluso para densidades posteriores conocidas. La alternativa a integrar es simular de la posterior y calcular las cantidades de interés a partir de las simulaciones. En general, esto es más fácil que integrar. En nuestro ejemplo, en lugar de usar una función de calcular_posterior, construimos una que es simular_posterior.\nEsta función será simple porque simular de una beta es un problema estándar, y existen muchas implementaciones. Podríamos escribir, por ejemplo:\n\nsimular_posterior &lt;- function(muestra, n){\n  tibble(theta = \n    rbeta(n, sum(muestra) + 1, length(muestra) - sum(muestra) + 1))\n}\n\n\nmuestra\n\n[1] 1 0 0 1 0\n\nsims_post &lt;- simular_posterior(muestra, 10000)\n\n\nsims_post |&gt; \n  ggplot(aes(x = theta)) +\n  geom_histogram(bins = 50)\n\n\n\n\n\n\n\n\nSi queremos calcular la media, por ejemplo, hacemos\n\n sims_post |&gt; pull(theta) |&gt;  mean()\n\n[1] 0.4280916\n\n\nSi queremos la probabilidad de que la prevalencia esté por debajo de 20% hacemos:\n\nsims_post |&gt; \n  summarise(prob = mean(theta &lt; 0.2))\n\n# A tibble: 1 × 1\n    prob\n   &lt;dbl&gt;\n1 0.0961\n\n\nMuchas veces se presentan intervalos de probabilidad posterior, por ejemplo, podríamos reportar que con 90% de probabilidad la prevalencia está en el siguiente intervalo:\n\nsims_post |&gt; \n  summarise(inf = quantile(theta, 0.05),\n            sup = quantile(theta, 0.95)) |&gt; \n  mutate(inf = round(inf, 2),\n         sup = round(sup, 2))\n\n# A tibble: 1 × 2\n    inf   sup\n  &lt;dbl&gt; &lt;dbl&gt;\n1  0.16  0.73\n\n\nObservación: No hay un intervalo mágico que debe reportarse (por ejemplo 95% de probabilidad es una costumbre o superstición). Hay varias maneras de construir intervalos de probabilidad. Dejaremos esta discusión para más adelante.\n\n\n\n\n\n\nMétodos Monte Carlo\n\n\n\nLos métodos Monte Carlo están basados en simulación de variables aleatorias. Las cantidades que nos interesan son integrales bajo una densidad de probabilidad. Si queremos calcular en general \\[I = \\int f(x)p(x)dx,\\] simulamos una gran cantidad de observaciones \\(x_1,\\ldots, x_M\\) bajo \\(p(x)\\), y entonces (Ley de los grandes números):\n\\[\\frac{1}{M} \\sum_{i=1}^{M} f(x_i) \\to I\\] cuando \\(M\\to \\infty\\). De este modo, podemos aproximar con la precisión que requiramos la integral \\(I\\).\n\n\nNota 1: Sin más información del proceso de simulación, no es posible demostrar que una aproximación es “suficientemente” buena, no importa que tan grande sea \\(M\\). Más adelante veremos una batería de diagnósticos para al menos excluir los casos comunes en los que la aproximación es mala.\nNota 2: En nuestro caso, las integrales de interés usualmente son de la forma \\[I = \\int f(\\theta)p(\\theta|D) d\\theta,\\] donde \\(D\\) es la información de la muestra, \\(\\theta\\) en general es un vector de parámetros del modelo, y \\(f(\\theta)\\) es una función de \\(\\theta\\) que nos interesa. Por ejemplo, para la media posterior de \\(\\theta\\), usaríamos \\(f(\\theta) = \\theta\\). Podemos aproximar cualquier integral si tenemos simulaciones de la posterior:\n\\[\\theta_i \\sim p(\\theta|D) \\implies \\frac{1}{M} \\sum_{i=1}^{M} f(\\theta_i) \\to I.\\]\n\nFinalmente, checamos todo nuestra construcción de estimación como hicimos arriba, la diferencia es que ahora usamos simulaciones para entender el comportamiento de la posterior. En este caso, el proceso es como sigue:\n\nGeneramos un valor de la apriori \\(\\theta_{sim} \\sim \\text{Beta}(1,3)\\)\nSimulamos datos de la muestra (\\(N=25\\)) con el valor simulado de \\(\\theta\\)\nSimulamos un número grande \\(M\\) de valores de la posterior (aquí usaremos \\(M=10000\\))\nRepetimos 1-3\n\n\nset.seed(812)\nsimulacion_rep &lt;- map_df(1:20, \n    function(rep){\n      # simular de la apriori\n      theta_sim &lt;- rbeta(1, 1, 3)\n      # simular datos según modelo\n      datos_sim &lt;- sim_pos_neg(theta = theta_sim, N = 25)\n      # simulaciones montecarlo para la posterior\n      posterior &lt;- simular_posterior(datos_sim$Pos, 10000)\n      # junta todo\n      posterior |&gt; mutate(n_sim = n()) |&gt;\n        mutate(rep = rep) |&gt;\n        mutate(theta_sim = theta_sim)\n    })\nsimular_posterior &lt;- function(muestra, n){\n  tibble(theta = \n    rbeta(n, sum(muestra) + 1, \n             length(muestra) - sum(muestra) + 3))\n}\n\nAhora usamos histogramas por ejemplo para mostrar cómo luce la posterior, y comparamos con los valores de la simulación:\n\nggplot(simulacion_rep, aes(x = theta)) +\n  geom_histogram(bins = 50) +\n  labs(x = \"theta\", y = \"Prob posterior\") +\n  geom_vline(aes(xintercept = theta_sim), color = \"red\", linetype = \"dashed\") +\n  facet_wrap(~rep)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Flujo de trabajo básico: motivación</span>"
    ]
  },
  {
    "objectID": "02-flujo-basico.html#observaciones-1",
    "href": "02-flujo-basico.html#observaciones-1",
    "title": "2  Flujo de trabajo básico: motivación",
    "section": "2.8 Observaciones",
    "text": "2.8 Observaciones\nEl proceso de arriba lo refinaremos considerablemente en el resto del curso.\n\nEn primer lugar, los modelos generativos serán más complicados, y estarán basados en teoría más compleja (que expresamos con diagramas causales)\nUsaremos más herramientas y componentes para construir modelos estadísticos apropiados, ya sea que construyamos un modelo completo para todo el proceso de generación de datos, o que usemos modelos estándar como regresión para aproximar respuestas, cuando es apropiado\nRefinaremos el proceso de checar que el cómputo (checar Monte Carlo) y la inferencia (verificación apriori) es correcta bajo nuestros supuestos.\nFinalmente, veremos qué hacer después de hacer la estimación y que los puntos de arriba están resueltos, para tener confianza en nuestras conclusiones.\n\n\n2.8.1 Resumen\nAquí juntamos algunas observaciones que se derivan de lo que hemos visto (flujo de trabajo y estimación bayesiana):\n\nTodo nuestro trabajo está fundamentado en entender qué es lo que queremos estimar dentro de un modelo generativo. Los diagramas causales nos ayudan a conectar el problema de interés con nuestros modelos, a construir modelos generativos y hacer explícitos nuestros supuestos.\nEl proceso de estimación siempre es el mismo: nuestro estimador es la distribución posterior, que se construye a partir de la verosimilitud y la apriori (modelo generativo). Nuestro estimador es la posterior de las cantidades de interés, que pueden resumirse de distintas maneras. Cualquier cálculo derivado de otras cantidades de interés debe considerar toda la posterior (no solo la media o la moda, etc. posterior).\nNuestro proceso incluye los chequeos predictivos a priori (basados en simulación de datos). Esto son cruciales para detectar problemas en nuestros supuestos (vs teoría) y que nuestro proceso sea internamente consistente. Esto también es una verificación de la información a priori.\nGeneralmente es más conveniente y práctico hacer simulaciones que calcular analíticamente la posterior o sus integrales.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Flujo de trabajo básico: motivación</span>"
    ]
  },
  {
    "objectID": "02-flujo-basico-2.html",
    "href": "02-flujo-basico-2.html",
    "title": "3  Flujo de trabajo básico: refinando el modelo",
    "section": "",
    "text": "3.1 Prevalencia con error conocido\nNuestro ejemplo de la sección anterior es poco realista pues usualmente las pruebas que son utilizadas para medir la prevalencia no son perfectas. Bajo condiciones muy controladas, el perfil de desempeño de las pruebas se mide, obteniendo resultados son del siguiente tipo:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Flujo de trabajo básico: refinando el modelo</span>"
    ]
  },
  {
    "objectID": "02-flujo-basico-2.html#prevalencia-con-error-conocido",
    "href": "02-flujo-basico-2.html#prevalencia-con-error-conocido",
    "title": "3  Flujo de trabajo básico: refinando el modelo",
    "section": "",
    "text": "En pruebas de gold standard, el kit identificó correctamente como positivos a 103 de 122 personas infectadas, e identificó correctamente como negativos a 399 de 401 personas no infectadas.\nSin considerar la incertidumbre, esto implica que la prueba tiene una sensibilidad de 84% y una especificidad de 99.5%.\n\n\n3.1.1 Paso 1: modelo generativo\nPrimero supondremos que estos porcentajes de error son fijos. Nuestro modelo que incluye el error de medición se como sigue:\n\n\nCódigo\ngrViz(\"\ndigraph {\n  graph [ranksep = 0.3, rankdir = LR]\n  node [shape=circle]\n    p\n    Npos\n  node [shape=plaintext]\n    N\n    Npos [label = &lt;N&lt;SUB&gt;+&lt;/SUB&gt;&gt;]\n    Nobs [label = &lt;N&lt;SUB&gt;obs&lt;/SUB&gt;&gt;]\n    #Nneg [label = &lt;N&lt;SUB&gt;-&lt;/SUB&gt;&gt;]\n    #sens\n    #esp\n  edge [minlen = 3]\n    p -&gt; Npos\n    #p -&gt; Nneg\n    N -&gt; Npos\n    Npos -&gt; Nobs\n    #N -&gt; Nneg\n    esp -&gt; Nobs\n    sens -&gt; Nobs\n    #esp -&gt; Nneg\n    #sens -&gt; Nneg\n{ rank = same; p; N }\n{ rank = same; Npos}\n{ rank = max; sens; esp}\n}\n\")#, width = 200, height = 50)\n\n\n\n\n\n\nDonde vemos ahora que el estado real de cada persona de la prueba es desconocido, aunque el resultado de la prueba depende de ese estado, y la cantidad de positivos que observamos es ahora \\(N_{obs}\\), que depende también de la sensibilidad y especificidad de la prueba.\nY para constuir el modelo generativo notamos que la probabilidad de que un individuo infectado salga positivo es \\(\\text{sens}\\), y la probabilidad de que un individuo no infectado salga positivo es \\((1-\\text{esp})\\). De este modo, el modelo generativo es:\n\nsim_pos_neg &lt;- function(theta = 0.01, N = 20, sens = 0.84, esp = 0.995) {\n  # verdaderos positivos que capturamos en la muestra\n  Pos_verdadero &lt;- rbinom(N, 1, theta)\n  Neg_verdadero &lt;- 1 - Pos_verdadero\n  # positivos observados en la muestra: si es positivo, calculamos\n  # la probabilidad de que realmente sea positivo\n  sim_tbl &lt;- tibble(Pos_verdadero, Neg_verdadero) |&gt; \n    mutate(Pos = rbinom(N, 1, Pos_verdadero * sens + Neg_verdadero * (1-esp))) |&gt; \n    mutate(Neg = 1 - Pos)\n  # Observaciones\n  sim_tbl |&gt; select(Pos, Neg)\n}\n\nHacemos unas pruebas:\n\nset.seed(8212)\nsim_pos_neg(theta = 0.3, N = 1e7, sens = 0.7, esp = 1) |&gt; pull(Pos) |&gt; mean() |&gt; \n  round(4)\n\n[1] 0.2099\n\nsim_pos_neg(theta = 0.3, N = 1e7, sens = 1, esp = 1) |&gt; pull(Pos) |&gt; mean() |&gt; \n  round(4)\n\n[1] 0.3001\n\n\n\n\n3.1.2 Paso 2: cantidad a estimar\nEn este punto hay que tener cuidado, porque no queremos estimar la proporción de positivos potenciales en la población (pues la prueba es imperfecta), sino la proporción de verdaderos positivos en la población. Esta cantidad sigue siendo representada por \\(\\theta\\) en nuestro modelo generativo.\n\n\n3.1.3 Paso 3: modelo estadístico\nEl modelo estadístico es ahora diferente. Vamos a plantear primero \\(p(D|\\theta, sens, esp)\\), que es la probabilidad de observar los datos \\(D\\) dado que \\(\\theta\\) es el parámetro de interés, y \\(sens\\) y \\(esp\\) (que en este caso suponemos conocidos). Es fácil ver que la probabilidad de obtener un positivo ahora es:\n\\(\\theta_{obs} = P(Positivo | \\theta, sens, esp) = \\theta \\cdot sens + (1-\\theta) \\cdot (1-esp)\\)\nSi llamamos a esta cantidad \\(\\theta_{obs}\\), de forma que dada una muestra de 0’s y 1’s, tenemos que la verosimilitud de la muestra dada cada conjetura \\(\\theta\\), y con \\(sens\\) y \\(esp\\) fijas, es:\n\\[p(D|\\theta, sens, esp) = \\theta_{obs}^{N_{+}}(1-\\theta_{obs})^{N_{-}}\\] Suponiendo que la distribución apriori de \\(\\theta\\) es uniforme, tenemos entonces que la distribución posterior cumple:\n\\[p(\\theta|D, sens, esp) \\propto \\theta_{obs}^{N_{+}}(1-\\theta_{obs})^{N_{-}}\\] donde \\(\\theta_{obs}\\) está dada por la fórmula de arriba. Sustituyendo:\n\\[p(\\theta|D, sens, esp) \\propto (\\theta \\cdot sens + (1-\\theta) \\cdot (1-esp))^{N_{+}}(\\theta(1-sens) + (1-\\theta)esp)^{N_{-}}\\]\nEsta posterior tiene la estructura de una distribución beta, pero es un poco más complicada. En este punto, utilizaremos una técnica que funciona para problemas chicos (de unos cuantos parámetros), y que consiste en hacer una aproximación discreta de la distribución posterior:\n\n\n\n\n\n\nMétodo de aproximación de rejilla\n\n\n\n\nDividimos el intervalo \\([0,1]\\) en \\(m\\) partes iguales, y calculamos el valor de la expresión proporcional a la posterior en cada uno de estos intervalos (por ejemplo en los puntos medios).\nNormalizamos estos valores para que sumen 1, y obtenemos una distribución discreta que aproxima la posterior.\nMuestreamos de esta distribución discreta para obtener una muestra de la posterior.\n\nEste método sólo es factible en modelos simples cuando hay solamente unos cuantos parámetros por estimar, pues su complejidad crece exponencialmente con el número de parámetros. Rara vez se usa en la práctica por esta razón.\n\n\nAquí implementamos esta técnica de aproximación por rejilla. Incluimos también una Beta(1,3) como a priori:\n\nsimular_posterior_error &lt;- function(muestra, n, sens = 1, esp = 1){\n    theta &lt;- seq(1e-12, 1-1e-12, by = 0.0001)\n    p_obs &lt;- theta * sens + (1 - theta) * (1 - esp)\n    # verosimilitud (en logaritmo)\n    log_dens_sin_norm &lt;- log(p_obs) * sum(muestra) +  \n      log(1-p_obs) * (length(muestra) - sum(muestra))\n    # a priori\n    log_dens_sin_norm &lt;- log_dens_sin_norm + dbeta(theta, 1, 3, log = TRUE)\n    # normalizar\n    log_dens_norm &lt;- log_dens_sin_norm - log_sum_exp(log_dens_sin_norm)\n    densidad_post &lt;- exp(log_dens_norm)\n    tibble(theta = sample(theta, size = n, replace = TRUE, prob = densidad_post))\n}\n\nY ahora podemos ver cómo se ve la posterior:\n\nset.seed(328)\nuna_muestra &lt;- sim_pos_neg(theta = 0.2, N = 600, sens = 0.6, esp = 0.999)\nmean(una_muestra$Pos)\n\n[1] 0.1233333\n\nsims_post_error &lt;- \n  simular_posterior_error(una_muestra$Pos, 5000, sens = 0.6, esp = 0.999) \nsims_post_error |&gt;\n  ggplot(aes(x = theta)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nAhora seguimos el flujo. Agregaremos la verificación a priori para entender si nuestro modelo recupera los parámetros.\n\nset.seed(8112)\nsimulacion_rep_error &lt;- map_df(1:20, \n    function(rep){\n      # simular de la apriori\n      theta_sim &lt;- rbeta(1, 1, 3)\n      # simular datos según modelo\n      datos_sim &lt;- sim_pos_neg(theta = theta_sim, N = 150, sens = 0.6, esp = 0.999)\n      # simulaciones montecarlo para la posterior\n      posterior &lt;- simular_posterior_error(datos_sim$Pos, 10000, sens = 0.6, esp = 0.999)\n      # junta todo\n      posterior |&gt; mutate(n_sim = n()) |&gt;\n        mutate(rep = rep) |&gt;\n        mutate(theta_sim = theta_sim)\n    })\n\nAhora usamos histogramas por ejemplo para mostrar cómo luce la posterior, y comparamos con los valores de la simulación:\n\nggplot(simulacion_rep_error, aes(x = theta)) +\n  geom_histogram(bins = 50) +\n  labs(x = \"theta\", y = \"Prob posterior\") +\n  geom_vline(aes(xintercept = theta_sim), color = \"red\", linetype = \"dashed\") +\n  facet_wrap(~rep)\n\n\n\n\n\n\n\nFigura 3.1: Verificación a priori\n\n\n\n\n\nContrasta con lo que pasaría si usaramos el modelo sin considerar fuentes de error:\n\nset.seed(812)\nsimulacion_rep &lt;- map_df(1:20, \n    function(rep){\n      # simular de la apriori\n      theta_sim &lt;- rbeta(1, 1, 3)\n      # simular datos según modelo\n      datos_sim &lt;- sim_pos_neg(theta = theta_sim, N = 150, sens = 0.6, esp = 0.999)\n      # simulaciones montecarlo para la posterior\n      posterior &lt;- simular_posterior_error(datos_sim$Pos, 10000, 1, 1)\n      # junta todo\n      posterior |&gt; mutate(n_sim = n()) |&gt;\n        mutate(rep = rep) |&gt;\n        mutate(theta_sim = theta_sim)\n    })\n\n\nggplot(simulacion_rep, aes(x = theta)) +\n  geom_histogram(bins = 50) +\n  labs(x = \"theta\", y = \"Prob posterior\") +\n  geom_vline(aes(xintercept = theta_sim), color = \"red\", linetype = \"dashed\") +\n  facet_wrap(~rep)\n\n\n\n\n\n\n\nFigura 3.2: Verificación a priori fallida (modelo incorrecto)\n\n\n\n\n\nEste resultado está lejos de ser aceptable.\nComparamos esta densidad con lo que obtendríamos sin considerar el error de medición, con los mismos datos:\n\nset.seed(8)\nsims_post &lt;- \n  simular_posterior_error(una_muestra$Pos, 5000, 1, 1)\nambas_sims_tbl &lt;- \n  sims_post_error |&gt;\n  mutate(tipo = \"Con error de medición\") |&gt;\n  bind_rows(sims_post |&gt;\n              mutate(tipo = \"Sin error de medición\"))\nambas_sims_tbl |&gt; ggplot(aes(x = theta, fill = tipo)) +\n  geom_histogram(position = \"identity\", alpha = 0.5, bins = 50) +\n  scale_fill_manual(values = c(\"red\", \"blue\")) +\n  geom_vline(xintercept = 0.2, linetype = \"dashed\", color = \"black\")\n\n\n\n\n\n\n\n\nY vemos que la diferencia entre las distribuciones es considerable. En primer lugar, la distribución con error de medición es más ancha (hay más incertidumbre). En segundo lugar, como estimador de el parámetro de interés, nuestro modelo que no considera el error parece dar estimaciones sesgadas hacia abajo. Esto es porque la prevalencia no es tan baja, y la sensibilidad de la prueba no es muy buena, de manera que con el modelo con error inferimos correctamente que hay más prevalencia que lo que indicaría la proporción de positivos en las pruebas.\nAunque este ejemplo es claro, prevalencia, sensibilidad y especificidad interactúan de maneras a veces poco intuitivas.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Flujo de trabajo básico: refinando el modelo</span>"
    ]
  },
  {
    "objectID": "02-flujo-basico-2.html#prevalencia-con-datos-de-referencia",
    "href": "02-flujo-basico-2.html#prevalencia-con-datos-de-referencia",
    "title": "3  Flujo de trabajo básico: refinando el modelo",
    "section": "3.2 Prevalencia con datos de referencia",
    "text": "3.2 Prevalencia con datos de referencia\nAhora haremos un paso adicional: los valores de sensibilidad y especificidad generalmente no son conocidos con certeza, sino que son estimados a partir de una muestra de “estándar de oro”. En esta prueba particular, el kit identificó correctamente como positivos a 103 de 122 personas infectadas, e identificó correctamente como negativos a 399 de 401 personas no infectadas. Consideraremos 122 y 401 como tamaños de muestra fijos y conocidos (las personas fueron extraídas de otra población).\nDenotamos como \\(Ref\\) a los datos de referencia de “estándar de oro”.\n\n\nCódigo\ngrViz(\"\ndigraph {\n  graph [ranksep = 0.3, rankdir = LR]\n  node [shape=circle]\n    theta\n    esp\n    sens\n    Npos [label = &lt;N&lt;SUB&gt;+&lt;/SUB&gt;&gt;]\n  node [shape=plaintext]\n    Nobs [label = &lt;N&lt;SUB&gt;obs&lt;/SUB&gt;&gt;]\n   # Nneg [label = &lt;N&lt;SUB&gt;-&lt;/SUB&gt;&gt;]\n  edge [minlen = 3]\n    theta -&gt; Npos\n    #p -&gt; Nneg\n    N -&gt; Npos\n    Npos -&gt; Nobs\n    #N -&gt; Nneg\n    esp -&gt; Nobs\n    sens -&gt; Nobs\n    #esp -&gt; Nneg\n    #sens -&gt; Nneg\n    esp -&gt; Ref\n    sens -&gt; Ref\n{ rank = same; theta; N }\n#{ rank = same; Npos; Nneg}\n{ rank = max; sens; esp}\n}\n\")#, width = 200, height = 50)\n\n\n\n\n\n\nUsando argumentos como los del modelo original, las distribuciones de esp y sens son beta y podemos incorporarlas en la simulación de la posterior. Nuestra nueva función para simular el proceso generativo es:\n\nsim_pos_neg &lt;- function(p = 0.01, N = 20, pos_gold = c(103,122), neg_gold = c(399,401)) {\n  # Simular especificidad y sensibilidad\n  sens &lt;- rbeta(1, pos_gold[1] + 1, pos_gold[2] - pos_gold[1] + 1)\n  esp &lt;- rbeta(1, neg_gold[1] + 1, neg_gold[2] - neg_gold[1] + 1)\n  # verdaderos positivos que capturamos en la muestra\n  Pos_verdadero &lt;- rbinom(N, 1, p)\n  Neg_verdadero &lt;- 1 - Pos_verdadero\n  # positivos observados en la muestra: si es positivo, calculamos\n  # la probabilidad de que realmente sea positivo\n  sim_tbl &lt;- tibble(Pos_verdadero, Neg_verdadero) |&gt; \n    mutate(Pos = rbinom(N, 1, Pos_verdadero * sens + Neg_verdadero * (1-esp))) |&gt; \n    mutate(Neg = 1 - Pos)\n  # Observaciones\n  sim_tbl |&gt; select(Pos, Neg)\n}\n\nConsiderando que tenemos tres parámetros, en este punto decidimos no hacer la aproximación de rejilla. Es posible hacer otro tipo de aproximaciones (por ejemplo cuadráticas), pero en lugar de esto veremos cómo lo haríamos con Stan. Más adelante discutiremos los algoritmos que Stan utiliza para simular de la posterior de modelos muy generales. Por el momento, notamos que está basado en un algoritmo de simulación MCMC (Markov Chain Montecarlo), que es el estándar para modelos que no son muy simples. Este ejemplo es para ilustrar cómo resolveríamos el problema más general, no es necesario que en este punto entiendas cómo funciona o los detalles de la implementación.\n\nlibrary(cmdstanr)\nmod_sc &lt;- cmdstan_model(\"./src/sclara.stan\")\nprint(mod_sc)\n\ndata {\n  int&lt;lower=0&gt; N;\n  int&lt;lower=0&gt; n;\n  int&lt;lower=0&gt; kit_pos;\n  int&lt;lower=0&gt; n_kit_pos;\n  int&lt;lower=0&gt; kit_neg;\n  int&lt;lower=0&gt; n_kit_neg;\n}\n\nparameters {\n  real&lt;lower=0, upper=1&gt; theta; //seroprevalencia\n  real&lt;lower=0, upper=1&gt; sens; //sensibilidad\n  real&lt;lower=0, upper=1&gt; esp; //especificidad\n}\n\ntransformed parameters {\n  real&lt;lower=0, upper=1&gt; prob_pos;\n\n  prob_pos = theta * sens + (1 - theta) * (1 - esp);\n\n}\nmodel {\n  // modelo de número de positivos\n  n ~ binomial(N, prob_pos);\n  // modelos para resultados del kit\n  kit_pos ~ binomial(n_kit_pos, sens);\n  kit_neg ~ binomial(n_kit_neg, esp);\n  // iniciales para cantidades no medidas\n  theta ~ beta(1.0, 10.0);\n  sens ~ beta(2.0, 1.0);\n  esp ~ beta(2.0, 1.0);\n}\n\n\n\nn &lt;- 50\nN &lt;- 3300\ndatos_lista &lt;- list(N = 3300, n = 50,\n kit_pos = 103, n_kit_pos = 122,\n kit_neg = 399, n_kit_neg = 401)\najuste &lt;- mod_sc$sample(data = datos_lista, refresh = 1000)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.0 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 0.0 seconds.\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 0.0 seconds.\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.6 seconds.\n\nsims &lt;- ajuste$draws(c(\"theta\", \"sens\", \"esp\"), format = \"df\")\nresumen &lt;- ajuste$summary(c(\"theta\"))\n\n\nresumen |&gt; select(variable, mean, q5, q95)\n\n# A tibble: 1 × 4\n  variable   mean      q5    q95\n  &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 theta    0.0104 0.00243 0.0174\n\n\nY podemos graficar la posterior de la seroprevalencia:\n\nggplot(sims, aes(x = theta)) + \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nY vemos que los datos son consistentes con el dato reportado por los autores (alrededor de 1.2%), pero que no podemos excluir valores de prevalencia muy bajos (por abajo de 0.3% por ejemplo). Por otro lado, también son consistentes valores muy altos de seroprevalencia, de manera que este estudio resultó ser poco informativo de la IFR del COVID.\nPodemos hacer diagnósticos adicionales acerca de la razón de esta variabilidad alta, si graficamos la relación entre especificidad de la prueba y estimación de prevalencia:\n\nggplot(sims, aes(x = esp, y = theta)) + geom_point() +\n  xlab(\"Especificidad del kit\") + ylab(\"Prevalencia\") + geom_smooth()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nLa asociación entre estas dos cantidades es interesante porque conceptualmente (y desde punto de vista del modelo), no hay relación entre estas dos variables: su asociación aparece porque son causas que compiten para explicar una observación.\nNótese que dada la prevalencia baja, la especificidad del kit es un factor importante para explicar la prevalencia observada, pero si no pensamos con cuidado podríamos concluir que los falsos positivos no deberían ser problema por que la especificidad para ser muy buena.\nY notamos que aún con una muestra relativamente grande, el rango de \\(\\theta\\) es considerable: va desde valores cercanos a 0 hasta valores alrededor de 0.025-0.03.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Flujo de trabajo básico: refinando el modelo</span>"
    ]
  },
  {
    "objectID": "03-modelos-genericos.html",
    "href": "03-modelos-genericos.html",
    "title": "4  Componentes de modelación 1",
    "section": "",
    "text": "4.1 Predicciones sin explicación\nEs posible obtener buenas predicciones con modelos estadísticos genéricos sin tener una explicación de cómo funciona el fenómeno que estamos modelando. Estos modelos, aunque pueden resultar en predicciones muy buenas y ser útiles, pueden ser riesgosos si se interpretan fuera de un contexto teórico con supuestos claros.\nEl primer ejemplo es de nuestra referencia de McElreath (2020): los epicilos planetarios del modelo geocéntrico para explicar el movimiento retrógrado de planetas en el cielo. Este modelo fue exitoso y muy preciso para calcular las posiciones futuras de los planetas en el cielo, pero sus fundamentos eran incorrectos: no es posible interpretar este modelo por sí solo para entender cómo funciona el sistema solar.\nModelos genéricos como regresión lineal o logística, métodos basados en árboles, redes neuronales típicamente caen en esta categoría de modelos de tipo “geocéntrico”: aunque pueden ser efectivos para predecir, debemos ser cuidadosos en su interpretación en términos de causas, efectos, y mecanismos del fenómeno que nos interesa.\nPodemos aplicar con éxito estas componentes de modelación si entendemos su papel: estos modelos genéricos no contienen o nos dan causas o mecanismos por sí solos, pero pueden ayudarnos extraer información causal bajo los supuestos apropiados.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Componentes de modelación 1</span>"
    ]
  },
  {
    "objectID": "03-modelos-genericos.html#ejemplo-regresión-lineal",
    "href": "03-modelos-genericos.html#ejemplo-regresión-lineal",
    "title": "4  Componentes de modelación 1",
    "section": "4.2 Ejemplo: regresión lineal",
    "text": "4.2 Ejemplo: regresión lineal\nEn este ejemplo, introducimos notación para representar modelos, usaremos posteriores con varios parámetros, y veremos cómo construir y aplicar modelos lineales, todo desde el punto de vista de nuestro flujo de trabajo.\nEn este ejemplo de McElreath (2020) queremos describir la relación entre peso y estatura de adultos de una población relativamente homogénea. Nuestro modelo causal es como sigue:\n\nEn primer lugar, la estatura (\\(H\\)) de las personas adultas influye en su peso (\\(W\\)). El peso, está influenciado también por otras variables no observadas:\n\n\n\nCódigo\ngrViz(\"\ndigraph {\n  graph [ranksep = 0.3, rankdir = LR]\n  node [shape=circle]\n    U\n  node [shape=plaintext]\n    H\n    W\n  edge [minlen = 3]\n    H -&gt; W\n    U -&gt; W\n}\n\")#, width = 200, height = 50)\n\n\n\n\n\n\nNótese que no consideramos \\(W\\to H\\), porque podemos pensar en varias intervenciones que podrían cambiar el peso por no cambian la estatura. Por otro lado, es difícil pensar en alguna intervención que cambie la estatura pero no cambie el peso de una persona. Adicionalmente, hay otros factores desconocidos no observados \\(U\\) que afectan el peso de cada persona adicionalmente a su estatura.\nAhora pasamos la modelo generativo. Supondremos que el peso de una persona adulta depende de su estatura de manera lineal, de forma que podemos escribir:\n\\[W = \\alpha + \\beta H + U\\] Como \\(U\\) no es observada, tenemos que definir cómo generar esta variable. Una distribución natural para esta variable es una distribución normal con media 0 y desviación estándar \\(\\sigma\\) no conocida, que escribimos como \\(U\\sim N(0,\\sigma)\\). Tenemos entonces que dados los valores \\(\\alpha\\) y \\(\\beta\\),\n\\[E[W|H] = \\alpha + \\beta H,\\] así que el valor esperado del peso de una persona, dada su estatura, es una función lineal de la estatura. La variable \\(U\\) representa la variabilidad en peso alrededor de este valor esperado. Usamos la distribución normal considerando que es una agregación de varias perturbaciones pequeñas, no relacionadas con estatura, que afectan el peso de una persona (aunque este supuesto también tiene que validarse).\nAdicionalmente, tenemos que hacer supuestos acerca del proceso generador para la estatura \\(H\\). Por el momento, y para ejemplificar, supondremos que la estatura es una variable normal con media en 160 cm y desviación estándar de 10cm.\nEmpezamos a escribir nuestro modelo generativo:\n\nsim_peso &lt;- function(n= 10, alpha, beta, sigma){\n  # simular estatura\n  H &lt;- rnorm(n, 160, 10)\n  # simular perturbación de peso\n  U &lt;- rnorm(n, 0, sigma)\n  # regresión lineal de peso dado estatura\n  W &lt;- alpha + beta * H + U\n  tibble(H, W)\n}\n\nPodemos checar nuestro modelo generativo con simulaciones predictivas a priori: generamos una muestra y checamos con el conocimiento del área:\n\nset.seed(9)\nsim_peso(100, alpha = 0, beta = 0.5, sigma = 5) |&gt; \n  ggplot(aes(x = H, y = W)) +\n  geom_point() +\n  labs(x = \"Estatura (cm)\", y = \"Peso (kg)\")\n\n\n\n\n\n\n\n\nPodemos escribir este generativo siguiendo el código de la función de arriba. Si cada persona la denotamos por un índice \\(i\\) entonces:\n\\[\n\\begin{align}\nW_i &= \\alpha + \\beta H_i + U_i \\\\\nU_i &\\sim N(0,\\sigma) \\\\\nH_i &\\sim N(160, 10)\n\\end{align}\n\\] De lado izquierdo están las variables, del lado derecho las definiciones, igualdad significa una relación determinística y \\(\\sim\\) significa “se distribuye como”.\nAhora podemos plantear nuestra pregunta inicial en términos de este modelo: nos interesa describir cómo cambia el peso esperado de una persona dependiendo de su estatura, es decir, describir la recta\n\\[ \\alpha + \\beta H\\] Con esto hemos terminado los primeros paso de nuestro flujo (modelo causal, modelo generativo, cantidad a estimar).\nNuestro método de estimación es bayesiano, así que podemos ser más específicos y decir que nos interesa la distribución posterior de \\(\\alpha,\\beta\\) dado datos observados. Otra manera de decir esto es que nos interesa describir la posterior de la recta \\(\\alpha + \\beta H\\).\nNótese en particular que en este ejemplo no nos interesa el proceso generador de \\(H\\), y dado nuestro diagrama causa, podemos considerar el análisis condicional a los valores que observamos de estatura.\nAhora podemos plantear nuestra estrategia de modelación estadística. Tenemos tres parámetros desconocidos \\(\\alpha,\\beta,\\sigma\\), y tenemos por la regla de bayes la posterior está dada por:\n\\[p(\\alpha,\\beta,\\sigma|W_i,H_i)\\propto p(W_i|H_i, \\alpha,\\beta,\\sigma)p(\\alpha,\\beta,\\sigma)\\] De modo que sólo nos interesa entender como es el peso condicional a la estatura, y por eso nuestra verosimilitud sólo considera \\(W_i\\) condicional a \\(H_i\\) (desde el punto de vista del diagrama, nos interesa modelar el nodo \\(W\\). En otros casos, quizá buscaríamos modelar la distribución conjunta de \\(W_i,H_i\\).\nAhora tenemos que poner distribuciones a priori \\(p(\\alpha, \\beta,\\sigma)\\) para los parámetros desconocidos, y continuar con nuestro flujo de modelación haciendo verificaciones a priori.\n\n\n\n\n\n\nDistribuciones a priori\n\n\n\nLa propuesta de distribuciones a priori depende de manera cercana de nuestras verificaciones a prior, como veremos más adelante. En general no existen distribuciones a priori “correctas”, sino justificables desde el punto de vista del conocimiento del área.\nLos chequeos a priori nos permite entender las consecuencias de nuestras decisiones acerca de las iniciales. Nótese que todo este trabajo se hace antes de ver los datos, lo que implica que no estamos buscando “sacar el resultado que queremos” de los datos.\n\n\n\n4.2.1 Distribuciones a priori\nEn primer lugar, haremos este trabajo más fácil si parametrizamos la recta de regresión de la siguiente manera, donde restamos a la estatura un valor típico de la distribución de estaturas (también puede usarse la media los datos, más comunmente, pero en este ejemplo tomamos un valor fijo para simplificar la explicación):\n\\[E[W|H] =  \\alpha + \\beta (H - 160)\\]\nLas apriori o iniciales expresan conocimiento del área (incluyendo las unidades que se están utilizando), y actúan como restricciones suaves. Supondremos que peso está en kilogramos y estatura en centímetros.\n\nCuando \\(H=160\\) esperamos que \\(W\\) esté alrededor de 50-70 kg, así que podemos centrar \\(\\alpha\\) en 60. Las unidades de \\(\\alpha\\) son kg. Una inicial (verificaremos dentro de un momento esa decisión) puede ser \\(\\alpha\\sim N(60, 10)\\). Recordemos que esto implica que \\(\\alpha\\) están dentre 60 - 2(10) = 40 y 60 + 10(2) = 80 con probabilidad 0.95, lo cual es considerable pero no excesivamente amplio para una persona de estatura 160cm.\nSi \\(\\alpha\\) es está alrededor de 60, la constante de proporcionalidad \\(\\beta\\) debe ser positiva, y no muy lejana de un valor entre 0 y 2. La razón es que no tiene sentido esperar que un aumento de 10 cm tenga un aumento esperado de peso de 20 kilos, por ejemplo. Podríamos poner una inicial como \\(\\beta\\sim N^+(0, 1)\\) (normal truncada en 0) por ejemplo. Esta distribución tiene los siguientes percentiles:\n\n\nquantile(rnorm(10000, 0, 1) |&gt; abs(), probs = seq(0, 1, 0.1)) |&gt; \n  round(3)\n\n   0%   10%   20%   30%   40%   50%   60%   70%   80%   90%  100% \n0.000 0.124 0.251 0.383 0.518 0.673 0.839 1.032 1.269 1.618 3.818 \n\n\nFinalmente, tenemos que poner una inicial para \\(\\sigma\\). Debe ser positiva, y representa la variabilidad que hay en el peso que no se debe a la estatura. Una inicial razonable es \\(\\sigma\\sim N^+(0, 20)\\), por ejemplo.\nQuedamos entonces con:\n\\[\n\\begin{align}\nW_i &= \\alpha + \\beta (H_i - 160) + U_i \\\\\nU_i &\\sim N(0,\\sigma) \\\\\n\\alpha &\\sim N(60, 10) \\\\\n\\beta &\\sim N^+(0, 1) \\\\\n\\sigma &\\sim N^+(0, 20) \\\\\n\\end{align}\n\\]\n\n\nChequeo predictivo a priori\nAhora podemos simular de la a priori cuáles son las posibilidades que estamos considerando. Utilizaremos valores razonables simulados de \\(H\\) para hacer el análisis.\n\nsim_peso_mod &lt;- function(n= 10){\n  alpha &lt;- rnorm(1, 60, 10)\n  beta &lt;- rnorm(1, 0, 1) |&gt; abs()\n  sigma &lt;- rnorm(1, 0, 20) |&gt; abs()\n  \n  # simular estaturas y pesos\n  H &lt;- rnorm(n, 160, 10)\n  mu_W = alpha + beta * (H - 160)\n  # simular perturbación de peso\n  U &lt;- rnorm(n, 0, sigma)\n  # regresión lineal de peso dado estatura\n  W &lt;- mu_W + U\n  tibble(alpha, beta, sigma, H, W)\n}\n\nY hacemos varias replicaciones:\n\nsims_tbl &lt;- map_df(1:20, function(rep) {\n  sim_peso_mod(100) |&gt; mutate(rep = rep)\n})\n\nNuestros supuestos actuales se ven como sigue:\n\nsims_tbl |&gt; \n  ggplot(aes(x = H, y = W)) +\n  geom_point() +\n  geom_abline(aes(intercept = alpha - 160 * beta, slope = beta), data = sims_tbl, color = \"red\") +\n  labs(x = \"Estatura (cm)\", y = \"Peso (kg)\") +\n  facet_wrap(~rep)\n\n\n\n\nSimulaciones predictivas a priori\n\n\n\n\n**Observación*: Esto parece ser razonable, aunque algunas replicaciones son algo extremas (muy poca variabilidad de peso, una relación muy débil o. muy fuerte entre estatura y peso). Comenzaremos con este modelo y seguiremos explorando sus consecuencias.\nNótese que en esta situación, un punto de vista que aparentemente es “conservador” en efecto pone peso en resultados que son infactibles del todo. Por ejemplo, si pusiéramos \\(\\beta\\sim N^+(0,100)\\) y \\(\\sigma\\sim N^+(0, 1000)\\), bajo el argumento de que no tenemos información acerca de \\(\\beta\\) o \\(\\sigma\\), obtendríamos:\n\n\nCódigo\nsim_peso_mod_mal &lt;- function(n= 10){\n  alpha &lt;- rnorm(1, 60, 10)\n  beta &lt;- rnorm(1, 0, 100) |&gt; abs()\n  sigma &lt;- rnorm(1, 0, 1000) |&gt; abs()\n  \n  # simular estaturas y pesos\n  H &lt;- rnorm(n, 160, 10)\n  mu_W = alpha + beta * (H - 160)\n  # simular perturbación de peso\n  U &lt;- rnorm(n, 0, sigma)\n  # regresión lineal de peso dado estatura\n  W &lt;- mu_W + U\n  tibble(alpha, beta, sigma, H, W)\n}\nsims_mal_tbl &lt;- map_df(1:20, function(rep) {\n  sim_peso_mod_mal(100) |&gt; mutate(rep = rep)\n})\nsims_mal_tbl |&gt; \n  ggplot(aes(x = H, y = W)) +\n  geom_point() +\n  geom_abline(aes(intercept = alpha - 160 * beta, slope = beta), data = sims_mal_tbl, color = \"red\") +\n  labs(x = \"Estatura (cm)\", y = \"Peso (kg)\") +\n  facet_wrap(~rep)\n\n\n\n\n\nModelo no realista\n\n\n\n\nEsta es una prueba inicial fallida. Regresaremos a este ejemplo más adelante. En este ejemplo particular que es muy simple y como veremos no es grave permitir algunos resultados algo extremos.\n\n\n\n\n\n\nTip\n\n\n\nEs riesgoso permitir valores de las apriori que no son consistentes con el conocimiento del área. Cuando tenemos muchos datos y relativamente pocos parámetros no es muy importante, pero conforme vayamos avanzando veremos que utilizar supuestos no realistas (como distribuiciones no informativas para los parámetros) tiene consecuencias considerables.\n\n\n\n\nModelo en Stan\nAunque en este punto es posible todavía hacer una aproximación por rejilla (sólo tenemos tres parámetros), escribiremos el modelo en Stan y simularemos de la posterior con MCMC (recuerda que más tarde explicaremos este proceso).\n\nNuestro objetivo ahora es calcular la posterior bajo los supuestos de arriba para datos generados de nuestro modelo, y ver si los cálculos funcionan apropiadamente.\n\nEl modelo en Stan se puede escribir como sigue:\n\nlibrary(cmdstanr)\nmod_peso &lt;- cmdstan_model(\"./src/peso-estatura-1.stan\")\nprint(mod_peso)\n\ndata {\n  int&lt;lower=0&gt; N;\n  vector[N]  h;\n  vector[N]  w;\n}\n\nparameters {\n  real alpha;\n  real &lt;lower=0&gt; beta;\n  real &lt;lower=0&gt; sigma;\n}\n\ntransformed parameters {\n  vector[N] w_media;\n  // determinístico dado parámetros\n  w_media = alpha + beta * (h - 160);\n}\n\nmodel {\n  // partes no determinísticas\n  w ~ normal(w_media, sigma);\n  alpha ~ normal(60, 10);\n  beta ~ normal(0, 1);\n  sigma ~ normal(0, 20);\n}\n\n\nUna vez que escribimos nuestro modelo, hacemos nuestra siguiente verificación a priori: dados los supuestos del modelo generativo, ¿nuestro estimador funciona apropiadamente para responder la pregunta de interés?\nUtilizaremos una muestra de 350 personas simulada de nuestro proceso generador de datos. En cada caso, buscamos correr nuestro modelo y checar que nuestra estimación de la recta de regresión es consistente con los valores que utilizamos para generar los datos.\n\nset.seed(881)\nsims_inicial_check_tbl &lt;- map_df(1:10, function(rep) {\n  sim_peso_mod(352) |&gt; mutate(rep = rep)\n}) |&gt; nest(datos_sim = c(H, W))\nsims_tbl\n\n# A tibble: 2,000 × 6\n   alpha  beta sigma     H     W   rep\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n 1  61.0 0.203  22.9  165.  37.1     1\n 2  61.0 0.203  22.9  176.  52.4     1\n 3  61.0 0.203  22.9  154.  37.2     1\n 4  61.0 0.203  22.9  162.  70.2     1\n 5  61.0 0.203  22.9  156.  22.5     1\n 6  61.0 0.203  22.9  158.  54.8     1\n 7  61.0 0.203  22.9  173.  77.1     1\n 8  61.0 0.203  22.9  174.  56.9     1\n 9  61.0 0.203  22.9  150.  41.4     1\n10  61.0 0.203  22.9  150.  41.6     1\n# ℹ 1,990 more rows\n\n\nEn la siguiente gráfica vemos un comportamiento razonable de nuestro proceso de estimación. Recuerda que cada punto negro representa una simulación de la posterior, y el punto rojo es el valor que usamos para hacer la simulación de cada recuadro:\nOjo: los ejes de los recuadros varían con la simulación\n\n# fig-cap: Posterior para datos simulados\ndatos_check_tbl &lt;- sims_inicial_check_tbl |&gt; \n  select(rep, post_tbl) |&gt; \n  unnest(post_tbl)\nggplot(datos_check_tbl, aes(x = alpha, y = beta)) +\n  geom_point(alpha = 0.2) +\n  labs(x = \"alpha\", y = \"beta\") +\n  geom_point(data = sims_inicial_check_tbl, color = \"red\", size = 3) +\n  facet_wrap(~ rep, scales = \"free\") \n\n\n\n\n\n\n\n\nEsta gráfica también podemos hacerla como sigue, usando rectas:\n\n# nota: el intercept en geom_abline es la ordenada al origen\n# mientras que la alpha es valor de la recta para h = 160\ndatos_check_tbl |&gt; \n  ggplot() +\n  geom_abline(aes(intercept = alpha - beta * 160, slope = beta),\n              alpha = 0.5, colour = \"gray\") +\n  facet_wrap(~ rep, scales = \"free\") +\n  scale_x_continuous(limits = c(130, 200)) + \n  scale_y_continuous(limits = c(0, 200)) +\n  geom_abline(data = sims_inicial_check_tbl, \n    aes(intercept = alpha - beta * 160, \n      slope = beta), color = \"red\", linewidth = 1.05)\n\n\n\n\n\n\n\n\nEsta prueba computacional tiene buenos resultados: en general, la posterior se concentra alrededor del valor verdadero de cada simulación. Ahora podemos proceder a cargar los datos reales y hacer una simulación de la posterior.\n\n\n4.2.2 Ajustando el modelo a los datos reales\nCargamos los datos y producimos simulaciones de la posterior.\n\nset.seed(81)\ndatos_tbl &lt;- read_delim(\"../datos/Howell1.csv\", delim = \";\") |&gt; \n  filter(age &gt;= 18)\n\nRows: 544 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\ndbl (4): height, weight, age, male\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\ndata_list &lt;- list(\n  N = nrow(datos_tbl),\n  h = datos_tbl$height,\n  w = datos_tbl$weight\n)\n# correr modelo en Stan\nmod_peso_ajuste &lt;- mod_peso$sample(\n  data = data_list,\n  iter_sampling = 2000,\n  refresh = 0)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.4 seconds.\nChain 2 finished in 0.4 seconds.\nChain 3 finished in 0.4 seconds.\nChain 4 finished in 0.4 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.4 seconds.\nTotal execution time: 1.7 seconds.\n\n\nLa posterior de los parámetros de la recta se ve como siguen:\n\nsims_peso_post_tbl &lt;- mod_peso_ajuste |&gt; \n  as_draws_df() |&gt; \n  select(.draw, alpha, beta, sigma) \n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\n\nggplot(sims_peso_post_tbl)+\n  geom_point(aes(x = alpha, y = beta)) \n\n\n\n\n\n\n\n\nY un resumen simple está dado por:\n\nmod_peso_ajuste$summary(c(\"alpha\", \"beta\", \"sigma\")) |&gt; \n  mutate(across(where(is.numeric),  ~ round(.x, 2)))\n\n# A tibble: 3 × 10\n  variable  mean median    sd   mad    q5   q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 alpha    48.4   48.4   0.28  0.28 47.9  48.8      1    4900.    5333.\n2 beta      0.63   0.63  0.03  0.03  0.58  0.68     1    4700.    5066.\n3 sigma     4.26   4.25  0.16  0.16  4     4.53     1    5690.    5133.\n\n\n\n\n\n\n\n\nSimulaciones conjuntas\n\n\n\nObserva que los resúmenes marginales (variable por variable) no cuentan la historia completa de la posterior. En nuestro ejemplo, \\(\\alpha\\) y \\(\\beta\\) están correlacionadas en la posterior como muestra la gráfica anterior. Por eso cuando queremos calcular resúmenes de cantidades en las que influyen varios parámetros, es importante trabajar con las simulaciones conjuntas de los parámetros.\n\n\nPuedes ver que en realidad no es posible calcular la distribución de cantidades como \\(\\alpha + 10\\beta\\), por ejemplo, a partir de la información de la tabla de arriba: por ejemplo, la varianza de esta suma no es simplemente la suma de las varianzas",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Componentes de modelación 1</span>"
    ]
  },
  {
    "objectID": "03-modelos-genericos.html#distribución-predictiva-posterior",
    "href": "03-modelos-genericos.html#distribución-predictiva-posterior",
    "title": "4  Componentes de modelación 1",
    "section": "4.3 Distribución predictiva posterior",
    "text": "4.3 Distribución predictiva posterior\nDado nuestro modelo, ahora podemos generar cómo se verían observaciones nuevas: en este caso, si tuviéramos una estatura, ¿cómo sería el peso de esa persona? Para esto tenemos que tener en cuenta tanto la posterior de los parámetros como el modelo de los datos.\nEn primer lugar, la posterior de la relación lineal es (cada línea de esta gráfica es una simulación de la posterior):\n\nggplot(sims_peso_post_tbl) +\n  geom_abline(aes(intercept = alpha - beta * 160, slope = beta),\n              colour = \"gray\", alpha = 0.1) +\n  scale_x_continuous(limits = c(130, 180)) +\n  scale_y_continuous(limits = c(20, 70)) \n\n\n\n\n\n\n\n\nEsto nos indican los valores esperados para estatura. Para la predictiva posterior, también tenemos que considerar dónde pueden aparecer individuos. Para simular a una estatura fija, por ejemplo, hacemos lo sugiente:\n\nsim_pred_post &lt;- function(n, sims_peso_post_tbl, h) {\n  # extraer parámetros de la posterior\n  pars &lt;- slice_sample(sims_peso_post_tbl, n = n, replace = TRUE) \n  # simular pesos\n  sims_tbl &lt;- map_df(h, function(h){\n    w_media &lt;- pars$alpha + pars$beta * (h - 160)\n    w &lt;- rnorm(n, w_media, pars$sigma)\n    tibble(rep = 1:n, h = h, w_media = w_media, w = w)\n  })\n  sims_tbl\n}\n\nLas predictivas posteriores para las estaturas \\(h = 160\\) y \\(h=150\\) son:\n\ncomp_ppost_tbl &lt;- sim_pred_post(5000, sims_peso_post_tbl, c(150, 160))\nggplot(comp_ppost_tbl, aes(x =  w, fill = factor(h))) +\n  geom_histogram(bins = 50, alpha = 0.5, position = \"identity\") \n\n\n\n\n\n\n\n\nque como vemos presentan variabilidad considerable más allá de la diferencia de valores esperados. Podemos calcular por ejemplo cuál es la probabilidad de que una persona de 150 cm sea más alta que una de 170 cm:\n\ncomp_ppost_tbl |&gt; \n  select(-w_media) |&gt; \n  pivot_wider(names_from = h, values_from = w) |&gt; \n  summarise(prop = mean(`150` &gt; `160`))\n\n# A tibble: 1 × 1\n   prop\n  &lt;dbl&gt;\n1 0.143\n\n\nY también es más útil calcular la distribución de la diferencia de pesos entre personas de 150 y 160 cm:\n\ncomp_ppost_tbl |&gt; \n  select(-w_media) |&gt; \n  pivot_wider(names_from = h, values_from = w) |&gt; \n  mutate(diferencia = `160` - `150`) |&gt; \nggplot(aes(x = diferencia)) +\n  geom_histogram(bins = 50, alpha = 0.5, position = \"identity\") +\n  labs(x = \"Diferencia de pesos 160 - 150cm\")\n\n\n\n\n\n\n\n\nEn contraste, si comparamos las estaturas medias de cada grupo, que no incluyen variabilidad individual más allá de la producida por la estatura:\n\nggplot(comp_ppost_tbl, aes(x =  w_media, fill = factor(h))) +\n  geom_histogram(bins = 100, alpha = 0.5, position = \"identity\") +\n  labs(x = \"Media de pesos\")\n\n\n\n\n\n\n\n\nPrácticamente tenemos seguridad que la media de pesos de los individuos de 150 cm es menor que la de los de 160 cm.\n\n\n\n\n\n\nTip\n\n\n\nEs importante no confundir el contraste a nivel individuo que hicimos arriba, y el que hicimos a nivel de medias de grupos. Los grupos tienen claramente medias diferentes, pero las distribuciones de individuos tienen traslape considerable.\n\n\nPodemos también resumir la distribución predictiva para distintas estaturas:\n\nresumen_ppost_tbl &lt;- sim_pred_post(20000, sims_peso_post_tbl, \n                                   h = seq(130, 180, 2.5)) |&gt; \n  group_by(h) |&gt; \n  summarise(q5 = quantile(w, 0.05), \n            q95 = quantile(w, 0.95),\n            q_media_5 = quantile(w_media, 0.05),\n            q_media_95 = quantile(w_media, 0.95)) \n\nEn nuestra gráfica anterior tendríamos entonces nuestra recta junto con rangos de 90% para la estatura de los individuos:\n\nggplot(sims_peso_post_tbl) +\n  geom_abline(aes(intercept = alpha - beta * 160, slope = beta),\n              colour = \"gray\", alpha = 0.01) +\n  scale_x_continuous(limits = c(130, 180)) +\n  scale_y_continuous(limits = c(10, 80)) + \n  geom_ribbon(data = resumen_ppost_tbl,\n    aes(x = h, ymin = q_media_5, ymax = q_media_95),\n      fill = NA, colour = \"gray\") + \n  geom_ribbon(data = resumen_ppost_tbl,\n    aes(x = h, ymin = q5, ymax = q95), fill = NA, colour = \"red\")\n\n\n\n\n\n\n\n\n\n4.3.1 Verificaciones de predictiva posterior\nAdemás de ser útil para calcular cantidades de interés de manera natural, la predictiva posterior también está sujeta a crítica y validación. Veremos más de este punto, pero la idea básica es la siguiente:\n\n\n\n\n\n\nVerificaciones de predictiva posterior\n\n\n\nUna vez que tenemos la posterior dados los datos:\n\nTomamos una simulación de todos los parámetros del modelo.\nSimulamos nuevas observaciones (una muestra del mismo tamaño) a partir de los parámetros simulados.\nComparamos los datos simulados con los datos observados (gráficas u otros resúmenes apropiados).\nRepetimos para varias simulaciones.\n\nDiferencias sistemáticas entre datos observados y datos simulados de la predictiva posterior indican fallas del modelo o áreas donde puede mejorar.\n\n\nHay muchas variaciones de este tipo de verificaciones. En nuestro caso podemos hacer manualmente este proceso una vez que tenemos simulaciones de la posterior, tomando las mismas estaturas de los datos y simulando datos del modelo para el peso.\n\nsims_check_post &lt;- sim_pred_post(10, sims_peso_post_tbl, \n                                 h = datos_tbl$height) |&gt; \n  select(-w_media)\nsims_check_post &lt;- bind_rows(sims_check_post, \n   datos_tbl |&gt; mutate(rep = 11) |&gt; select(rep, h = height, w = weight)) |&gt; \n  mutate(rep = digest::digest2int(as.character(rep), seed = 992)) \nggplot(sims_check_post) +\n  geom_point(aes(x = h, y = w), alpha = 0.2) + facet_wrap(~ rep)\n\n\n\n\n\n\n\n\n¿Puedes reconocer dónde están los datos? Si hay desajustes graves y sistemáticas, deberías poder detectarlos en una gráfica de este tipo. Veremos más de este tipo de verificaciones en el curso.\n\n# Respuesta\ndigest::digest2int(\"11\", seed = 992)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Componentes de modelación 1</span>"
    ]
  },
  {
    "objectID": "03-modelos-genericos.html#ampliando-el-modelo",
    "href": "03-modelos-genericos.html#ampliando-el-modelo",
    "title": "4  Componentes de modelación 1",
    "section": "4.4 Ampliando el modelo",
    "text": "4.4 Ampliando el modelo\nEntre los adultos humanos, hombres y mujeres tienen distintas distribuciones de peso y estatura. La variable \\(S\\) (sexo) influye tanto en estatura como en peso. La relación la consideramos causalmente partiendo en \\(S\\):\n\n\nCódigo\ngrViz(\"\ndigraph {\n  graph [ranksep = 0.3, rankdir = LR]\n  node [shape=circle]\n    U\n    V\n    Z\n  node [shape=plaintext]\n    H\n    W\n    S\n  edge [minlen = 3]\n    H -&gt; W\n    U -&gt; W\n    S -&gt; H\n    S -&gt; W\n    V -&gt; H\n    Z -&gt; S\n}\n\", width = 200, height = 50)\n\n\n\n\n\n\nOmitiendo del diagrama las variables no observadas que también son causas únicamente de \\(S\\) y \\(W, H\\):\n\n\nCódigo\ngrViz(\"\ndigraph {\n  graph [ranksep = 0.3, rankdir = LR]\n  node [shape=circle]\n  \n  node [shape=plaintext]\n    H\n    W\n    S\n  edge [minlen = 3]\n    H -&gt; W\n    S -&gt; H\n    S -&gt; W\n\n}\n\", width = 200, height = 50)\n\n\n\n\n\n\nSi queremos saber cómo influye el sexo en el peso, este diagrama indica que hay dos tipos de preguntas que podemos hacer:\n\n¿Cuál es el efecto causal de \\(S\\) sobre \\(W\\) (efecto total) ?\n¿Cuál es el efecto causal directo de \\(S\\) sobre \\(W\\)? Es decir, que no actúa a través de \\(H\\).\n\nAunque tenemos un solo modelo causal, pueden construirse distintos modelos estadísticos para contestar cada pregunta. El modelo causal nos dice que si no tenemos causas comunes de \\(S\\) y \\(H\\) y \\(W\\), entonces podemos estimar el efecto total de \\(S\\) sobre \\(W\\) (esto lo formalizaremos más adelante).\nEmpezamos con el efecto total. Para esto, podemos usar el modelo lineal e ignorar la estatura, donde \\(S_i=2\\) si el individuo \\(i\\) es hombre y \\(S_i=1\\) si el individuo \\(i\\) es mujer.\n\\[\n\\begin{align}\nW_i &\\sim N(\\alpha_{S_i}, \\sigma)\\\\\n\\alpha_1,\\alpha_2 &\\sim N(60, 10) \\\\\n\\sigma &\\sim N^+(0, 20) \\\\\n\\end{align}\n\\] Nótese que tenemos dos posibles medias para el peso, una para hombres y otra para mujeres. La estatura no nos importa porque la pregunta es acerca del efecto total de sexo sobre estatura. Para las iniciales podemos seguir un argumento similar al de arriba.\nNota: esta parametrización es más conveniente que utilizar un indicador (o dummy) de sexo en términos de interpetación y en términos de poner iniciales acordes con el conocimiento del área, aunque estadísticamente son equivalentes.\nEl modelo generador simplificado para este caso puede ser:\n\nsim_peso_mod_s &lt;- function(S, alpha, sigma){\n  n &lt;- length(S)\n  W &lt;- rnorm(n, alpha[S], sigma)\n  tibble(alpha_1 = alpha[1], alpha_2 = alpha[2], \n         sigma, S = S, W = W)\n}\n\nDado este modelo generador, ¿cuál es el efecto causal de sexo? Tenemos que definir esta cantidad en términos del modelo. En nuestro caso, definiremos el efecto causal promedio sobre la población, que definimos como la diferencia promedio de estaturas de dos poblaciones: una compuesta enteramente por hombres y otra por mujeres.\n\nset.seed(2021)\n# Fjamos mismos valores de los parámetros para simular dos\n# poblaciones\nsim_hombres &lt;-  sim_peso_mod_s(rep(2, 1000), c(55, 70), 10)\nsim_mujeres &lt;-  sim_peso_mod_s(rep(1, 1000), c(55, 70), 10)\nmean(sim_hombres$W - sim_mujeres$W)\n\n[1] 14.75203\n\n\n\nVerificación a priori\nAhora generamos una población con estos parámetros y vemos si podemos recuperar el efecto causal promedio sobre la población. Nuestro modelo es como definimos arriba:\n\nlibrary(cmdstanr)\nmod_peso &lt;- cmdstan_model(\"./src/peso-estatura-2.stan\")\nprint(mod_peso)\n\ndata {\n  int&lt;lower=0&gt; N;\n  vector[N]  w;\n  array[N] int s;\n}\n\nparameters {\n  array[2] real alpha;\n  real &lt;lower=0&gt; sigma;\n}\n\ntransformed parameters {\n\n}\n\nmodel {\n  // modelo para peso\n  w ~ normal(alpha[s], sigma);\n  // también se puede escribir como\n  // for (i in 1:N) {\n  //   w[i] ~ normal(alpha[s[i]], sigma);\n  // }\n  // iniciales\n  alpha ~ normal(60, 10);\n  sigma ~ normal(0, 20);\n}\n\n\nSimulamos datos y ajustamos el modelo, usando los mismos parámetros fijos:\n\nS_sim &lt;- sample(c(1,2), 1000, replace = TRUE)\ndatos_sim_tbl &lt;- sim_peso_mod_s(S_sim, c(55, 70), 10)\n\n\nmod_2_fit &lt;- mod_peso$sample(\n  data = list(N = nrow(datos_sim_tbl), \n              s = datos_sim_tbl$S, \n              w = datos_sim_tbl$W),\n  refresh = 0, seed = 221\n)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.1 seconds.\nChain 2 finished in 0.1 seconds.\nChain 3 finished in 0.1 seconds.\nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.7 seconds.\n\n\n\nmod_2_fit$summary(c(\"alpha\", \"sigma\"))\n\n# A tibble: 3 × 10\n  variable  mean median    sd   mad    q5   q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 alpha[1] 55.0   55.0  0.435 0.429 54.2   55.7  1.00    4611.    2866.\n2 alpha[2] 70.9   70.9  0.435 0.444 70.2   71.6  1.00    4350.    2891.\n3 sigma     9.77   9.76 0.223 0.227  9.41  10.1  1.00    4210.    3013.\n\n\nNótese que la diferencia de medias poblacionales es de alrededor de 15 cm, que es lo que esperábamos según el cálculo de arriba. Podemos replicar el cálculo que hicimos arriba directamente usando simulación:\n\nPara cada simulación de la posterior calculamos una población hipotética de hombres y otras de mujeres (mismos parámetros)\nCalculamos la diferencia de medias poblacionales\nResumimos con la posterior.\n\nEsto es fácil hacerlo directamente en Stan, pero en este ejemplo lo calcularemos manualmente:\n\nsims_post_tbl &lt;- mod_2_fit$draws() |&gt; as_draws_df() |&gt; \n  as_tibble()\nsimular_diferencia_post &lt;- function(sims_post_tbl){\n  # Simulamos parámetros de la posterior\n  pars &lt;- sample_n(sims_post_tbl, 1) |&gt; \n    select(starts_with(\"alpha\"), sigma)\n  # Simulamos datos\n  sims_hombres &lt;- sim_peso_mod_s(rep(2, 1000), \n      alpha = c(pars$`alpha[1]`, pars$`alpha[2]`), pars$sigma)\n  sims_mujeres &lt;- sim_peso_mod_s(rep(1, 1000), \n      c(pars$`alpha[1]`, pars$`alpha[2]`), pars$sigma)\n  diferencia &lt;- mean(sims_hombres$W - sims_mujeres$W)\n  # Calculamos la diferencia de medias\n  tibble(diferencia = diferencia) |&gt; bind_cols(pars)\n}\n\n\nsimular_diferencia_post(sims_post_tbl)\n\n# A tibble: 1 × 4\n  diferencia `alpha[1]` `alpha[2]` sigma\n       &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1       15.3       55.5       70.4  9.60\n\n\nY ahora calculamos el resumen de interés, que es la posterior del contraste o diferencia entre las dos poblaciones simuladas. Comparamos con la línea en rojo que es la cantidad que establecimos a estimar:\n\nmap_df(1:4000, ~ simular_diferencia_post(sims_post_tbl) |&gt; \n         mutate(rep = .x)) |&gt;  \nggplot(aes(x = diferencia)) +\n  geom_histogram(bins = 50) +\n  labs(x = \"Efecto de sexo en estatura hombres vs mujeres (cm)\") +\n  geom_vline(xintercept = mean(sim_hombres$W - sim_mujeres$W), \n             color = \"red\", linewidth = 1.5)\n\n\n\n\n\n\n\n\nPuedes repetir este ejercicio para distintos valores de los parámetros, como hicimos en los ejemplos de arriba.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Componentes de modelación 1</span>"
    ]
  },
  {
    "objectID": "03-modelos-genericos.html#ajustar-a-los-datos-observados-y-resumir",
    "href": "03-modelos-genericos.html#ajustar-a-los-datos-observados-y-resumir",
    "title": "4  Componentes de modelación 1",
    "section": "4.5 Ajustar a los datos observados y resumir",
    "text": "4.5 Ajustar a los datos observados y resumir\nAhora usamos los datos reales y calculamos el estimador que probamos arriba.\n\nmod_2_fit &lt;- mod_peso$sample(\n  data = list(N = nrow(datos_tbl), \n              s = datos_tbl$male + 1, \n              w = datos_tbl$weight),\n  refresh = 0, seed = 221\n)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.1 seconds.\nChain 2 finished in 0.1 seconds.\nChain 3 finished in 0.1 seconds.\nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.5 seconds.\n\n\nY repetimos exactamente el proceso que probamos arriba:\n\nsims_post_tbl &lt;- mod_2_fit$draws() |&gt; as_draws_df() |&gt; \n  as_tibble()\ndif_tbl &lt;- map_df(1:4000, ~ simular_diferencia_post(sims_post_tbl) |&gt; \n         mutate(rep = .x)) \ndif_tbl |&gt; \nggplot(aes(x = diferencia)) +\n  geom_histogram(bins = 50) +\n  labs(x = \"Efecto de sexo en peso hombres vs mujeres (kg)\") \n\n\n\n\n\n\n\n\nConcluimos que el efecto total de sexo sobre peso está entre unos 5.5 y 8 kg de diferencia de hombres vs mujeres.\nEjercicio: explica porqué mostrar por separado las distribuciones de poblaciones de hombres vs la de poblaciones de mujeres no da la respuesta que buscamos.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Componentes de modelación 1</span>"
    ]
  },
  {
    "objectID": "03-modelos-genericos.html#efecto-directo-de-sexo",
    "href": "03-modelos-genericos.html#efecto-directo-de-sexo",
    "title": "4  Componentes de modelación 1",
    "section": "4.6 Efecto directo de sexo",
    "text": "4.6 Efecto directo de sexo\nAhora pensemos cómo podemos calcular el efecto directo de sexo sobre peso, sin tomar en cuente su influencia en la estatura. En nuestro diagrama, nos interesa sólo considerar la influencia que va directamente de sexo a peso, y no la que pasa por el camino que va a través de la estatura. Este tipo de análisis se llama a veces análisis de mediación.\nLa idea es bloquear el camino que va de sexo a estatura, y esto podemos hacer condicionando o estratificando por los valores de \\(H\\). Es decir, para cada valor de \\(H\\), queremos calcular cuál es la diferencia entre una población de hombres y de mujeres (con la misma estatura \\(H\\)). Las diferencias que encontremos no puede deberse a estatura, pues esta valor es fijo. Al estratificar por \\(H\\), decimos que el camino \\(S\\to H\\to W\\) está bloqueado, y refinaremos esta idea más adelante.\nEn términos de cantidad a estimar, quisiéramos, para cada estatura \\(H\\), calcular la diferencia de una población de hombres vs una de mujeres. La diferencia es el efecto directo a la estatura \\(H\\).\nEl modelo estadístico que proponemos para estimar el efecto directo es entonces:\n\\[\n\\begin{align}\nW_i &\\sim N(\\mu_i, \\sigma)\\\\\n\\mu_i &= \\alpha_{S_i} + \\beta_{S_i} (H_i - \\bar{H})\\\\\n\\alpha_1,\\alpha_2 &\\sim N(60, 10) \\\\\n\\beta_1,\\beta_2 &\\sim N^+(0, 1) \\\\\n\\sigma &\\sim N^+(0, 20) \\\\\n\\end{align}\n\\]\nEl contraste que queremos calcular lo podemos identificar con parámetros en el modelo. Por ejemplo, si \\(\\beta_1 = \\beta_2\\), el efecto directo, para cualquier estatura, debería ser \\(\\alpha_2 - \\alpha_1\\). Sin embargo, seguimos con nuestro camino de hacer simulación para mantener más flexibilidad y simplicidad.\nEjercicio: Haz verificaciones a priori: genera datos sintéticos, examínalos, y verifica que el modelo es capaz de recuperar el contraste de interés.\n\n4.6.1 Ajuste a datos reales y resumen\n\nmod_peso_2 &lt;- cmdstan_model(\"./src/peso-estatura-3.stan\")\nprint(mod_peso_2)\n\ndata {\n  int&lt;lower=0&gt; N;\n  vector[N]  w;\n  vector[N]  h;\n  array[N] int s;\n}\n\ntransformed data {\n  real h_media;\n  h_media = mean(h);\n}\n\nparameters {\n  array[2] real alpha;\n  array[2] real&lt;lower=0&gt; beta;\n  real &lt;lower=0&gt; sigma;\n}\n\ntransformed parameters {\n  array[N] real mu;\n  for (i in 1:N) {\n    mu[i] = alpha[s[i]] + beta[s[i]] * (h[i] - h_media);\n  }\n}\n\nmodel {\n  // modelo para peso\n  w ~ normal(mu, sigma);\n  // también se puede escribir:\n  //for (i in 1:N) {\n  //  w[i] ~ normal(mu[i], sigma);\n  //}\n  alpha ~ normal(60, 10);\n  beta ~ normal(0, 1);\n  sigma ~ normal(0, 20);\n}\n\ngenerated quantities {\n\n}\n\n\n\nmod_3_fit &lt;- mod_peso_2$sample(\n  data = list(N = nrow(datos_tbl), \n              s = datos_tbl$male + 1, \n              h = datos_tbl$height,\n              w = datos_tbl$weight),\n  init = 0.01, step_size = 0.01, refresh = 0, seed = 221\n)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.3 seconds.\nChain 2 finished in 0.3 seconds.\nChain 3 finished in 0.3 seconds.\nChain 4 finished in 0.3 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.3 seconds.\nTotal execution time: 1.3 seconds.\n\n\n\nmod_3_fit$summary(c(\"alpha\", \"beta\", \"sigma\")) |&gt; \n  knitr::kable(digits = 2)\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\nalpha[1]\n45.17\n45.17\n0.45\n0.45\n44.45\n45.91\n1\n2270.19\n2455.26\n\n\nalpha[2]\n45.09\n45.10\n0.47\n0.45\n44.31\n45.83\n1\n2822.98\n2524.93\n\n\nbeta[1]\n0.66\n0.66\n0.06\n0.06\n0.55\n0.76\n1\n2109.75\n2083.05\n\n\nbeta[2]\n0.61\n0.61\n0.06\n0.06\n0.52\n0.70\n1\n2737.57\n2646.95\n\n\nsigma\n4.27\n4.26\n0.16\n0.16\n4.02\n4.55\n1\n4090.86\n2699.93\n\n\n\n\n\n\n\nLa diferencia entre las dos rectas parece ser chica. Eso implicaría que el efecto directo de sexo en peso es débil. Sin embargo, es mejor calcular y resumir el contraste como hemos hecho en otros ejemplos.\nRepetimos exactamente el proceso que probamos arriba. Haremos los cálculos manualmente otra vez (aunque conviene más hacerlos dentro de stan):\n\nsims_post_tbl &lt;- mod_3_fit$draws() |&gt; as_draws_df() |&gt; \n  as_tibble()\nh_media &lt;- mean(datos_tbl$height)\n# función para simular pesos\nsim_peso_mod_sh &lt;- function(S, H, alpha, beta, sigma, h_media){\n  n &lt;- length(S)\n  W &lt;- rnorm(n, alpha[S] + beta[S] * (H - h_media), sigma)\n  tibble(W = W)\n}\nsimular_diferencia_post_2 &lt;- function(sims_post_tbl, h){\n  pars &lt;- sample_n(sims_post_tbl, 1) |&gt; \n    select(starts_with(c(\"alpha\", \"beta\")), sigma)\n  alpha &lt;- c(pars$`alpha[1]`, pars$`alpha[2]`)\n  beta &lt;- c(pars$`beta[1]`, pars$`beta[2]`)\n  diferencia &lt;- numeric(length(h))\n  # para cada nivel de estatura especificado\n  for(i in seq_along(h)){\n    # Simulamos poblaciones\n    sims_hombres &lt;- sim_peso_mod_sh(rep(2, 1000), h[i],\n      alpha = alpha, beta = beta, pars$sigma, h_media = h_media)\n    sims_mujeres &lt;- sim_peso_mod_sh(rep(1, 1000), h[i],\n      alpha = alpha, beta = beta, pars$sigma, h_media = h_media)\n    diferencia[i] &lt;- mean(sims_hombres$W - sims_mujeres$W)\n  }\n  tibble(diferencia = diferencia, h = h) |&gt; bind_cols(pars)\n}\n\nPor ejemplo:\n\nsimular_diferencia_post_2(sims_post_tbl, h = c(150, 170))\n\n# A tibble: 2 × 7\n  diferencia     h `alpha[1]` `alpha[2]` `beta[1]` `beta[2]` sigma\n       &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1      1.38    150       45.2       45.9     0.610     0.502  4.25\n2     -0.836   170       45.2       45.9     0.610     0.502  4.25\n\n\n\nh &lt;- seq(130, 190, by = 5)\ndif_tbl &lt;- map_df(1:1000, \n    ~ simular_diferencia_post_2(sims_post_tbl, h) |&gt; \n         mutate(rep = .x))\n\n\ndif_tbl |&gt; \nggplot(aes(x = h, y = diferencia, group = rep)) +\n  geom_line(alpha = 0.1) +\n  labs(x = \"Contraste de peso hombres vs mujeres (kg)\") +\n  geom_hline(yintercept = 0, colour = \"red\") \n\n\n\n\n\n\n\n\nEsto muestra que el efecto directo de sexo en peso es relativamente chico: la mayor parte del efecto es a través de la estatura. Existe una ligera tendencia a que los hombre de menos estatura sean más pesados, y las mujeres de más estatura sean relativamente menos pesadas, pero realmente no podemos afirmar con confianza ningún efecto claro.\n\n\n\n\n\n\nTip\n\n\n\nCuando agregamos una variable como estatura en el modelo de regresión, decimos que estamos estratificando por estatura. En este caso, bloqueamos el efecto causal que tiene sexo en peso a través de la estatura. El efecto causal entre los la variable sexo, que se hace dentro de cada estrato, y se expresa en los coeficientes de sexo, tiene una interpretación totalmente diferente en comparación al modelo que no incluye estatura.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Componentes de modelación 1</span>"
    ]
  },
  {
    "objectID": "03-modelos-genericos.html#regresión-logística-tiros-de-golf",
    "href": "03-modelos-genericos.html#regresión-logística-tiros-de-golf",
    "title": "4  Componentes de modelación 1",
    "section": "4.7 Regresión logística: tiros de golf",
    "text": "4.7 Regresión logística: tiros de golf\nEste caso está basado en el paper Gelman y Nolan (2002) y el caso de estudio de Gelman.\nQueremos entender la probabilidad de éxito de putts de Golf (putts: tiros relativamente cerca del hoyo que buscan que la pelota ruede al hoyo o muy cerca de él), y cómo depende el éxito de la distancia del tiro. ¿Qué tan precisos son los profesionales a diferentes distancias? Los datos que tenemos son varios putts de Golf de profesionales a varias distancias, y para cada distancia el porcentaje de éxitos de esos putts.\nComenzaremos con el siguiente diagrama causal:\n\n\nCódigo\ngrViz(\"\ndigraph {\n  graph [ranksep = 0.3, rankdir = LR]\n  node [shape=circle]\n    V\n    Ang [label = &lt;&theta;&gt;]\n    U\n  node [shape=plaintext]\n    D\n    Y\n  edge [minlen = 3]\n    D -&gt; V\n    D -&gt; Y\n    V -&gt; Y\n    Ang -&gt; Y\n    U -&gt; Y\n{rank = same; D; V}\n{rank = same; Ang; Y}\n{rank = max; U}\n}\n\")#, width = 200, height = 50)\n\n\n\n\n\n\nEn este caso, el modelo causal es como sigue: conocemos la distancia \\(D\\) al hoyo en cada tiro. El éxito (\\(Y=1\\)) o fracaso (\\(Y=0\\)) depende de la distancia, junto con la velocidad a la que sale la pelota (muy alto o muy bajo puede dar un tiro fallido), y el ángulo \\(\\theta\\) de salida. Adicionalmente, hay otros factors \\(U\\) que pueden afectar la probabilidad de éxito. Nótese que no escribiríamos, por ejemplo \\(Y \\leftarrow D\\), porque la distancia no cambia causalmente con el resultado del tiro, aunque es cierto que si intervenimos en la distancia, esperaríamos obtener tasas de éxito diferentes. Igualmente, es necesario poner una flecha de \\(V\\) a \\(D\\) y \\(V\\) a \\(Y\\).\nTodas estas variables antecedentes de \\(Y\\) interactúan para determinar el éxito de un tiro.\nCantidad a estimar: queremos conocer el efecto total de \\(D\\) sobre \\(Y\\). Esto es, queremos conocer la curva \\(p(Y=1|D=d)\\), que es una función de \\(d\\), y también hacer contrastes del tipo \\(p(Y=1|D=d + 10) - p(Y = 1|D=d),\\) por ejemplo.\nModelo estadístico: Nuestro diagrama causal justifica que no es necesario considerar \\(V\\) o \\(\\theta\\) en el modelo, pues nos interesa el efecto total de la distancia sobre \\(Y\\). Esto justifica el uso de un modelo genérico usamos \\(Y\\) como respuesta y \\(D\\) como variable independiente. Intentaremos con un modelo genérico como regresión logística.\nPlanteamos entonces un modelo logístico:\n\\(p(Y = 1| D = d) = \\frac{1}{1 + \\exp(-\\alpha - \\beta d)} = h(\\alpha +\\beta d)\\)\nY nuestro modelo es:\n\\[\n\\begin{align}\nY_i &\\sim Bern(p_i(D_i)) \\\\\np_i &= \\frac{1}{1+\\exp(-\\alpha - \\beta D_i)} \\\\\n\\end{align}\n\\]\nEn términos de este modelo, queremos estimar la curva \\(h(\\alpha +\\beta d)\\), o los parámetros \\(\\alpha\\) y \\(\\beta\\).\nAhora construimos un modelo generativo, donde \\(D\\) está dada en centímetros:\n\nsimular_putts &lt;- function(distancias, alpha, beta) {\n  p &lt;- 1 / (1 + exp(-alpha - beta * distancias))\n  tibble(y = rbinom(length(distancias), 1, p), d = distancias) |&gt; \n    select(d, y)\n}\n\nFijamos parámetros y simulamos datos:\n\nset.seed(22)\ndistancias &lt;- seq(0, 1000, 5) |&gt; rep(each = 5)\nsimular_putts(distancias, 3, -0.005) |&gt; \n  ggplot(aes(x = d, y = y)) +\n  geom_jitter(height = 0.1) +\n  labs(x = \"Distancia (cm)\", y = \"Éxito\") +\n  geom_smooth(span = 0.5)\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nNótese que para este ejemplo utilizamos valores de \\(\\alpha\\) y \\(\\beta\\) fijos. Valores imposibles para golfistas profesionales, por ejemplo, podrían ser los siguientes:\n\nset.seed(22)\ndistancias &lt;- seq(0, 1000, 5) |&gt; rep(each = 5)\nsimular_putts(distancias, 10, -1) |&gt; \n  ggplot(aes(x = d, y = y)) +\n  geom_jitter(height = 0.1) +\n  labs(x = \"Distancia (cm)\", y = \"Éxito\") +\n  geom_smooth(span = 0.5)\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nEsta configuración de parámetros no es razonable, pues implicaría que sólo pueden completar los tiros a unos cuantos centímetros del hoyo. Sabemos que esto no es cierto.\nPara completar nuestro modelo generativo, es necesario especificar los valores que pueden tomar estas variables. Pondremos distribuciones iniciales o a priori apropiadas para los parámetros. En este punto no hemos visto ningún dato, así que podemos experimentar para hacer una selección apropiada desde el punto de vista del conocimiento del área que tenemos actualmente.\nPodemos poner por ejemplo \\[\\alpha \\sim \\text{Normal}(6, 2),\\] pues sabemos a que distancias de casi cero, es muy seguro lograr el tiro (no lejos de 100% de éxito). Para la \\(\\beta\\), consideramos que a unos 100 cm es muy probable hacer el tiro, pero a 1000 cm (10 m) la probabilidad baja considerablemente. Experimentaremos poniendo (debe ser negativa)\n\\[\\beta \\sim \\text{Normal}^-(0, 0.025).\\]\ny ahora reescribimos nuestra función de simulación:\n\nsimular_putts &lt;- function(distancias) {\n  alpha &lt;- rnorm(1, 6, 2)\n  beta &lt;-  - abs(rnorm(1, 0, 0.025))\n  p &lt;- 1 / (1 + exp(-alpha - beta * distancias))\n  tibble(y = rbinom(length(distancias), 1, p), p = p, d = distancias) |&gt; \n    select(d, p, y) |&gt; \n    mutate(alpha = alpha, beta = beta)\n}\n\nY podemos ver cómo se ven diversas curvas que incluye nuestro modelo:\n\ndistancias &lt;- seq(0, 1000, 1) \nmap_df(1:100,  \\(x) simular_putts(distancias) |&gt; mutate(id = x)) |&gt; \n  ggplot(aes(x = d, y = p, group = id)) +\n  geom_line(alpha = 0.2) +\n  labs(x = \"Distancia (cm)\", y = \"Probabilidad de Éxito\")\n\n\n\n\n\n\n\n\nLos casos extremos son poco creíbles (0 probabilidad a 2 metros o 100% de probabilidad a 6 metros), pero tenemos un rango razonable para las curvas. Podemos ver la curva promedio:\n\nreps_sim &lt;- map_df(1:1000,  \\(x) simular_putts(distancias) |&gt; mutate(id = x)) \nresumen &lt;- reps_sim |&gt; group_by(d) |&gt; summarise(p_5 = quantile(p, 0.10),\n                                                p95 = quantile(p, 0.90),\n                                                p = mean(p))\nreps_sim |&gt; \n  ggplot(aes(x = d, y = p)) +\n  #geom_line(aes(group = id), alpha = 0.2) +\n  labs(x = \"Distancia (cm)\", y = \"Probabilidad de Éxito\") +\n  geom_ribbon(data = resumen, aes(ymin = p_5, ymax = p95), alpha = 0.2) +\n  geom_line(data = resumen, color = \"red\", linewidth = 2) \n\n\n\n\n\n\n\n\nUna vez que estamos satisfechos con nuestro modelo (nótese que hay lugar para varias críticas), podemos proceder a calcular la distribución posterior, primero con datos simulados.\nLa posterior en este caso es más complicada y no tenemos una manera simple de simularla. Explicaremos más adelante cómo hacer esto (usando MCMC). Por ahora, escribiremos un programa de Stan que nos da simulaciones de la posterior. Tomaremos por el momento Stan como una caja negra, y después justificaremos este procedimiento.\n\n#! message: false\nlibrary(cmdstanr)\nmod_logistica &lt;- cmdstan_model(\"./src/golf-logistico.stan\")\nprint(mod_logistica)\n\ndata {\n  int&lt;lower=0&gt; N;\n  array[N] int n;\n  vector[N] d;\n  array[N] int y;\n}\nparameters {\n  real alpha;\n  real&lt;upper = 0&gt; beta;\n}\nmodel {\n  y ~ binomial_logit(n, alpha + beta * d);\n  alpha ~ normal(6, 2);\n  beta ~ normal(0, 0.025);\n}\n\n\n\nset.seed(425)\ndistancias &lt;- rnorm(100, 0, 2000) |&gt; abs() \ndatos &lt;- simular_putts(distancias)\ndatos_mod &lt;- datos |&gt; group_by(d) |&gt; \n  summarise(n = n(), y = sum(y)) |&gt; \n  ungroup()\najuste &lt;- mod_logistica$sample(\n  data = list(N = nrow(datos_mod), \n              d = datos_mod$d, y = datos_mod$y, n = datos_mod$n), \n                        refresh = 1000, init = 10, step_size = 0.01)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.1 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 0.1 seconds.\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 0.1 seconds.\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.5 seconds.\n\nsims &lt;- ajuste$draws(c(\"alpha\", \"beta\"), format = \"df\")\nresumen &lt;- ajuste$summary()\n\n\nresumen\n\n# A tibble: 3 × 10\n  variable     mean   median     sd    mad       q5      q95  rhat ess_bulk\n  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 lp__     -11.3    -11.0    1.01   0.772  -13.3    -10.3     1.00    1197.\n2 alpha      3.85     3.77   1.29   1.33     1.86     6.08    1.00    1051.\n3 beta      -0.0364  -0.0356 0.0114 0.0116  -0.0560  -0.0191  1.00    1072.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\n\n\ndatos$alpha[1]\n\n[1] 2.691742\n\ndatos$beta[1]\n\n[1] -0.02548956\n\n\n\nggplot(sims, aes(alpha, beta)) + geom_point() +\n  geom_point(data = datos |&gt; first(), aes(alpha, beta), color = \"red\", size = 3)\n\n\n\n\n\n\n\n\nY podemos graficar la posterior de interés, que se construye con todas las curvas simuladas:\n\ngrafs_tbl &lt;- sims |&gt;\n  rowwise() |&gt;\n  mutate(graf = list(tibble(d = seq(0, 600, 10)) |&gt; \n           mutate(p = 1 / (1 + exp(-alpha - beta * d)))\n  )) |&gt; \n  ungroup() |&gt; \n  slice_sample(n = 1000) |&gt; \n  select(.draw, graf) |&gt; \n  unnest(graf) \n\nY graficamos:\n\ngrafs_tbl |&gt; \n  ggplot(aes(x = d, y = p, group = .draw)) +\n  geom_line(alpha = 0.1) +\n  labs(x = \"Distancia (cm)\", y = \"Probabilidad de Éxito\")\n\n\n\n\n\n\n\n\nPor el momento, podríamos hacer unas cuantas simulaciones distintas, y ver que las curvas obtenidas son las que esperaríamos (más tarde formalizaremos este proceso). También podríamos hacer simulaciones con distintos tamaños de muestra, y entender cuánta incertidumbre en la estimación de las curvas podríamos tener.\nAhora llegamos al siguiente paso, que tomar los datos y estimar nuestra curva de desempeño según la distancia. Será necesario convertir de pies a centímetros en la variable de distancia:\n\n# nota: x está en pies (ft)\ndatos_golf &lt;- read_delim(\"../datos/golf.csv\", delim = \" \")\n\nRows: 19 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (3): x, n, y\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(datos_golf)\n\n# A tibble: 6 × 3\n      x     n     y\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     2  1443  1346\n2     3   694   577\n3     4   455   337\n4     5   353   208\n5     6   272   149\n6     7   256   136\n\n\n\nset.seed(1225)\najuste &lt;- mod_logistica$sample(\n  data = list(N = nrow(datos_golf), \n              d = 30.48 * datos_golf$x, y = datos_golf$y, n = datos_golf$n), \n                        refresh = 1000, init = 10, step_size = 0.01)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.0 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 0.0 seconds.\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 0.0 seconds.\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.5 seconds.\n\nsims &lt;- ajuste$draws(c(\"alpha\", \"beta\"), format = \"df\")\nresumen &lt;- ajuste$summary()\n\n\nresumen\n\n# A tibble: 3 × 10\n  variable        mean   median      sd     mad       q5      q95  rhat ess_bulk\n  &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 lp__     -3028.      -3.03e+3 9.67e-1 6.82e-1 -3.03e+3 -3.03e+3  1.00    1487.\n2 alpha        2.23     2.23e+0 5.77e-2 5.73e-2  2.14e+0  2.32e+0  1.01    1128.\n3 beta        -0.00839 -8.39e-3 2.18e-4 2.21e-4 -8.75e-3 -8.04e-3  1.01    1148.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\n\nAhora simulamos la posterior y la contrastamos con los datos:\n\ngrafs_tbl &lt;- sims |&gt;\n  rowwise() |&gt;\n  mutate(graf = list(tibble(d = 30.48 * seq(0, 20, 0.5)) |&gt; \n           mutate(p = 1 / (1 + exp(-alpha - beta * d)))\n  )) |&gt; \n  ungroup() |&gt; \n  slice_sample(n = 100) |&gt; \n  select(.draw, graf) |&gt; \n  unnest(graf) \n\n\nresumen_golf &lt;- datos_golf |&gt;\n  mutate(d = 30.48 * x, p = y / n)\n\n\ngrafs_tbl |&gt; \n  ggplot(aes(x = d, y = p)) +\n  geom_line(aes(group = .draw), alpha = 0.1) +\n  labs(x = \"Distancia (cm)\", y = \"Probabilidad de Éxito\") +\n  geom_point(data = resumen_golf, color = \"red\") +\n  geom_linerange(data = resumen_golf, \n    aes(ymin = p - 2 * sqrt(p * (1 - p) / n),  \n        ymax = p + 2 * sqrt(p * (1 - p) / n)),\n    color = \"red\")\n\n\n\n\nChequeo predictivo posterior (deficiente)\n\n\n\n\nVemos que la curva estimada desajusta. Este tipo de análisis se llama chequeo a posteriori, y mas frecuentemente se hace un chequeo predictivo posterior, que veremos más adelante. Por el momento, nos quedamos con la conclusión de que el modelo no es apropiado para estos datos: no hemos capturado apropiadamente la relación que hay entre éxito y distancia.\nEn este punto veremos dos caminos:\n\nEl primero es continuar con modelos genéricos que no toman en cuenta mecanismos específicos de los datos. Por ejemplo, podríamos poner más terminos derivados de la distancia (polinomios o splines).\nEl segundo camino, es utilizar más información acerca del fenómeno de interés. Sabemos como funciona básicamente el golf, y también sabemos geometría y física que determina el proceso generador de datos. Podemos utilizar esta información para construir un modelo más apropiado.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Componentes de modelación 1</span>"
    ]
  },
  {
    "objectID": "03-modelos-genericos.html#usando-teoría-para-construir-modelos",
    "href": "03-modelos-genericos.html#usando-teoría-para-construir-modelos",
    "title": "4  Componentes de modelación 1",
    "section": "4.8 Usando teoría para construir modelos",
    "text": "4.8 Usando teoría para construir modelos\nEn la gráfica causal que vimos arriba, hay mucha información adicional que no hemos utilizado acerca de la naturaleza de la dependencia de las variables involucradas. En esta parte, en lugar de usar un modelo genérico, utilizaremos geometría y algo de física para construir un modelo más apropiado.\nEl problema es considerablemente complicado conceptualmente (Holmes (1991), Penner (2002)) si consideramos todas las fuentes de variación: ángulo de tiro, potencia de tiro, declive en greens y así sucesivamente.\nSeguiremos haciendo la simplificación de superficie plana, pero consideramos dos parámetros para el tiro con distintas condiciones de éxito:\n\nEl ángulo del tiro\nLa velocidad con la que la pelota llega (o no llega) al hoyo\n\nEl diámetro de una pelota de golf y el hoyo (en centrímetros) es de\n\ndiam_pelota &lt;- (1.68 * 2.54) |&gt;  round(1)\ndiam_hoyo &lt;- (4.25 * 2.54) |&gt;  round(1)\nc(diam_pelota, diam_hoyo)\n\n[1]  4.3 10.8\n\n\nSupondremos por el momento que los greens de golf (áreas cerca del hoyo) son perfectamente planos, de modo que el éxito depende de\n\nTirar la pelota con un ángulo suficientemente cercano a cero con respecto a la línea que va del centro de la pelota al centro del hoyo.\nTirar la pelota con una velocidad suficiente para llegue al hoyo pero no tan alta que vuele por encima del hoyo.\n\nMejores datos de los tipos de fallo sería útil, pero por el momento no consideramos que estén disponibles.\nEmpezamos construyendo nuestro modelo considerando sólamente el ángulo de tiro.\n\n4.8.0.1 Ángulo de tiro\nSupongamos que la distancia del centro de la pelota al centro del hoyo es \\(d\\), y que \\(\\theta\\) es el ángulo del tiro con respecto a la recta que va del centro de la pelota al centro del hoyo. El tiro es exitoso cuando (si \\(\\theta\\) está en radianes):\n\\[\\tan(\\theta) &lt; \\frac{R - r}{2d}\\]\n Tenemos que\n\n(diam_hoyo - diam_pelota)/2\n\n[1] 3.25\n\n\nAsí que para nuestro problema simplificado, la condición de éxito es (d está dado en centímetros y \\(\\theta\\) en radianes):\n\\[\\tan(\\theta) &lt; \\frac{3.25}{d}\\]\nMejores golfistas tendrán mejor control sobre \\(\\theta\\), y conforme \\(d\\) es más grande, la probabilidad de tener éxito baja:\n\ntibble(d = seq(10, 600, 1)) |&gt;  \n  mutate(theta_grados = (180 / pi) * atan(3.25 / d)) |&gt;  \nggplot(aes(d, theta_grados)) + geom_point() +\n  xlab(\"Distancia (cm)\") +\n  ylab(expression(paste(\"Desviación máxima en grados\"))) +\n  labs(subtitle = \"Desviación máxima permitida para tener éxito a distintas distancias\") +\n  scale_y_log10()\n\n\n\n\n\n\n\n\nPor el momento, sólo consideraremos el ángulo y la distancia. Supongamos que un tirador profesional tira con un ángulo \\(\\theta\\) que se distribuye como\n\\[\\theta\\sim N(0,\\sigma),\\] donde \\(\\theta=0\\) indica que el tiro es directo al hoyo. La probabilidad de tener éxito es entonces\n\\[P(Y=1|\\theta) = P(|\\theta| &lt; \\arctan(3.25/d)) = 2\\Phi \\left (\\frac{\\arctan(3.25/d)}{\\sigma}  \\right )-1\\] Así que nuestro modelo completo es\n\\[\n\\begin{align}\nY_i &\\sim Bern(p_i(D_i)) \\\\\np_i &= 2\\Phi(\\arctan(3.25/D_i)/\\sigma)-1 \\\\\n\\end{align}\n\\] Nótese que sólo tenemos un parámetro \\(\\sigma\\) en este modelo en lugar de dos como en la regresión logística. La diferencia grande también es la forma funcional de la probabilidad de éxito.\nAntes de escribir nuestra función, necesitamos poner una inicial sobre \\(\\sigma\\). Probaremos considerando que un jugador profesional puede tener una desviación de 0 a 10 grados, por ejemplo. Convirtiendo a radianes podríamos poner:\n\\[\n\\begin{align}\nY_i &\\sim Bern(p_i(D_i)) \\\\\np_i &= 2\\Phi(\\arctan(3.25/D_i)/\\sigma)-1 \\\\\n\\sigma &\\sim N^+(0, 5\\pi/180))\n\\end{align}\n\\]\n\nsimular_putts_angulo &lt;- function(distancias) {\n  sigma &lt;- rnorm(1, 0, 5 * pi / 180) |&gt; abs()\n  p &lt;- 2 * pnorm(atan(3.25 / distancias)/sigma) - 1\n  tibble(y = rbinom(length(distancias), 1, p), p = p, d = distancias) |&gt; \n    select(d, p, y) |&gt; \n    mutate(sigma = sigma)\n}\n\nY podemos ver cómo se ven diversas curvas que incluye nuestro modelo:\n\ndistancias &lt;- seq(0, 600, 1) \nmap_df(1:100,  \\(x) simular_putts_angulo(distancias) |&gt; mutate(id = x)) |&gt; \n  ggplot(aes(x = d, y = p, group = id)) +\n  geom_line(alpha = 0.2) +\n  labs(x = \"Distancia (cm)\", y = \"Probabilidad de Éxito\")\n\n\n\n\n\n\n\n\nEsta inicial espera que en general los tiros a menos de menos de 50cm se consigan con mucha frecuencia o casi seguro, 2 metros se consigan con cierta frecuencia, y 6 metros se consigan con relativamente menos frecuencia. Tiene el defecto de que pone mucho peso en desviaciones muy chicas, lo cual es poco creíble (las rectas que se pegan mucho probabilidad uno para todas las distancias).\nUn cambio razonable que podemos hacer, por ejemplo, es poner:\n\\[\\theta_{grados} \\sim Gamma(a, b)\\]\nCuya media queremos poner por ejemplo en 2 grados \\(a/b = 2\\), y suponemos una desviación estándar de unos \\(a/b^2 = 0.5\\) grados. En este caso,\n\nmu &lt;- 2\nbeta  &lt;- 2\nqplot(rgamma(1e5, mu * beta, beta))\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nsimular_putts_angulo &lt;- function(distancias) {\n  sigma &lt;- (pi/180) * rgamma(1, mu * beta, beta) \n  p &lt;- 2 * pnorm(atan(3.25 / distancias)/sigma) - 1\n  tibble(y = rbinom(length(distancias), 1, p), p = p, d = distancias) |&gt; \n    select(d, p, y) |&gt; \n    mutate(sigma = sigma)\n}\n\nY podemos ver cómo se ven diversas curvas que incluye nuestro modelo:\n\ndistancias &lt;- seq(0, 600, 1) \nmap_df(1:200,  \\(x) simular_putts_angulo(distancias) |&gt; mutate(id = x)) |&gt; \n  ggplot(aes(x = d, y = p, group = id)) +\n  geom_line(alpha = 0.2) +\n  labs(x = \"Distancia (cm)\", y = \"Probabilidad de Éxito\") +\n  ylim(c(0,1))\n\n\n\n\n\n\n\n\nNota: Checando con esta gráfica podemos modificar los parámetros de la Gamma para poner una inicial apropiada.\nDejaremos como ejercicio hacer las pruebas predictivas a priori, y continuaremos con el ajuste a los datos para checar el ajuste de este modelo:\n\n#! message: false\nlibrary(cmdstanr)\nmod_golf &lt;- cmdstan_model(\"./src/golf-principios-1.stan\")\nprint(mod_golf)\n\ndata {\n  int&lt;lower=0&gt; N;\n  array[N] int n;\n  vector[N] d;\n  array[N] int y;\n}\ntransformed data {\n  vector[N] angulo_maximo = atan(3.25 ./ d);\n}\nparameters {\n  real&lt;lower=0&gt; sigma_ang;\n  }\ntransformed parameters {\n  real sigma = sigma_ang * pi() / 180;\n  vector[N] p = 2 * Phi(angulo_maximo / sigma) - 1;\n}\nmodel {\n  y ~ binomial_logit(n, p);\n  sigma_ang ~ gamma(4, 2);\n}\n\n\n\nset.seed(225)\najuste_2 &lt;- mod_golf$sample(\n  data = list(N = nrow(datos_golf),\n              d =  30.48 * datos_golf$x, y = datos_golf$y, n = datos_golf$n),\n  refresh = 1000, init = 0.01, step_size = 0.01)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.0 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 0.0 seconds.\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 0.0 seconds.\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.5 seconds.\n\nsims_2 &lt;- ajuste_2$draws(c(\"sigma\", \"sigma_ang\"), format = \"df\")\nresumen_2 &lt;- ajuste_2$summary(c(\"sigma\", \"sigma_ang\"))\n\n\nresumen_2\n\n# A tibble: 2 × 10\n  variable    mean median      sd     mad     q5    q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 sigma     0.0308 0.0307 0.00136 0.00139 0.0286 0.0331  1.00    1271.    1797.\n2 sigma_ang 1.76   1.76   0.0782  0.0795  1.64   1.90    1.00    1271.    1797.\n\n\nAhora simulamos la posterior y la contrastamos con los datos:\n\ngrafs_2_tbl &lt;- sims_2 |&gt;\n  rowwise() |&gt;\n  mutate(graf = list(tibble(d = 30.48 * seq(0, 20, 0.5)) |&gt;\n                       mutate(p = 2 * pnorm(atan(3.25 / d)/sigma) - 1))) |&gt;\n  ungroup() |&gt;\n  slice_sample(n = 100) |&gt;\n  select(.draw, graf) |&gt;\n  unnest(graf)\n\nEl resultado es considerablemente mejor que el de la regresión logística, aunque tiene algunos fallos en rangos medios:\n\ngrafs_2_tbl |&gt;\n  ggplot(aes(x = d, y = p)) +\n  geom_line(aes(group = .draw), alpha = 0.1) +\n  labs(x = \"Distancia (cm)\", y = \"Probabilidad de Éxito\") +\n  geom_point(data = resumen_golf, color = \"red\") +\n  geom_linerange(data = resumen_golf,\n                 aes(ymin = p - 2 * sqrt(p * (1 - p) / n),\n                     ymax = p + 2 * sqrt(p * (1 - p) / n)),\n                 color = \"red\") +\n  ylim(c(0,1))\n\n\n\n\nChequeo predictivo posterior\n\n\n\n\nEste es un modelo relativamente simple que sólo toma en cuenta ángulos y distancia. Puedes ver en el artículo del Gelman et al citado arriba cómo refinarlo para poner condiciones sobre la velocidad inicial, por ejemplo.\n\n\n\n\nGelman, Andrew, y Deborah Nolan. 2002. «A Probability Model for Golf Putting». Teaching Statistics 24 (septiembre): 93-95. https://doi.org/10.1111/1467-9639.00097.\n\n\nHolmes, Brian W. 1991. «Putting: How a golf ball and hole interact». American Journal of Physics 59 (2): 129-36. https://doi.org/10.1119/1.16592.\n\n\nMcElreath, R. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. A Chapman & Hall libro. CRC Press. https://books.google.com.mx/books?id=Ie2vxQEACAAJ.\n\n\nPenner, Albert. 2002. «The physics of putting». Canadian Journal of Physics 80 (febrero): 83-96. https://doi.org/10.1139/p01-137.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Componentes de modelación 1</span>"
    ]
  },
  {
    "objectID": "05-dags.html",
    "href": "05-dags.html",
    "title": "5  Modelos gráficos y causalidad",
    "section": "",
    "text": "5.1 Modelos gráficos\nEn primer lugar, podemos pensar cómo se asignan los valores de las variables en nuestro proceso generador de datos. Pensamos entonces de qué depende directamente cada variable para determinar su valor, de manera que cada nodo \\(X\\) de la variable se puede escribir por ejemplo como \\(Y = f(X, W)\\) y \\(Z = g(X)\\). Esto es desde el punto de vista téorico y de conocimiento de área que tenemos, y representa supuestos causales. En este ejemplo particular, tenemos un modelo gráfico asociado, escribimos como:\nCódigo\ngrViz(\"\ndigraph {\n  graph [ranksep = 0.2, rankdir=LR]\n  node [shape=plaintext]\n    X\n    Y\n    Z\n    W\n  edge [minlen = 3]\n   X -&gt; Y\n   Z -&gt; X\n   W -&gt; Y\n}\n\", width = 150, height = 40)\nNótese que no describimos exactamente cómo son las funciones que relacionan las variables, sino más bien qué variables son causas directas de qué otras. Por ejemplo, aunque en nuestro ejemplo de arriba \\(Y\\) depende de \\(Z\\), no hay una causa directa a \\(Y\\), porque cambios en \\(Z\\) afectan a \\(X\\), y es el cambio en \\(X\\) que es causa directa de \\(Y\\).\nNota: hay varias maneras de construir modelos causales además de DAGs. Una de ellas es sistemas de ecuaciones diferenciales (en el tiempo), que a veces son necesarias para modelos de biología, clima o epidemiología por ejemplo. También pueden utilizarse modelos de agentes (modelamos partes más pequeñas o simples del sistemas y sus interacciones). Quizá los DAGs son los modelos con más populares y tienen amplia aplicabilidad.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Modelos gráficos y causalidad</span>"
    ]
  },
  {
    "objectID": "05-dags.html#modelos-gráficos",
    "href": "05-dags.html#modelos-gráficos",
    "title": "5  Modelos gráficos y causalidad",
    "section": "",
    "text": "Modelos gráficos Y DAGs\n\n\n\nEn un modelo gráfico, dibujamos una arista de un nodo \\(X\\) a un nodo \\(Y\\) si el valor de la variable \\(Y\\) depende directamente del valor de la variable \\(X\\), es decir si \\(X\\) es una causa directa de \\(Y\\). En estos modelos no especificamos la fórmula o naturaleza de cada relación directa, sino simplemente que ésta existe.\nTrabajamos principalmente con modelos causales que pueden representarse como DAGs (gráficas dirigidas acícilcas), donde no existen ciclos de causas entre las variables.\nExisten dos tipos de nodos en estas gráficas: variables exógenas que no dependen de otros nodos para tomar su valor, y variables endógenas que son descendientes de al menos otro nodo. Cuando conocemos las variables exógenas, en teoría podemos simular todo el sistema si especificamos el modelo de cada nodo endógeno.\n\n\n\n\n5.1.1 Ejemplo simple\nPara entender los conceptos empezamos con una historia de datos sencilla. En un juego de azar, supongamos que escogemos al azar un número \\(X\\) entre 0 y 1, y luego tiramos dos veces cinco volados con probabilidad de sol \\(X\\). Medimos el número de soles en cada prueba como \\(S_1\\) y \\(S_2\\). Finalmente, la ganancia \\(G\\) obtenida es la suma de \\(S_1+S_2\\) si el día es lluvioso o solamente \\(S_1\\) si el día es soleado.\nNótese que tanto como \\(S_1\\) y \\(S_2\\) dependen de su valor de \\(X\\), además de que dependen de otras variables \\(U_1\\) y \\(U_2\\), muy complicadas, que determinan cómo caen los volados. \\(G\\) depende de su valor de \\(S_1\\) y \\(S_2\\), además de depender de una variable \\(D\\) que describe si el día actual es lluvioso o soleado. El diagrama causal resultante es el que sigue, donde consideramos que observaremos \\(U1\\), \\(U2\\) y \\(U3\\).\n\n\nCódigo\ngrViz(\"\ndigraph {\n  graph [ranksep = 0.2, rankdir=LR]\n  node [shape=circle]\n    U1\n    U2\n    U3\n  node [shape=plaintext]\n    S1\n    S2\n    X\n  edge [minlen = 3]\n   X -&gt; S1\n   X -&gt; S2\n   U1 -&gt; S1\n   U2 -&gt; S2\n   S1 -&gt; G\n   S2 -&gt; G\n   D -&gt; G\n   U3 -&gt; D\n{\n  rank = same; S1; S2;U1;U2\n}\n\n}\n\")\n\n\n\n\n\n\nSabemos que no podemos saber \\(U1\\) y \\(U2\\), y no nos interesa modelar la física de monedas, manera de lanzarlas, etc. En este ejemplo también no consideraremos qué hace un día soleado o lluvioso (no nos interesa modelar el clima). En este momento, en teoría tenemos ecuaciones determinísticas para todas las variables, y si conocemos todas las variables exógenas \\(X,U1,U2,U3\\) podríamos determinar exactamente lo que va a suceder con la ganancia, por ejemplo, o cualquier otra variable del sistema.\nSin embargo, muchas veces excluímos variables exógenas que sólo afectan a una variable endógena, y consideramos que las relaciones de dependiencia de la gráfica son probabilísticas:\n\n\nCódigo\ngrViz(\"\ndigraph {\n  graph [ranksep = 0.2, rankdir=LR]\n  node [shape=circle]\n   \n  node [shape=plaintext]\n    S1\n    S2\n    X\n  edge [minlen = 3]\n   X -&gt; S1\n   X -&gt; S2\n   S1 -&gt; G\n   S2 -&gt; G\n   D -&gt; G\n{\n  rank = same; S1; S2\n}\n\n}\n\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Modelos gráficos y causalidad</span>"
    ]
  },
  {
    "objectID": "05-dags.html#modelos-gráficos-y-regla-del-producto",
    "href": "05-dags.html#modelos-gráficos-y-regla-del-producto",
    "title": "5  Modelos gráficos y causalidad",
    "section": "5.2 Modelos gráficos y regla del producto",
    "text": "5.2 Modelos gráficos y regla del producto\nLos modelos gráficos también nos muestran cómo hacer factorizaciones útiles de los datos.\nSi tenemos cualquier conjunto de variables aleatorias \\(X_1,\\ldots,X_p\\), la distibución conjunta de estas variables \\(p(x_1,x_2,\\ldots, x_n)\\) nos sirve para calcular cualquier cantidad de interés que involucra estas variables.\nRecordamos ahora la regla del producto: para cualquier conjunto de variables aleatorias \\(X_1,\\ldots,X_p\\), la conjunta se puede factorizar siempre como:\n\\[p(x_1,x_2,\\ldots, x_n) = p(x_1)p(x_2|x_1)p(x_3|x_1,x_2)\\ldots p(x_p|x_1,x_2,\\ldots,x_{p-1})\\] Hay muchas manera de escribir esta factorización, dependiendo de cómo ordenamos las variables. El modelo gráfico nos da un ordenamiento natural (primero van padres y luego hijos) de las variables que nos permite aplicar la regla del producto, según la dirección de las flechas del diagrama:\n\nEjemplo\nEn el ejemplo de arriba, un ordenamiento es \\(X,S1,S2,D,G\\). Entonces, podemos escribir la regla del producto:\n\\[p(x,s_1,s_2,d,g) = p(x)p(s_1|x)p(s_2|x,s_1)p(d|x,s_1,s_2,d)p(g|x,s1,s2,d)\\] Que podemos simplificar porque por ejemplo, \\(p(s_2|x, s_1) = p(s_2|x)\\), ya que \\(s_1\\) no influye directamente en \\(s_2\\). Por la misma lógica, \\(p(d|x,s_1,s_2) = p(d|x)\\) y \\(p(g|x,s_1,s_2,d) = p(g|s_1,s_2,d)\\). Entonces:\n\\[p(x,s_1,s_2,d,g) = p(x)p(d)p(s_1|x)p(s_2|x)p(g|s_1,s_2,d)\\] Que nos da una manera mucho más parsimoniosa de modelar las relaciones entre estas variables. Bajo nuestros supuestos no es necesario modelar toda la cadena de dependencias, pues algunas flechas no están presentes.\nEn nuestro caso,\n\n\\(p(x)\\) es uniforme en \\([0,1]\\),\nsupondremos por ejemplos que \\(p(d=lluvioso) =0.3\\) (tomando un día del año al azar),\n\\(S_1|X\\) y \\(S_2|X\\) son binomiales con 5 pruebas y probabilidad \\(X\\), y\n\\(G|S_1,S_2,D\\) es determinística: \\(G\\) toma el valor \\(S_1+ S_2\\) si \\(D\\) es lluvioso, y \\(S_1\\) en otro caso. Con esto, tenemos un modelo conjunto completo del sistema de interés.\n\n\n\nEjemplo\nPara entender mejor la parsimonia que podemos alcanzar usando supuestos causales, considera la cadena \\(X\\to Y\\to Z\\to W\\to A\\). Imagina que cada una de estas variables puede tomar 2 valores. La conjunta de cuatro variables \\((X,Y,Z,W)\\), en general, requiere \\(2^5-1=31\\) parámetros. Sin embargo, si se satisface \\(X\\to Y\\to Z\\to W\\to A\\) sólo requirimos 1 parámetro para \\(p(x)\\), 2 para \\(p(y|x)\\), 2 para \\(p(z|y)\\), etc. En total, sólo necesitamos 9 parámetros.\n\n\n\n\n\n\nRegla del producto para DAGs\n\n\n\nSupongamos que tenemos un DAG con variables \\(X_i\\). Si denotamos por \\(pa_i\\) a los padres de \\(X_i\\), entonces siempre podemos factorizar la conjunta como\n\\[p(x_1,\\ldots, x_p) = \\prod_{i=1}^p p(x_i|pa_i)\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Modelos gráficos y causalidad</span>"
    ]
  },
  {
    "objectID": "05-dags.html#regla-del-producto-y-simulación",
    "href": "05-dags.html#regla-del-producto-y-simulación",
    "title": "5  Modelos gráficos y causalidad",
    "section": "5.3 Regla del producto y simulación",
    "text": "5.3 Regla del producto y simulación\nEl orden del modelo gráfico también nos indica cómo simular las variables de la gráfica. Como cada modelo gráfico nos da una factorización de la conjunta, podemos utlizar esta para simular datos una vez que conocemos o estimamos las relaciones de dependencia directa. Empezamos con las variables exógenas (que no tienen padres) y vamos simulando hacia adelante.\n\nEjemplo\nEn nuestro ejemplo simulamos primero \\(X\\) y \\(D\\). A partir de \\(X\\) podemos simular \\(X_1\\) y \\(S_2\\), y a partir de \\(D\\), junto con \\(S_1\\) y \\(S_2\\), podemos simular \\(G\\). En nuestro ejemplo tendríamos\n\nsimular_juego &lt;- function(N){\n  x &lt;- runif(N)\n  d &lt;- sample(c(\"lluvioso\",\"soleado\"), N, replace = TRUE, prob = c(0.3,0.7))\n  s1 &lt;- rbinom(N, 5, x)\n  s2 &lt;- rbinom(N, 5, x)\n  g &lt;- ifelse(d==\"lluvioso\", s1+s2, s1)\n  tibble(x, d, s1, s2, g)\n}\nsimular_juego(5)\n\n# A tibble: 5 × 5\n      x d           s1    s2     g\n  &lt;dbl&gt; &lt;chr&gt;    &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1 0.295 soleado      2     1     2\n2 0.414 lluvioso     1     4     5\n3 0.803 soleado      4     5     4\n4 0.596 soleado      5     2     5\n5 0.836 soleado      5     5     5",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Modelos gráficos y causalidad</span>"
    ]
  },
  {
    "objectID": "05-dags.html#estructuras-básicas-de-dags",
    "href": "05-dags.html#estructuras-básicas-de-dags",
    "title": "5  Modelos gráficos y causalidad",
    "section": "5.4 Estructuras básicas de DAGs",
    "text": "5.4 Estructuras básicas de DAGs\nVeremos que para razonar acerca de las asociaciones e independencias (condicionales o no) que pueden aparecer o no en una conjunta, podemos examinar la gráfica que la represente, o dicho de otra manera, entender bajo qué condiciones puede propagarse información de un nodo a otro.\n\n\n\n\n\n\nTip\n\n\n\nLas independencias condicionales que se cumplen para cualquier relación funcional entre variables del sistema, pueden leerse directamente de la estructura del modelo gráfico que representa la conjunta.\nEsto nos permite hacer razonamiento lógico de qué puede estar asociado o no según nuestros supuestos causales.\n\n\nPara entender modelos gráficos en general, basta endender cuatro estructuras básicas que pueden aparecer. Consideremos entonces tres variables \\(X\\), \\(Y\\) y \\(Z\\). Las cuatro estructuras que tenemos que entender en primer lugar pueden verse también como métodos de razonamiento lógico derivados de las leyes de probabilidad:\n\nCausa común o bifurcaciones \\(X\\leftarrow Z \\rightarrow Y\\).\nCadenas o mediación \\(X\\rightarrow Z \\rightarrow Y\\).\nColisionadores \\(X\\rightarrow Z \\leftarrow Y\\).\nDescendientes en cadenas \\(X\\rightarrow Z \\rightarrow Y, Z\\to A\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Modelos gráficos y causalidad</span>"
    ]
  },
  {
    "objectID": "05-dags.html#bifurcaciones-o-causa-común",
    "href": "05-dags.html#bifurcaciones-o-causa-común",
    "title": "5  Modelos gráficos y causalidad",
    "section": "5.5 Bifurcaciones o causa común",
    "text": "5.5 Bifurcaciones o causa común\nEn el siguiente ejemplo, llamamos a \\(Z\\) una causa que es común a \\(X\\) y \\(Y\\).\n\n\nCódigo\ngrViz(\"\ndigraph {\n  graph [ranksep = 0.2]\n  node [shape=plaintext]\n    X\n    Y\n    Z\n  edge [minlen = 3]\n   Z -&gt; X\n   Z -&gt; Y\n}\n\", width = 200, height = 50)\n\n\n\n\n\n\nEn este caso,\n\n\\(X\\) y \\(Y\\) tienen asociación\nSi condicionamos (o estratificas) con \\(Z\\), entonces \\(X\\) y \\(Y\\) son condicionalmente independientes.\n\nEste tipo de estructura también se llama bifurcación, o decimos que \\(Z\\) es un confusor en esta gráfica. Variación en \\(Z\\) produce variación conjunta de \\(X\\) y \\(Y\\).\nPor ejemplo, podríamos encontrar que el uso de aspirina \\(X\\) está asociado a una mortalidad más alta \\(Y\\). Una causa común es enfermedad grave que produce dolor (\\(Z\\)). Sin embargo, si condicionamos a personas sanas, veríamos que no hay relación entre uso de aspirina y mortalidad, igualmente veríamos que entre las personas enfermas el uso de aspirina no les ayuda a vivir más tiempo.\nEn este caso, tenemos:\n\\[p(x, y, z) =  p(z)p(x|z)p(y|z)\\] Y como el lado izquierdo es igual (en general) a \\(p(x,y|z)p(z)\\), obtenemos la independiencia condicional de \\(X\\) y \\(Y\\) dado \\(Z\\).\n\nEjemplo (simulación)\n\nrbern &lt;- function(n, prob){\n  rbinom(n, 1, prob = prob)\n} \nsimular_confusor &lt;- function(n = 10){\n  z &lt;- rbern(n, p = 0.5) |&gt; as.numeric()\n  x &lt;- rbern(n, p = z * 0.3 + (1 - z) * 0.8)\n  y &lt;- rbinom(n, 4, z * 0.9 + (1 - z) * 0.3)\n  tibble(x, z, y)\n}\nsims_confusor &lt;- simular_confusor(50000)\n\n\\(X\\) y \\(Y\\) son dependientes:\n\nsims_confusor |&gt; select(x, y) |&gt; \n  count(x, y) |&gt; \n  group_by(x) |&gt; \n  mutate(p_cond = n / sum(n)) |&gt;\n  select(x, y, p_cond) |&gt; \nggplot(aes(x = y, y = p_cond, fill = factor(x))) +\n  geom_col(position = \"dodge\") +\n  labs(subtitle = \"Condicional de Y dada X\")\n\n\n\n\n\n\n\n\nSin embargo, si condicionamos a \\(Z\\), que puede tomar los valores 0 o 1:\n\nsims_confusor |&gt; \n  count(x, y, z) |&gt; \n  group_by(x, z) |&gt; \n  mutate(p_cond = n / sum(n)) |&gt;\n  select(x, y, z, p_cond) |&gt; \nggplot(aes(x = y, y = p_cond, fill = factor(x))) +\n  geom_col(position = \"dodge\") + facet_wrap(~ z) +\n  labs(subtitle = \"Condicional de Y dada X y Z\")\n\n\n\n\n\n\n\n\nY vemos que la condicional de \\(Y\\) dada \\(Z\\) y \\(X\\) sólo depende de \\(Z\\). Una consecuencia es por ejemplo que la correlación debe ser cero:\n\ncor(sims_confusor |&gt; filter(z == 1) |&gt; select(x,y)) |&gt; round(3)\n\n       x      y\nx  1.000 -0.005\ny -0.005  1.000\n\ncor(sims_confusor |&gt; filter(z == 0) |&gt; select(x,y)) |&gt; round(3)\n\n      x     y\nx 1.000 0.006\ny 0.006 1.000\n\n\nUn ejemplo con variables continuas podría ser como sigue:\n\nsimular_bifurcacion &lt;- function(n = 10){\n  z &lt;- rbern(n, p = 0.5)\n  x &lt;- rnorm(n, 100 + 20 * z, 15)\n  y &lt;- rnorm(n, 100 + 30 * z, 20)\n  tibble(x, z, y)\n}\nsims_bifurcacion &lt;- simular_bifurcacion(5000)\n\n\\(X\\) y \\(Y\\) son dependientes (por ejemplo si vemos la media condicional de \\(Y\\) dado \\(X\\):\n\nggplot(sims_bifurcacion, aes(x = x, y = y, colour = z)) + \n  geom_point(alpha = 0.2) +\n  geom_smooth(span = 1, se = FALSE)\n\n\n\n\n\n\n\n\nSi condicionamos a \\(Z\\), no hay dependencia entre \\(X\\) y \\(Y\\)\n\nggplot(sims_bifurcacion, aes(x = x, y = y, colour = z, group = z)) + \n  geom_point(alpha = 0.2) +\n  geom_smooth(span = 2)\n\n\n\n\n\n\n\n\n\n\nEjemplo: matrimonio y divorcio\nEn este ejemplo de McElreath (2020), se muestra que regiones de Estados Unidos con tasas más altas de matrimonio también tienen tasas más altas de divorcio.\n\ndata(WaffleDivorce)\nWaffleDivorce |&gt; \n  ggplot(aes(x = Marriage, y = Divorce)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nAunque esta es una correlación clara, lo que nos interesa en este caso el efecto causal de \\(M\\to D\\). Es importante notar que hay considerable variabilidad de la edad promedio al casarse a lo largo de los estados:\n\nWaffleDivorce |&gt; \n  ggplot(aes(sample = MedianAgeMarriage)) +\n  geom_qq() +\n  geom_qq_line()\n\n\n\n\n\n\n\n\nPara el modelo causal, tenemos que considerar las siguientes afirmaciones que no son muy difíciles de justificar:\n\nLa edad al casarse es un factor que influye en la tasa de divorcio (menor edad a casarse implica mayores tasas de divorcio, pues las parejas tienen más tiempo para divorciarse, porque la gente cambia más cuando es joven).\nAdicionalmente, si la gente tiende a casarse más joven, en cualquier momento hay más gente con probabilidad de casarse, por lo que esperaríamos que la edad al casarse también influye en la tasa de matrinomio\n\nEsto implica que tenemos que considerar una causa común de la edad al casarse en nuestro diagrama causal:\n\n\nCódigo\ngrViz(\"\ndigraph {\n  graph [ranksep = 0.2]\n  node [shape=plaintext]\n    M\n    D\n    Edad\n  edge [minlen = 3]\n   Edad -&gt; M\n   Edad -&gt; D\n   M -&gt; D\n{rank=same; M; D;}\n\n}\n\", width = 200, height = 50)\n\n\n\n\n\n\nPor la discusión de arriba, es claro que es necesario considerar la edad al casarse si queremos estimar el efecto de tasa de matrimonio en la tasa de divorcio. Es posible que la correlación entre estas dos tasas puede ser explicada por la edad al casarse, y que en realidad al flecha \\(M\\to D\\) sea muy débil o inexistente.\nYa que tenemos este modelo causal básico, tendríamos que proponer un proceso generador, proponer un modelo estadístico, y probar nuestra estimación. Este paso nos lo saltaremos (ver sección anterior), aunque sigue siendo necesario.\nPor el momento recordemos que si condicionamos (se dice también estratificar) por edad al casarse, y no vemos relación condicional entre las dos tasas, la relación que vimos en los datos es factible que haya aparecido por la causa común que induce correlación. Una manera en que estratificamos o condicionamos a una variable continua en un modelo lineal, como sigue:\n\\[D_i\\sim N(\\mu_i, \\sigma)\\] donde \\[\\mu_i = \\alpha + \\beta_M M_i + \\beta_E Edad_i\\] ¿De qué manera estamos estratificando por edad en este ejemplo? Obsérvese que para cada Edad que fijemos, la relación entre \\(M\\) y \\(D\\) es:\n\\[\\mu_i = (\\alpha + \\beta_E Edad) + \\beta_M M_i  \\] Cada valor de \\(E\\) produce una relación diferente entre \\(M\\) y \\(D\\) (en este caso particular, una recta diferente con distinta altura).\nAhora tenemos que poner iniciales para terminar nuestro modelo estadístico. En este punto poner iniciales informadas para estos coeficientes puede ser complicado (depende de cuánta demografía sabemos). Podemos usar un enfoque más simple, considerando las variables estandarizadas. De esta forma podemos poner iniciales más estándar. Utilizaremos\n\nescalar &lt;- function(x){\n  (x - mean(x))/sd(x)\n}\nWaffleDivorce &lt;- WaffleDivorce |&gt; \n  mutate(Marriage_est = escalar(Marriage), \n         Divorce_est = escalar(Divorce), \n         MedianAgeMarriage_est = escalar(MedianAgeMarriage))\ndatos_lista &lt;- list(\n  N = nrow(WaffleDivorce),\n  d_est = WaffleDivorce$Divorce_est, \n  m_est = WaffleDivorce$Marriage_est, \n  edad_est = WaffleDivorce$MedianAgeMarriage_est)\n\n\nmod_mat_div &lt;- cmdstan_model(\"./src/matrimonio-divorcio-1.stan\")\nprint(mod_mat_div)\n\ndata {\n  int&lt;lower=0&gt; N;\n  vector[N]  d_est;\n  vector[N]  m_est;\n  vector[N]  edad_est;\n}\n\nparameters {\n  real alpha;\n  real  beta_M;\n  real  beta_E;\n  real &lt;lower=0&gt; sigma;\n}\n\ntransformed parameters {\n  vector[N] w_media;\n  // determinístico dado parámetros\n  w_media = alpha + beta_M * m_est + beta_E * edad_est;\n}\n\nmodel {\n  // partes no determinísticas\n  d_est ~ normal(w_media, sigma);\n  alpha ~ normal(0, 1);\n  beta_M ~ normal(0, 0.5);\n  beta_E ~ normal(0, 0.5);\n  sigma ~ normal(0, 1);\n}\n\ngenerated quantities {\n  real dif;\n  {\n    //simulamos 50 estados\n    int M = 50;\n    array[M] real dif_sim;\n    for(i in 1:M){\n      real edad_sim_est = normal_rng(0, 1);\n      // fijamos el valor de M en 0 y 1 para el modelo con do(M)\n      real M_sim_0 = normal_rng(alpha * beta_M * 0 + beta_E * edad_sim_est, sigma);\n      real M_sim_1 = normal_rng(alpha * beta_M * 1 + beta_E * edad_sim_est, sigma);\n      dif_sim[i] = M_sim_1 - M_sim_0;\n    }\n    dif = mean(dif_sim);\n  }\n\n}\n\n\n\nsims_mod &lt;- mod_mat_div$sample(data = datos_lista, \n                   chains = 4, \n                   init = 0.1, step_size = 0.1,\n                   iter_warmup = 1000, \n                   iter_sampling = 1000,\n                   refresh = 0)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.1 seconds.\nChain 2 finished in 0.1 seconds.\nChain 3 finished in 0.1 seconds.\nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.6 seconds.\n\n\n\nresumen &lt;- sims_mod$summary(c(\"alpha\", \"beta_M\", \"beta_E\", \"sigma\"))\n\n\nresumen |&gt; \n  ggplot(aes(x = variable, y = mean, ymin = q5, ymax = q95)) +\n  geom_hline(yintercept = 0, color = \"red\") +\n  geom_point() +\n  geom_linerange() +\n  coord_flip()\n\n\n\n\n\n\n\n\nY el resultado que obtenemos es que no observamos un efecto considerable de las tasas de matrimonio en las tasas de divorcio, una vez que estratificamos por la causa común de edad de matrimonio. Este ejemplo es simple y podemos ver el efecto causal directo en un sólo coeficiente \\(\\beta_M\\), pero de todas formas haremos contrastes como hicimos en la parte anterior.\n\n\n5.5.1 Simulando intervenciones\nLa manera más directa de definir efecto causal, bajo nuestros supuestos causales, es a través de intervenciones (imaginarias o reales)\n\n\n\n\n\n\nNota\n\n\n\nEntendemos saber una causa como poder predecir correctamente las consecuencias de una intervención en el sistema generador de datos.\n\n\nEn nuestro caso, el diagrama de arriba muestra nuestro modelo causal. Si nosotros alteramos este proceso causal, interviniendo en la tasa de matrimonio, la distribución de matrimonio ya no depende de la Edad (pues está bajo nuestro control). Esto quiere decir que ahora consideramos el siguiente diagrama, en donde la nueva dependendencia del divorcio del matrimonio la escribiremos como \\(p(D|do(M))\\):\n\n\nCódigo\ngrViz(\"\ndigraph {\n  graph [ranksep = 0.2]\n  node [shape=plaintext]\n    M\n    D\n    Edad\n  edge [minlen = 3]\n   Edad -&gt; D\n   M -&gt; D\n{rank=same; M; D;}\n\n}\n\", width = 200, height = 50)\n\n\n\n\n\n\nEs decir, borramos todas las flechas que caen en \\(M\\) (pues la estamos interveniendo al valor que queramos), y luego simulando \\(D\\).\nEn nuestro ejemplo (ve el código de Stan de arriba, la parte de generated quantities) simularemos los 50 estados bajo dos intervenciones: todos tienen la tasa promedio de matrimonio vs. los 50 estados con tasa de matrimonio un error estándar por encima de la tasa promedio. Repetimos esta comparación sobre todas las simulaciones de la posterior:\n\nsims_tbl &lt;- sims_mod$draws(format = \"df\") |&gt; \n  select(dif) \nsims_tbl |&gt; summarize(\n  q5 = quantile(dif, 0.05),\n  q95 = quantile(dif, 0.95)\n)\n\n# A tibble: 1 × 2\n      q5   q95\n   &lt;dbl&gt; &lt;dbl&gt;\n1 -0.278 0.272\n\n\n\nggplot(sims_tbl, aes(x = dif)) +\n  geom_histogram(bins = 50) +\n  geom_vline(xintercept = 0, color = \"red\")\n\n\n\n\n\n\n\n\nEn este caso, vemos que el resultado de la intervención no tienen una tendencia clara hacia incrementar o disminuir la tasa de divorcio, aunque existe variabilidad por la incertidumbre que tenemos acerca de las relaciones modeladas.\nFinalmente, antes de terminar sería apropiado hacer chequeos predictivos posteriores, pero por el momento los omitiremos para avanzar en los otros tipos de estructuras básicas en los DAGs.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Modelos gráficos y causalidad</span>"
    ]
  },
  {
    "objectID": "05-dags.html#cadenas-o-mediación",
    "href": "05-dags.html#cadenas-o-mediación",
    "title": "5  Modelos gráficos y causalidad",
    "section": "5.6 Cadenas o mediación",
    "text": "5.6 Cadenas o mediación\nEn este caso tenemos:\n\n\nCódigo\ngrViz(\"\ndigraph {\n  graph [ranksep = 0.2, rankdir=LR]\n  node [shape=plaintext]\n    X\n    Y\n    Z\n  edge [minlen = 3]\n   X -&gt; Z\n   Z -&gt; Y\n}\n\", width = 150, height = 20)\n\n\n\n\n\n\nEn este caso,\n\nExiste asociación entre \\(X\\) y \\(Y\\), pero no existe relación directa entre ellas. Decimos que \\(Z\\) es un mediador del efecto de \\(X\\) sobre \\(Y\\).\nSi condicionamos a un valor de \\(Z\\), \\(X\\) y \\(Y\\) son condicionalmente independientes.\n\nPodemos pensar en \\(Z\\) como un mediador del efecto de \\(X\\) sobre \\(Y\\). Si no permitimos que \\(Z\\) varíe, entonces la información de \\(X\\) no fluye a \\(Y\\).\nPor ejemplo, si \\(X\\) tomar o no una medicina para el dolor de cabeza, \\(Z\\) es dolor de cabeza y \\(Y\\) es bienestar general, \\(X\\) y \\(Y\\) están relacionadas. Sin embargo, si condicionamos a un valor fijo de dolor de cabeza, no hay relación entre tomar la medicina y bienestar general.\nEn términos de factorización, podemos checar la independencia condicional: como \\(p(x,y,z) = p(x)p(z|x)p(y|z)\\), entonces\n\\[p(x, y | z) = p(x,y,z) / p(z) = (p(x)(z|x)) (p(y|z) / p(z))\\] y vemos que el lado izquierdo se factoriza en una parte que sólo involucra a \\(x\\) y \\(z\\) y otro factor que sólo tiene a \\(y\\) y \\(z\\): no hay términos que incluyan conjuntamente a \\(x\\), \\(y\\) y \\(z\\). Podemos de cualquier forma continuar notando\n\\[p(x)p(z|x)/p(z) = p(x,z)/p(z) = p(x | z)\\] de modo que\n\\[p(x, y | z) = p(x|z) p(y|z) \\]\nY mostramos un ejemplo simulado:\n\nrbern &lt;- function(n, prob){\n  rbinom(n, 1, prob = prob)\n} \nsimular_mediador &lt;- function(n = 10){\n  x &lt;- rbern(n, p = 0.5) |&gt; as.numeric()\n  z &lt;- rbern(n, p = x * 0.8 + (1 - x) * 0.3)\n  y &lt;- rbinom(n, 2, z * 0.7 + (1 - z) * 0.5)\n  tibble(x, z, y)\n}\nsims_mediador &lt;- simular_mediador(50000)\n\n\\(X\\) y \\(Y\\) son dependientes:\n\nsims_mediador |&gt; select(x, y) |&gt; \n  count(x, y) |&gt; \n  group_by(x) |&gt; \n  mutate(p_cond = n / sum(n)) |&gt;\n  select(x, y, p_cond) |&gt; \nggplot(aes(x = y, y = p_cond, fill = factor(x))) +\n  geom_col(position = \"dodge\") +\n  labs(subtitle = \"Condicional de Y dada X\")\n\n\n\n\n\n\n\n\nSin embargo, si condicionamos a \\(Z\\), que puede tomar los valores 0 o 1:\n\nsims_mediador |&gt; \n  count(x, y, z) |&gt; \n  group_by(x, z) |&gt; \n  mutate(p_cond = n / sum(n)) |&gt;\n  select(x, y, z, p_cond) |&gt; \nggplot(aes(x = y, y = p_cond, fill = factor(x))) +\n  geom_col(position = \"dodge\") + facet_wrap(~ z) +\n  labs(subtitle = \"Condicional de Y dada X y Z\")\n\n\n\n\n\n\n\n\nY vemos que la condicional de \\(Y\\) dada \\(Z\\) y \\(X\\) sólo depende de \\(Z\\). Una consecuencia es por ejemplo que la correlación debe ser cero:\n\ncor(sims_mediador |&gt; filter(z == 1) |&gt; select(x,y)) |&gt; round(3)\n\n       x      y\nx  1.000 -0.008\ny -0.008  1.000\n\ncor(sims_mediador |&gt; filter(z == 0) |&gt; select(x,y)) |&gt; round(3)\n\n      x     y\nx 1.000 0.007\ny 0.007 1.000\n\n\nPodemos también hacer un ejemplo continuo:\n\nsimular_mediador &lt;- function(n = 10){\n  x &lt;- rnorm(n, 100, 10)\n  prob &lt;- 1 / (1 + exp(-(x - 100)/5))\n  z &lt;- rbern(n, p = prob)\n  y &lt;- rnorm(n, 100 + 30 * z, 15)\n  tibble(x, z, y)\n}\nsims_mediador &lt;- simular_mediador(2000)\n\n\\(X\\) y \\(Y\\) son dependientes (por ejemplo si vemos la media condicional de \\(Y\\) dado \\(X\\):\n\nggplot(sims_mediador, aes(x = x, y = y, colour = z)) + geom_point() +\n  geom_smooth(span = 1, se = FALSE)\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\nWarning: The following aesthetics were dropped during statistical transformation: colour\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\n\n\n\n\n\n\n\nSi condicionamos a \\(Z\\), no hay dependencia entre \\(X\\) y \\(Y\\)\n\nggplot(sims_mediador, aes(x = x, y = y, colour = z, group = z)) + \n  geom_point() +\n  geom_smooth(span = 2)\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nNótese que en este ejemplo sí hay un efecto causal de \\(X\\) sobre \\(Y\\), pero está mediado por otra variable \\(Z\\). Si condicionamos a \\(Z\\), no hay relación entre \\(X\\) y \\(Y\\). El análisis condicionado podría llevarnos a una conclusión errónea de que \\(X\\) no influye sobre \\(Y\\).\n\n\n\n\n\n\nTip\n\n\n\nNota que no existe una diferencia estadística entre una bifurcación y una cadena: en ambos casos, las variables \\(X\\) y \\(Y\\) están correlacionadas, y son independientes una vez que condicionamos o estratificamos por \\(Z\\). Sin embargo, su tratamiento en inferencia causal es muy diferente.\n\n\n\nSesgo post-tratamiento\nEn McElreath (2020) se discute que en algunos estudios experimentales, se estratifica por variables que son consecuencia del tratamiento. Esto induce sesgo post-tratamiento, lo cual puede llevar a equivocaciones en donde parece que el tratamiento no tiene efecto cuando sí lo tiene. Incluso bajo condiciones de experimento (donde el tratamiento es asignado al azar) estratificar por mediadores es una mala idea. Ver más en McElreath (2020), donde por ejemplo cita una fuente que en estudios experimentales de Ciencia Política, casi la mitad de ellos sufre de este tipo de sesgo por estratificación por mediadores.\n\n\nEjemplo: Burks\nEste ejemplo es de Pearl y Mackenzie (2018). En 1926 Burks recolectó datos sobre qué tanto podría esperarse que la inteligencia de padres se hereda a los hijos (medido según una prueba de IQ). Construyó un diagrama parecido al de abajo:\n\n\nCódigo\ngrViz(\"\ndigraph {\n  graph [ranksep = 0.2]\n  node [shape = circle]\n    U\n  node [shape=plaintext]\n  edge [minlen = 3]\n    IntPadres -&gt; NSE\n    NSE -&gt; IntHijos\n    U -&gt; NSE\n    U -&gt; IntHijos\n    IntPadres -&gt; IntHijos\n{rank = same; U}\n}\n\")\n\n\n\n\n\n\nComo el NSE es del hogar (una medida general de estatus social), se consideró en principio como una variable pre-tratamiento a la inteligencia de los niños por la que tradicionalmente se controlaba. Burks notó que hacer esto tenía no era apropiado, pues tiene como consecuencia cortar parte del efecto total de la inteligencia sobre el la inteligencia de los hijos. En otras palabras: la inteligencia de los padres hace más probable mejor NSE, y mejor NSE presenta mejores condiciones de desarrollo para sus hijos. Estatificar por esta variable ignora este efecto.\nAdicionalmente, como veremos, condicionar a NSE abre un camino no causal entre Inteligencia de Padres e Hijos.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Modelos gráficos y causalidad</span>"
    ]
  },
  {
    "objectID": "05-dags.html#colisionador-o-causas-alternativas",
    "href": "05-dags.html#colisionador-o-causas-alternativas",
    "title": "5  Modelos gráficos y causalidad",
    "section": "5.7 Colisionador o causas alternativas",
    "text": "5.7 Colisionador o causas alternativas\nEn este caso, a \\(Z\\) también le llamamos un colisionador. Este es el caso que puede ser más difícil de entender en un principio. Consiste de la siguiente estructura:\n\n\nCódigo\ngrViz(\"\ndigraph {\n  graph [ranksep = 0.2]\n  node [shape=plaintext]\n    X\n    Y\n    Z\n  edge [minlen = 3]\n   X -&gt; Z\n   Y -&gt; Z\n}\n\", width = 200, height = 50)\n\n\n\n\n\n\n\nEn este caso \\(X\\) y \\(Y\\) son independientes. Tanto \\(X\\) como \\(Y\\) influyen en \\(Z\\).\nSin embargo, si condicionamos a \\(Z\\) entonces \\(X\\) y \\(Y\\) están asociados.\n\nPor ejemplo, si observamos que el pasto está mojado, entonces saber que no llovió implica que probablemente se encendieron los aspersores.\nComo la conjunta se factoriza como:\n\\[p(x,y,z) = p(x)p(y)p(z|x,y)\\] Entonces integrando sobre \\(Z\\):\n\\[p(x,y) = \\int p(x,y,z)dz = p(x)p(y)\\int p(z|x,y)\\, dz\\] pero \\(p(z|x,y)\\) integra uno porque es una densidad, de forma que \\(x\\) y \\(y\\) son independientes.\nMostramos un ejemplo simulado:\n\nsimular_colisionador &lt;- function(n = 10){\n  x &lt;- rbern(n, 0.5) \n  y &lt;- rbinom(n, 2, 0.7)\n  z &lt;- rbern(n, p = 0.1 + 0.7 * x * (y &gt; 1)) \n  tibble(x, z, y)\n}\nsims_colisionador &lt;- simular_colisionador(50000)\n\n\\(X\\) y \\(Y\\) son independientes:\n\nsims_colisionador|&gt; select(x, y) |&gt; \n  count(x, y) |&gt; \n  group_by(x) |&gt; \n  mutate(p_cond = n / sum(n)) |&gt;\n  select(x, y, p_cond) |&gt; \nggplot(aes(x = y, y = p_cond, fill = factor(x))) +\n  geom_col(position = \"dodge\") +\n  labs(subtitle = \"Condicional de Y dada X\")\n\n\n\n\n\n\n\n\nSin embargo, si condicionamos a \\(Z\\), que puede tomar los valores 0 o 1:\n\nsims_colisionador |&gt; \n  count(x, y, z) |&gt; \n  group_by(x, z) |&gt; \n  mutate(p_cond = n / sum(n)) |&gt;\n  select(x, y, z, p_cond) |&gt; \nggplot(aes(x = y, y = p_cond, fill = factor(x))) +\n  geom_col(position = \"dodge\") + facet_wrap(~ z) +\n  labs(subtitle = \"Condicional de Y dada X y Z\")\n\n\n\n\n\n\n\n\nY vemos que la condicional de \\(Y\\) dada \\(Z\\) y \\(X\\) depende de \\(X\\) y de \\(Z\\).\nLas correlaciones condicionales, por ejemplo, no son cero:\n\nprint(\"Dado Z = 0\")\n\n[1] \"Dado Z = 0\"\n\ncor(sims_colisionador |&gt; filter(z == 0) |&gt; select(x,y)) |&gt; round(3)\n\n       x      y\nx  1.000 -0.281\ny -0.281  1.000\n\nprint(\"Dado Z = 1\")\n\n[1] \"Dado Z = 1\"\n\ncor(sims_colisionador |&gt; filter(z == 1) |&gt; select(x,y)) |&gt; round(3)\n\n      x     y\nx 1.000 0.372\ny 0.372 1.000\n\n\nOtro ejemplo con variables continuas:\n\nsimular_colisionador_2 &lt;- function(n = 10){\n  x &lt;- rnorm(n, 100, 20) \n  y &lt;- rnorm(n, 100, 20)\n  z &lt;- rbern(n, p = 0.92 * ((x + y) &gt; 220) + 0.05) \n  tibble(x, z, y)\n}\nsims_colisionador &lt;- simular_colisionador_2(1000)\n\n\\(X\\) y \\(Y\\) son independientes:\n\nggplot(sims_colisionador, aes(x = x, y = y)) + geom_point()\n\n\n\n\n\n\n\n\nSin embargo, si condicionamos a un valor de \\(Z\\), \\(X\\) y \\(Y\\) ya no son independientes:\n\nggplot(sims_colisionador, aes(x = x, y = y, group = z, colour = factor(z))) + \n  geom_point() + geom_smooth(method = \"lm\", se = FALSE) \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nY vemos que condicional a \\(Z\\), \\(X\\) y \\(Y\\) están correlacionadas, aunque no hay relación causal entre \\(X\\) y \\(Y\\).\n\n5.7.1 Ejemplos\nExisten muchos ejemplos de colisionadores en análisis de datos. Algunos ejemplos se deben a sesgo de selección (puedes dibujar diagramas para cada uno de estos):\n\nPodemos observar correlaciones entre habilidades que en realidad son independientes si observamos muestras de estudiantes seleccionados por un examen de admisión (por ejemplo, para entrar es necesario tener alta habilidad atlética y/o alta habilidad académica).\nEntre los artículos científicos publicados (ver McElreath (2020)), aquellos que son más tomados por las noticias son los menos confiables. Esta correlación puede aparecer aunque no exista relación en proyectos científicos entre confiabilidad e interés de los medios, pues lo que se fondea o publica puede tener dos razones: ser trabajo muy confiable, o ser trabajo que “está de moda” o atrae la atención de los medios.\n\nPero también puede ser consecuencia de condicionar a colisionadores endógenos, y ocurren como parte del procesamiento o construcción de modelos. Un ejemplo interesante de McElreath (2020) es el siguiente:\n\nNos interesa saber si la edad influye en la felicidad o bienestar de las personas.\nAlgún investigador puede pensar que es necesario controlar por sí las personas están casadas o no, por ejemplo, para “quitar” ese efecto o algo así.\nEsto puede ser mala idea si consideramos que un diagrama apropiado puede ser \\(F \\rightarrow Matrim \\leftarrow Edad\\), que se basa en las observaciones de que personas más felices generalmente tienen mayor posibilidad de casarse, y también conforme pasa el tiempo, hay más oportunidades para casarse.\nEsto induce una correlación no causal entre edad y felicidad dentro de los grupos de casados y no casados, y puede llevar a conclusiones incorrectas.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Modelos gráficos y causalidad</span>"
    ]
  },
  {
    "objectID": "05-dags.html#razonamiento-de-descendientes",
    "href": "05-dags.html#razonamiento-de-descendientes",
    "title": "5  Modelos gráficos y causalidad",
    "section": "5.8 Razonamiento de descendientes",
    "text": "5.8 Razonamiento de descendientes\nCondicionar a un descendiente puede entenderse como “condicionar parcialmente” o “débilmente” a los padres de ese descendiente.\nPor ejemplo, condicionar a un colisionador también produce dependencias condicionales:\n\n\nCódigo\ngrViz(\"\ndigraph {\n  graph [ranksep = 0.2]\n  node [shape=plaintext]\n    X\n    Y\n    Z\n    A\n  edge [minlen = 3]\n   X -&gt; Z\n   Y -&gt; Z\n   Z -&gt; A\n}\n\", width = 200, height = 50)\n\n\n\n\n\n\nEn este caso,\n\n\\(X\\) y \\(Y\\) son independientes\n\\(X\\) y \\(Y\\) son dependientes si condicionamos a \\(A\\).\n\nDependiendo de la naturaleza de la asociación entre el colisionador \\(Z\\) y su descendiente \\(A\\), esta dependencia puede ser más fuerte o más débil.\nPor ejemplo, en nuestro ejemplo donde el pasto mojado es un colisionador entre cuánta agua dieron los aspersores y cuánta lluvia cayó, un descendiente del pasto mojado es el estado de las plantas del jardín. Aunque los aspersores trabajan independientemente de la lluvia, si observamos que las plantas se secaron entonces lluvia y aspersores están correlacionados: por ejemplo, si noto que los aspersores están descompuestos, entonces concluimos que no hubo lluvia.\n\ngrViz(\"\ndigraph {\n  graph [ranksep = 0.2]\n  node [shape=plaintext]\n    X [label = lluvia]\n    Y [label = aspersores]\n    Z [label = humedad]\n    A [label = plantas]\n  edge [minlen = 3]\n   X -&gt; Z\n   Y -&gt; Z\n   Z -&gt; A\n}\n\", width = 200, height = 50)\n\n\n\n\n\n\nEjemplo\n\nsimular_desc &lt;- function(n = 10){\n  x &lt;- rbern(n, 0.5) \n  y &lt;- rbinom(n, 2, 0.7)\n  z &lt;- rbern(n, p = 0.1 + 0.7 * x * (y &gt; 1)) \n  a &lt;- rbern(n, p = 0.5 + 0.5 * z)\n  tibble(x, z, y, a)\n}\nsims_colisionador &lt;- simular_desc(50000)\n# No hay correlación\ncor(sims_colisionador$x, sims_colisionador$y)\n\n[1] 0.003457166\n\n\nSin embargo,\n\ncor(sims_colisionador |&gt; filter(a ==0) |&gt; select(x,y))\n\n           x          y\nx  1.0000000 -0.2793349\ny -0.2793349  1.0000000\n\n\n\ncor(sims_colisionador |&gt; filter(a ==1) |&gt; select(x,y))\n\n          x         y\nx 1.0000000 0.1173797\ny 0.1173797 1.0000000\n\n\n\n\n5.8.1 Ejemplo: dependencias de colisionador\nVerificamos que en nuestro modelo de Santa Clara, efectivamente nuestro modelo no implica ninguna dependencia no condicional entre sensibilidad de la prueba y prevalencia. Eso debería ser claro de la simulación, pero de todas formas lo checamos\n\nlibrary(cmdstanr)\nmod_sc &lt;- cmdstan_model(\"./src/sclara.stan\")\nprint(mod_sc)\n\ndata {\n  int&lt;lower=0&gt; N;\n  int&lt;lower=0&gt; n;\n  int&lt;lower=0&gt; kit_pos;\n  int&lt;lower=0&gt; n_kit_pos;\n  int&lt;lower=0&gt; kit_neg;\n  int&lt;lower=0&gt; n_kit_neg;\n}\n\nparameters {\n  real&lt;lower=0, upper=1&gt; theta; //seroprevalencia\n  real&lt;lower=0, upper=1&gt; sens; //sensibilidad\n  real&lt;lower=0, upper=1&gt; esp; //especificidad\n}\n\ntransformed parameters {\n  real&lt;lower=0, upper=1&gt; prob_pos;\n\n  prob_pos = theta * sens + (1 - theta) * (1 - esp);\n\n}\nmodel {\n  // modelo de número de positivos\n  n ~ binomial(N, prob_pos);\n  // modelos para resultados del kit\n  kit_pos ~ binomial(n_kit_pos, sens);\n  kit_neg ~ binomial(n_kit_neg, esp);\n  // iniciales para cantidades no medidas\n  theta ~ beta(1.0, 10.0);\n  sens ~ beta(2.0, 1.0);\n  esp ~ beta(2.0, 1.0);\n}\n\n\nEn este caso, no pondremos información acerca de positivos en la prueba:\n\ndatos_lista &lt;- list(N = 0, n = 0,\n kit_pos = 103, n_kit_pos = 122,\n kit_neg = 399, n_kit_neg = 401)\najuste &lt;- mod_sc$sample(data = datos_lista, refresh = 1000, iter_sampling = 400)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 Iteration:    1 / 1400 [  0%]  (Warmup) \nChain 1 Iteration: 1000 / 1400 [ 71%]  (Warmup) \nChain 1 Iteration: 1001 / 1400 [ 71%]  (Sampling) \nChain 1 Iteration: 1400 / 1400 [100%]  (Sampling) \nChain 1 finished in 0.0 seconds.\nChain 2 Iteration:    1 / 1400 [  0%]  (Warmup) \nChain 2 Iteration: 1000 / 1400 [ 71%]  (Warmup) \nChain 2 Iteration: 1001 / 1400 [ 71%]  (Sampling) \nChain 2 Iteration: 1400 / 1400 [100%]  (Sampling) \nChain 2 finished in 0.0 seconds.\nChain 3 Iteration:    1 / 1400 [  0%]  (Warmup) \nChain 3 Iteration: 1000 / 1400 [ 71%]  (Warmup) \nChain 3 Iteration: 1001 / 1400 [ 71%]  (Sampling) \nChain 3 Iteration: 1400 / 1400 [100%]  (Sampling) \nChain 3 finished in 0.0 seconds.\nChain 4 Iteration:    1 / 1400 [  0%]  (Warmup) \nChain 4 Iteration: 1000 / 1400 [ 71%]  (Warmup) \nChain 4 Iteration: 1001 / 1400 [ 71%]  (Sampling) \nChain 4 Iteration: 1400 / 1400 [100%]  (Sampling) \nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.5 seconds.\n\nsims &lt;- ajuste$draws(c(\"theta\", \"sens\", \"esp\"), format = \"df\")\nresumen &lt;- ajuste$summary(c(\"theta\"))\n\n\nggplot(sims, aes(x = theta, y = sens)) + geom_point() +\n  scale_x_sqrt()\n\n\n\n\n\n\n\n\nNo vemos ninguna asocación entre estas dos variables.\nSin embargo, al condicionar al valor de Positivos, creamos una relación que no podemos interpretar como casual. En este caso particular supondremos prácticamente fija la sensibilidad para ver solamente lo que sucede en el colisionador de especificidad y número de positivos (la especificidad en este ejemplo es más crítica):\n\ndatos_lista &lt;- list(N = 3300, n = 50,\n kit_pos = 1030000, n_kit_pos = 1220000, # números grandes para que esté practicamente\n# fija la sensibilidad\n kit_neg = 399, n_kit_neg = 401)\najuste &lt;- mod_sc$sample(data = datos_lista, refresh = 1000, iter_sampling = 400)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 Iteration:    1 / 1400 [  0%]  (Warmup) \nChain 1 Iteration: 1000 / 1400 [ 71%]  (Warmup) \nChain 1 Iteration: 1001 / 1400 [ 71%]  (Sampling) \nChain 1 Iteration: 1400 / 1400 [100%]  (Sampling) \nChain 1 finished in 0.0 seconds.\nChain 2 Iteration:    1 / 1400 [  0%]  (Warmup) \nChain 2 Iteration: 1000 / 1400 [ 71%]  (Warmup) \nChain 2 Iteration: 1001 / 1400 [ 71%]  (Sampling) \nChain 2 Iteration: 1400 / 1400 [100%]  (Sampling) \nChain 2 finished in 0.0 seconds.\nChain 3 Iteration:    1 / 1400 [  0%]  (Warmup) \nChain 3 Iteration: 1000 / 1400 [ 71%]  (Warmup) \nChain 3 Iteration: 1001 / 1400 [ 71%]  (Sampling) \nChain 3 Iteration: 1400 / 1400 [100%]  (Sampling) \nChain 3 finished in 0.0 seconds.\nChain 4 Iteration:    1 / 1400 [  0%]  (Warmup) \nChain 4 Iteration: 1000 / 1400 [ 71%]  (Warmup) \nChain 4 Iteration: 1001 / 1400 [ 71%]  (Sampling) \nChain 4 Iteration: 1400 / 1400 [100%]  (Sampling) \nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.5 seconds.\n\nsims &lt;- ajuste$draws(c(\"theta\", \"sens\", \"esp\"), format = \"df\")\nresumen &lt;- ajuste$summary(c(\"theta\"))\n\n\nggplot(sims, aes(x = theta, y = esp)) + geom_point() \n\n\n\n\n\n\n\n\nY vemos que condiconando al colisionador, obtenemos una relación fuerte entre prevalencia y especificidad de la prueba: necesitaríamos más datos de especificidad para obtener una estimación útil.\n\nLa razón de que la especificidad es más importante en este ejemplo es que la prevalencia es muy baja al momento del estudio, y los falsos positivos pueden introducir más error en la estimación\nTambién repetimos nótese que el análisis correcto de estos datos no se puede hacer con intervalos separados para cada cantidad, sino que debe examinarse la conjunta de estos parámetros.\n\n\nCon estas tres estructuras elementales podemos entender de manera abstracta la existencia o no de asociaciones entre nodos de cualquier gráfica dirigida.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Modelos gráficos y causalidad</span>"
    ]
  },
  {
    "objectID": "05-dags.html#d-separación",
    "href": "05-dags.html#d-separación",
    "title": "5  Modelos gráficos y causalidad",
    "section": "5.9 d-separación",
    "text": "5.9 d-separación\nAhora buscaremos describir todas las posibles independendencias condicionales y no condicionales que pueden aparecer en una gráfica, para entender cómo aparecen asociaciones entre variables de nuestro modelo, dependiendo del tipo de condicionamiento que hacemos.\nVeremos que el criterio es algorítmico. Más adelante discutiremos cuáles de estas asociaciones se deben a efectos causales y cuáles no, y esto nos permitirá establecer estrategias de condicionamiento (qué variables controlar o no), recolección de datos para construir los estimadores correctos de los efectos causales de interés.\n\n\n\n\n\n\nd-separación: Caminos activos y bloqueados\n\n\n\n\nUn camino entre \\(X\\) y \\(Y\\) es una sucesión de aristas que conecta a \\(X\\) con \\(Y\\) (sin importar) la dirección de las aristas.\n\nAhora supongamos que \\(Z = \\{Z_1,Z_2,\\ldots, Z_q\\}\\) son una colección de nodos. Decimos que un camino \\(p\\) entre \\(X\\) y \\(Y\\) está activo condicional a los nodos en \\(Z\\) cuando:\n\nSiempre que hay un colisionador \\(X_i\\to U\\gets X_j\\) en el camino \\(p\\), entonces \\(U\\) o alguno de sus descendientes está en \\(Z\\)\nNingún otro nodo a lo largo de \\(p\\) está en \\(Z\\).\n\nEn caso contrario, decimos que el camino \\(p\\) está bloqueado.\nSi \\(Z\\) bloquea todos los caminos posibles entre \\(X\\) y \\(Y\\), decimos que \\(X\\) y \\(Y\\) están \\(d\\)-separados condicionalmente a \\(Z\\), o \\(d\\)-separados por \\(Z\\).\n\n\nSegún la discusión que tuvimos arriba de los modos de razonamiento en gráficas de modelos probabilísticos, el siguiente teorema no es sorpresa:\n\n\n\n\n\n\nCriterio de d-separación\n\n\n\nEn una DAG \\(G\\):\n\nSi dos variables están d-separadas por las variables \\(Z\\), entonces \\(X\\) y \\(Y\\) son condicionalmente independientes dadas las variables en \\(Z\\) para cualquier conjunta representada por \\(G\\).\nSi dos variables no están d-separadas por \\(Z\\), entonces existen conjuntas representadas por \\(G\\) tales que \\(X\\) y \\(Y\\) no tienen dependencia condicional dado \\(Z\\).\n\n\n\nNota 1: nótese que este teorema nos da una manera abstracta de razonar acerca de la asociación en un modelo gráfico: no es necesario saber la forma particular de las condicionales para utilizarlo.\nNota 2: Vale la pena mencionar que el segundo inciso en general es una implicación más fuerte: cuando no hay \\(d\\)-separación, existe algún tipo de dependencia casi seguro (en el sentido probabilístico de posible conjuntas).\nNota 3: Las independencias condicionales también pueden ser útiles para checar los supuestos de nuestro modelo: si encontramos asociaciones fuertes (condicionales o no) entre variables que nuestra estructura implica independencia condicional, entonces puede ser que nuestra estructura causal requiera revisión. Qué tanto podemos probar esto depende del tamaño de los datos que tengamos y de el tipo de condicionamiento que estamos haciendo.\nFinalmente (ver por ejemplo Koller y Friedman (2009), p 75), existe un algoritmo eficiente para encontrar todas las posibles independencias condicionales implicadas por una gráfica:\n\n\n\n\n\n\nCálculo de d-separación\n\n\n\nExiste un algoritmo de complejidad lineal en el tamaño de la gráfica para encontrar todos los nodos con caminos activos a un nodo \\(X\\) condicional a las variables \\(A\\).\n\n\nVer por ejemplo el sitio dagitty.net, donde podemos poner nuestra gráfica y enlistar todas los supuestos de independencia condicional implicados por un modelo.\n\nEjemplo\n\n\nCódigo\ngrViz(\"\ndigraph {\n  graph [ranksep = 0.2]\n  node [shape=plaintext]\n    Z \n    W \n    X\n    Y \n    U\n  edge [minlen = 3]\n    Z -&gt; W\n    X -&gt; W\n    X -&gt; Y\n    W -&gt; U\n    S -&gt; Y\n    UZ -&gt; Z\n    V -&gt; Z\n    V -&gt; S\n}\n\")\n\n\n\n\n\n\nConsideremos la relación entre Z y Y. Primero vemos que hay dos caminos entre \\(Z\\) y \\(Y\\), que son \\(p_1:X\\gets V \\to S\\) y \\(p_2: Z\\to W \\gets X \\to Y\\)\n\nEn primer lugar, ¿son independientes si no condicionamos a ninguna variable? No, pues el camino \\(p_1\\) es activo, e induce correlación.\n¿Son condicionalmente independientes si condicionamos a \\(V\\)? En este caso, condicionar a \\(V\\) bloquea el camino \\(p_1\\). El camino \\(p_2\\) está bloqueado por el colisionador \\(W\\), así que todos los caminos están bloqueados si condicionamos a \\(V\\).\nSi condicionamos a \\(W y V\\), ¿son independientes? No. El camino \\(p_1\\) está bloqueado, así que ese no induce asociación. Sin embargo, al condicionar al colisionador \\(W\\) activamos el camino \\(p_2\\).\nAhora supongamos que tenemos datos condicionales a algún valor de \\(W\\) solamente. Condicionando a \\(V\\) bloqueamos el camino \\(p_1\\), pero el camino \\(p_2\\) está activo. ¿Qué pasaría si condicionamos adicionalmente a \\(X\\)? En este caso, el conjunto de condicionamiento es \\(\\{V, W, X\\}\\). El camino \\(p_2\\) está bloqueado. Y aunque condicionamos al colisionador, \\(X\\) bloque el camino. Por lo tanto \\(Z\\) y \\(Y\\) son condicionalmente independientes dado \\(\\{V, W, X\\}\\).\n\n\n\n5.9.1 Ejercicio\nRepite el ejemplo anterior para la siguiente gráfica. Analiza que pasa si condicionamos o no a valores de \\(T\\), y qué pasa si adicionalmente condicionamos a \\(W\\), y luego repite los pasos del ejemplo anterior.\n\n\nCódigo\ngrViz(\"\ndigraph {\n  graph [ranksep = 0.2]\n  node [shape=plaintext]\n    Z \n    W \n    X\n    Y \n    U\n    T\n  edge [minlen = 3]\n    T -&gt; Z\n    T -&gt; Y\n    Z -&gt; W\n    X -&gt; W\n    X -&gt; Y\n    W -&gt; U\n    S -&gt; Y\n    UZ -&gt; Z\n}\n\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Modelos gráficos y causalidad</span>"
    ]
  },
  {
    "objectID": "05-dags.html#caminos-causales",
    "href": "05-dags.html#caminos-causales",
    "title": "5  Modelos gráficos y causalidad",
    "section": "5.10 Caminos causales",
    "text": "5.10 Caminos causales\nSi el DAG que consideramos representa relaciones causales (mecanísticas) entre las variables, es decir, qué variable “escucha” a qué otras para decidir su valor, entonces podemos hacer una definición adicional\n\n\n\n\n\n\nCaminos causales\n\n\n\nEn un DAG, los caminos causales entre \\(X\\) y \\(Y\\) son de la forma \\(X\\to U_1\\to U_2 \\to \\cdots U_j \\to Y\\). Puede haber varios de ellos en un diagrama dado, y cada uno representa un mecanismo en que cambios en \\(X\\) producen cambios en \\(Y\\)\nSi nos interesa el efecto total de \\(X\\) sobre \\(Y\\),\n\nQueremos que todos los caminos causales de \\(X\\) a \\(Y\\) estén activos,\nQueremos condicionar para que todos los caminos no causales estén bloqueados, en particular, no queremos condicionar a colisionadores o sus descendientes que introduzcan relaciones no causales, y queremos bloquear caminos no casuales creados por bifurcaciones.\n\n\n\n\n\n\n\nKoller, D., y N. Friedman. 2009. Probabilistic Graphical Models: Principles and Techniques. MIT Press.\n\n\nMcElreath, R. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. A Chapman & Hall libro. CRC Press. https://books.google.com.mx/books?id=Ie2vxQEACAAJ.\n\n\nPearl, Judea, y Dana Mackenzie. 2018. The Book of Why: The New Science of Cause and Effect. New York: Basic Books.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Modelos gráficos y causalidad</span>"
    ]
  }
]